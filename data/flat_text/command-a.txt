SOURCE_ID: command-a
SOURCE_TITLE: Cohere Safety Framework (ArXiv)
SOURCE_URI: https://arxiv.org/pdf/2504.00698
--------------------
Command A:
An Enterprise-Ready Large Language Model

Cohere1

Abstract
In this report we describe the development of Command A, a powerful large language model purpose-built to
excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model,
with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top
of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with
grounding and tool use to automate sophisticated business processes. These abilities are achieved through
a decentralised training approach, including self-refinement algorithms and model merging techniques. We
also include results for Command R7B which shares capability and architectural similarities to Command A.
Weights for both models have been released for research purposes. This technical report details our original
training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks
and public benchmarks, demonstrating excellent performance and efficiency.

Introduction

1
Large Language Models (LLMs) are Artificial Intelligence (AI) models designed to understand and gener-
ate human-like text conditioned on the input they receive. Recent advancements have led to remarkable
breakthroughs in their ability to comprehend and produce human language with unparalleled accuracy and
fluency. This progress has been instrumental in their widespread adoption across various real-world and
enterprise environments, where they significantly boost operational efficiency and deepen understanding.

This technical report describes the development of Command A and Command R7B, two LLMs designed to
excel in real-world enterprise settings. Both the 111B parameter Command A and Command R7B perform
best-in-class across a suite of established benchmarks for their respective model sizes. We also highlight
key innovations and technical contributions including data and architectural optimisations, self-refinement
algorithms, and a model merging-based approach optimised to bring out expert-level performance across
capabilities within a single set of model weights, providing fast and efficient performance.

Command A is tailored for excellent performance in enterprise-relevant settings such as Retrieval Augmented
Generation (RAG), where models can interact with, understand, and process information distributed across a
wide range of documents. As part of this focus, our models also excel in the multilingual setting, supporting 23
key languages of global business: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean,
Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian,
Greek, Hindi, Hebrew, and Persian.

Along with its impressive overall performance, achieving best-in-class results for any model in its size and
efficiency range on common benchmarks such as MATH, Command A outperforms across an extensive
suite of human evaluation tasks as shown in Figure 1. Furthermore, Command A achieves strong results on
enterprise-relevant agentic benchmarks such as Taubench, as shown in Table 1.

Command A focuses on delivering competitive performance as efficiently as possible. With a serving footprint
of just two A100s or H100s, Command A requires considerably less computational overhead than comparable
models. This is of particular importance for privacy-preserving enterprise settings and on-premises deploy-
ments. Command A can deliver tokens at a rate of up to 156 tokens/sec which is 1.75x higher than GPT-4o
and 2.4x higher than DeepSeek V3.

1Please cite this technical report as “Cohere (2025)”. A full author list can be found at the end of this document.

1

Released as a preprint on April 7, 2025

Figure 1: Head-to-head human evaluation win rates. All examples are blind-annotated by specially trained
human annotators, assessing enterprise-focused accuracy, instruction following, and style.

A
d
n
a
m
m
o
C

3
V
k
e
e
S
p
e
e
D

85.5

80.0

90.9

50.8

51.7

63.8

86.2

59.5

92.6

68.8

88.5

70.2

86.1

59.1

39.1

58.6

89.9

53.1

92.2

69.8

o
4
-
T
P
G

85.7

68.5

83.8

53.6

51.2

72.1

86.5

50.5

91.2

71.0

B
0
7
3.3
a
m
Lla
86.0

77.0

92.1

50.5

21.0

51.4

84.4

58.0

85.6

62.5

B
7
R
d
n
a
m
m
o
C

65.2

59.1

77.9

26.3

52.2

72.0

42.2

69.6

48.1

B
8
3.1
a
m
Lla
71.1

51.9

78.6

23.4

50.9

72.8

41.9

73.6

49.2

B
8
al
r
t

Minis

71.1

54.5

59.0

23.4

51.8

61.1

33.2

62.0

36.8

Capability

Benchmark

Academic

Agents

Code

Multilingual

MMLU

MATH

IFEval

GPQA

Taubench

BFCL

MBPP+

Bird-SQL

RepoQA

NTREX

Table 1: Command A and Command R7B results on key academic, agentic, code and multilingual bench-
marks, in comparison to relevant external models.

We also release model weights to the research community to facilitate community-based exploration under
a CC-BY-NC License (Non-Commercial) with an acceptable use addendum. The model checkpoints are
available on the HuggingFace model hub.

2 Pre-training

2.1 Overview
Pre-training language models involves training a model on trillions of tokens of unlabelled text data to learn
general language patterns, syntax, and semantics, enabling it to generate contextually relevant responses.
This foundational step leverages self-supervised learning techniques, such as next-token prediction, to build
a versatile representation of language that can subsequently be fine-tuned for specific downstream tasks. Pre-
training is computationally intensive but essential for achieving state-of-the-art performance across diverse
natural language processing applications.

2

Human Preference EvaluationCommand A vs GPT-4o (Nov)Command A vs DeepSeek-V3GeneralBusinessSTEMCodeGeneralBusinessSTEMCode50.4%​51.4%​46.8%​49.6%​48.6%​53.2%​49.0%​49.3%​54.7%​51.0%​50.7%​45.3%​2.2 Data
Command A models are trained on multilingual data (also see Section 3.3.3.1) from various sources including
publicly available text and code data from the web, a collection of synthetic datasets generated internally,
instruction-tuning datasets obtained from human annotators, and high quality data sourced from specialised
data vendors. We optimise the web text data by enhancing the ratio of educational samples that are relatively
sparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based
quality filters after careful de-duplication and heuristic filtering for safety and quality. The final data mixture
is determined by running a series of ablations using smaller models.

2.3 Model Architecture

Figure 2: Schematic of the Command A model architecture.

We use a decoder-only Transformer architecture (Vaswani et al., 2017) as depicted in Figure 2. We highlight
a few key architectural decisions below:

• SwiGLU. The SwiGLU activation (Shazeer, 2020) demonstrates performance improvements over other

activation functions.

• Interleaved attention layers. We use interleaved layers of sliding window attention and full attention
in 3:1 ratio. Each sliding window layer uses Rotary Positional Embeddings (RoPE) (Su et al., 2021) and
every full attention layer uses No Positional Embeddings (NoPE) (Kazemnejad et al., 2023). Further
details can be found in Yang et al. (2025).

• GQA. We use grouped-query attention (GQA; (Ainslie et al., 2023)) to increase serving throughput.
We use document masking to ensure that each individual sequence in a batch can only attend to itself.
• Parallel transformer block. This shows equivalent performance but significant improvement in

throughput compared to the vanilla transformer block.

• No bias. Similar to PaLM (Chowdhery et al., 2023), we do not employ bias terms, which improves

training stability at larger scales.

• Input and output embeddings. We share the input and output embedding matrices, which provides
a large reduction in memory requirements due to our large vocabulary size. We do not observe any
performance degradation across ablations.

3

Sliding Window Self-A!ention  - Grouped-query a!ention  - RoPE positional embeddingsMLP  - SwiGLU activation  - No bias terms Self-A!entionMLP. . .  Tokenizer - Vocabulary size : 255,000  - Multilingual  - Special tokens for chat turns, tool calls.  LM Head  Output embeddings  Input embeddingsCommand ACommand A Transformer Block (SWA)Shared input and output embeddingsInterleaved SWA and full a!ention (3:1 ratio)  Transformer Block 4 Self-A!ention  Transformer Block 2(SWA) Self-A!entionMLP  Transformer Block 3(SWA)(Full) Self-A!entionMLP  Transformer Block 1(SWA)MLP  Transformer Block N Self-A!ention(Full)MLPFull  Self-A!ention    - Grouped-query a!ention  - No positional embeddingsMLP  - SwiGLU activation  - No bias termsCommand A Transformer Block (Full)2.4 Pre-training Recipe
We perform most of our hyperparameter optimisation at considerably smaller scales than those representing
our final family of models. We use µP and µTransfer (Yang et al., 2021) to tune hyper-parameters on smaller
models and zero-shot transfer them to our larger models. Sweeps are performed for each model size as they
assume a fixed number of layers.

2.4.1 Distributed Training
We train all our models on our NVIDIA H100 GPU cluster using our internal JAX-based (Frostig et al.,
2018) distributed training framework. Our framework leverages JAX’s GSPMD (Xu et al., 2021) as the
backbone to implement complex sharding strategies. We split the available GPUs into a mesh with four axes
for each of the following sharding schemes:

• Data Parallel (DP) axis shards the activations along the batch dimension, which behaves as standard

data parallel training when all GPUs are allocated to it.

• Fully Sharded Data Parallel (FSDP) axis shards both the activations along the batch dimension
and model states along a specified dimension. The model states are replicated across the data parallel
axis to contain the communication costs to the FSDP submesh.

• Sequence Parallel (SP). Given the restrictions on critical batch-size of LLMs, scaling the number of
GPUs in pure FSDP/DP scenarios is infeasible. We thus use sequence parallelism (Li et al., 2023b) to
shard activations along the sequence dimension. The activations after the QKV projection are sharded
along the heads dimension to remove communication costs during the attention computation. The
attention outputs are sharded on the outer dimension, and the weight matrix of the final attention
output transformation is sharded along the contracting dimension as in Megatron-style (Shoeybi et al.,
2019) sharding. This allows us to operate using a single all-gather and a single reduce-scatter for the
activations, while only gathering QKV and attention outputs along the FSDP axis. At the feed forward
block, the FFN transformation is independent along the sequence axis, therefore there is no need for
any activation communication. Moreover, since we use parallel attention and a FFN block setup, we
completely overlap the computation of the FFN expansion layer and the all-gather of the attention
activations. The reduce-scatter after the attention block is further overlapped with the execution of the
FFN reduction layer. Since all other major operations such as layer norm, input and output embedding
layers are independent along the sequence axis, they incur no communications along the activations.
• Tensor Parallel (TP) axis for a pure Megatron-style sharding, where two complementary matrix
multiplications are sharded such that the activations are all-gathered before the first matrix multipli-
cation (where the weight is sharded on the outer axis, resulting in the activations being sharded on the
outer axis as well), and one all-reduce after the second matrix multiplication (to sum the partial out-
puts). Pure TP is desirable when moving activations between devices as it is much cheaper compared
to moving weights, a layout class sometimes referred to as weight-stationary (Pope et al., 2023). We
use pure TP for fast decoding and in low batch-size scenarios.

Our models are trained with varying combinations of the parallelism strategies mentioned above. During
pre-training, since we are in the high batch-size and throughput regime, we opt for a combination of DP,
FSDP and SP to minimise activation communication. Furthermore, we can unroll the model’s forward loop
to overlap the communication of the weights of the next layer with the execution of the current layer.

We leverage Hopper-specific features such as FP8 tensor cores (Micikevicius et al., 2022) to further improve
throughput. While many works have reported instability while training with FP8 precision for long training
horizons (Fishman et al., 2025), we observe no such instability. In fact, we observe minimal run interventions
due to loss spikes and optimisation instability. We keep our main weights and optimiser states in FP32
precision, and cast the model weights to BF16 or FP8 prior to the computation. We keep sensitive operations
such as exponentials, softmaxes, layer norms, and output embeddings in FP32 precision, and run the attention
computation in BF16 precision. While we do not observe any training instabilities with FP8 matmuls, we
notice that there is a small but non-trivial degradation in downstream performance if the entire training
run is in FP8. To mitigate this effect, we first perform a number of steps in BF16 precision, which brings
performance back to the full BF16 trained model’s performance range.

4

Figure 3: Command A goes through multiple post-training phases including two weighted model merging
steps, and a model polishing phase.

2.5 Cooldown
We linearly anneal the learning rate from 2.5 × 10−4 to 1 × 10−6 for 50,000 steps in BF16 precision with
purposely curated high quality datasets. The context length is initially maintained at 8k tokens for the
first 30,000 steps, then extended to 32k, 128k, and 256k for 10,000, 5,000, and 5,000 steps respectively by
interleaving long context pre-training data every fourth step. During the long-context stages, we adjust the
overall ratio of datasets to ensure a balanced mixture across domains (Fu et al., 2024), while maintaining a
sufficient proportion of long-context data.

3 Post-training

3.1 Overview
Command A is post-trained using a novel decentralised approach to maximise and control its performance
over a wide spectrum of domains and capability areas. More precisely, Command A is trained by alternating
centralised training stages, where a single model is fine-tuned, and decentralised training stages, where
multiple expert models are trained separately to maximise domain-wise performance before merging their
parameters. Although the classic post-training paradigm involves training a single model sequentially with
varying data-mixtures (Dubey et al., 2024; Team et al., 2024), Command A is the first large-scale attempt
to combine multiple parallel expert tracks, with parameter merging techniques playing a central role. This
section details the high-level post-training recipe of Command A, illustrated in Figure 3.

We divide the global Command A post-training recipe into several sub-stages, each producing intermediary
model artifacts:

• Instruct Model: We train an initial Instruct model with supervised learning on top of the base model

to provide the core basic capabilities of the model.

• SFT Expert Models: We train six SFT experts on top of the Instruct checkpoint with specialised

data mixtures to maximise capability-specific performance.

• SFT Soup Model: We merge the six model experts into a Soup model with parameter-merging

methods (see Section 3.4) to produce a single SFT aggregate model.

5

Model MergingRAG, Tool Use & Agents  Expert (SFT)Multilingual Expert (SFT)Code Expert (SFT)Math & Reasoning Expert (SFT)Long Context Expert (SFT)Safety Expert (SFT)Model MergingReinforcement LearningSupervised Fine-TuningPolishingInstruction-Tuned CheckpointMerged Checkpoint (SFT Soup Model)Instruction-Following Expert (SFT)Expert WeightsRAG, Tool Use & Agents  Expert (RL)Multilingual Expert (RL)Code Expert (RL)Math & Reasoning Expert (RL)Long Context Expert (RL)Safety Expert (RL)Instruction-Following Expert (RL)Supervised Fine-Tuning (SFT)Oﬄine-Preference Reinforcement Learning (RL)Reinforcement Learning (RLHF)Oﬄine-Preference Reinforcement Learning (RL)RAG & AgentsMultilingualCodeReasoningLong-ContextSafetyInstructionExpert WeightsRAG & AgentsMultilingualCodeReasoningLong-ContextSafetyInstructionMerged Checkpoint (RL Soup Model)• RL Expert Models: We train six RL experts on top of the SFT Soup checkpoint using RL algorithms

tailored to each domain, using pairwise comparisons or verifiable rewards.

• RL Soup Model: We merge the six RL experts into a RL Soup model with parameter-merging

methods to produce a single RL aggregate model.

• Polished Model: We perform a final stage on the RL Soup model to enhance human interaction
performance by alternating between best-of-N methods, offline preference, and online RL algorithms.

Six expert models are created at each expert stage: Code, Safety, RAG, Math, Multilingual, and a General
Long-Context expert. This approach allows us to adapt each expert’s training procedure, tailoring it to the
specific capability or domain of interest. This allows fine-grained hyperparameter tuning, specialised data
mixture optimisation, local optimisation (e.g., seed merging), and the capability-specific selection of the
most appropriate algorithms. This becomes even more crucial during the RL stage, as different domains
demand distinct RL techniques — for example, verifiable rewards for Math and Code, or preference pairs
for Safety and Multilingual. Our late-merging procedure allows us to re-balance Soup model performance a
posteriori without requiring additional training (§3.4). From an organisational perspective, merging allows
contributors to collaborate closely in parallel, fostering a unique model development synergy. Overall, this
decentralised training procedure maximises individual expert performance while controlling the final global
model capacity, allowing us to optimise both model performance and efficiency.

Finally, the model undergoes a polishing phase to improve its writing style. First, we apply a best-of-N
supervised training stage to the RL Soup model. Then, we alternate between offline preference and online RL
optimisation in a ping-pong approach, iterating as required until we observe a human preference performance
plateau, to obtain the final Command A model.

In the following sections, we introduce the methods and algorithms that we use at various stages of the
post-training process. We detail individual expert recipe considerations and provide further technical details
on the merging techniques applied. Finally, we discuss key features of the model polishing phase.

3.2 Methods

3.2.1 Supervised Fine-Tuning
In all cases, the first stage of our post-training pipeline involves finetuning the pretrained model to follow
instructions and operate in a conversational setting. We structure Supervised Fine-Tuning (SFT) (Wei et al.,
2021; Sanh et al., 2021) datapoints as prompts and completions. Prompts are model input sequences that may
contain information such as preambles or system prompts defining expected model behaviour, tool specifi-
cations, conversational history, special tokens (e.g., <|START_OF_TURN_TOKEN|> or <|SYSTEM_TOKEN|>), and
queries or instructions. Completions refer to the sequence of tokens that the model is trained to generate
conditioned on a given prompt. We train the model using a cross-entropy loss with the loss corresponding
to prompt tokens masked out. Depending on the specific setting, we may choose to regularise training either
by including some small proportion of pretraining data, or a parameter-based L2 penalty to the pretrained
model. We optimise using the AdamW algorithm (Loshchilov & Hutter, 2017) with decoupled weight decay.

3.2.2 Reinforcement Learning
Depending on the stage and task, we perform direct alignment using preference training (Rafailov et al.,
2024; Azar et al., 2024) or we optimise for a reward signal through reinforcement learning (RL) (Sutton et al.,
2023), either offline or online. This reward signal can be the learnt reward model, or a verifiable reward (for
example, based on unit tests for code generation or on response correctness for reasoning).

3.2.2.1 Preference Training with Self-refinement
We consider preference training methods for learning offline from preference datasets: Sequence Likelihood
Calibration, or SLiC (Zhao et al., 2023), Direct Preference Optimisation, or DPO (Rafailov et al., 2024), and
Identity Preference Optimisation, or IPO (Azar et al., 2024). In addition to these conventional preference-
training methods, our model-training pipeline incorporates our novel Self-improving Robust Preference Op-
timisation (SRPO) (Choi et al., 2025). This recently-developed approach represents a significant departure

6

from traditional preference training techniques, introducing a novel mechanism for continuously enhancing
model alignment and robustness. It amounts to solving the following min-max optimisation problem:

min
π

max
π†

ExEy1∼π(·|x),y2∼π†(·|x,y1) [P (y2 ≻ y1|x) − β KL(π†||πref|x, y1) + β KL(π||πref|x)] .

This objective function aims to learn a self-improvement policy π† that can improve generations from π,
according to the preference model P without deviating too much from a reference model πref, and at the
same time at learning a policy π of which generations cannot be improved by π†. SRPO’s novelty partly
lies in its robustness: unlike classical methods, it does not depend on the sampling distribution of the
preference dataset. This independence ensures greater generalisation and stability in varied deployment
scenarios. Furthermore, SRPO uniquely enables iterative self-revision at inference time, a process where the
model sequentially refines its output: Given an initial prompt, the model first generates an initial completion
using the generative policy π, followed by multiple sequential refinements through the self-refinement policy
π†, each progressively improving the quality and alignment of the final output. This iterative refinement
capability is not present in conventional alignment pipelines, underscoring SRPO’s innovative contribution.

3.2.2.2 Optimising the Reward Model with RL
When given a reward function, be it the reward model or a verifiable reward, we consider the classic KL-
regularised reinforcement learning objective, J(π) = ExEy∼π(·|x)[R(x, y) − β KL(π||πref|x)]. In all settings
(offline or online), we optimise it using the recent Contrastive Policy Gradient approach, or CoPG (Flet-
Berliac et al., 2024). For a prompt x and k > 1 completions y1, . . . , yk of arbitrary origin, the corresponding
loss to be minimised is

ℓ(x, y1, . . . , yk; π) =

1
k − 1

(cid:88)

i>j

(cid:0)Rπ

β(x, yi) − Rπ

β(x, yj)(cid:1)2 with Rπ

β(x, y) = R(x, y) − β ln

π(y|x)
πref(y|x)

.

The CoPG loss can be used in both the offline and online cases. In the online case, it can be used with
a replay buffer, possibly combined with additional datasets, or in a pure on-policy fashion, in which case
it is equivalent to Reinforce Leave-One Out, or RLOO (Ahmadian et al., 2024). Furthermore, Flet-Berliac
et al. (2024) show that the gradient of this loss is a form of (negative) off-policy policy gradient, not relying
on importance sampling, clipping, or on an additional value network. It also comes with strong theoretical
guarantees, notably estimating the KL-regularised optimal policy π∗ even in the offline case, and generalising
policy gradient and some preference training approaches. In the offline case, it can be used with any dataset,
as long as there is more than one completion per prompt, and that we can compute the associated rewards.
We mostly use CoPG offline and online on-policy.

3.2.3 Reward Models
We train a Bradley-Terry reward model for use in online preference learning, evaluation, and data filtering.
Similar to Gunter et al. (2024), we use a cross-entropy loss with soft labels as targets.

We find that reward models tend to suffer from high memorisation, causing catastrophic collapse in perfor-
mance on a second epoch over the same data; so, we train the model in two stages. The first stage consists
of approximately 4 million samples designated as “lower quality” and relabelled using an ensemble of reward
models. Training is carried out for one epoch with a batch size of 1024 with a cosine learning rate schedule
with a peak of 4 × 10−5. The second stage consists of approximately 350,000 high quality samples, with
labels derived from the strength of human preferences, ensembles of models (with labels inconsistent with
human annotations moved to the first stage), or a constant label value of 0.999 for gold-standard data and
0.5 for gold-standard tied pairs. This stage uses a smaller batch size of 16, and a lower maximum learning
rate of 3 × 10−6. Both stages use packed data, where multiple pairs of preference data are encoded in a single
training sample for efficiency, using attention masking to avoid different (non-packed) samples influencing
each other. The pairs are left-padded to align their <EOS_TOKEN>, and distributed to aim for a 75% fill while
keeping the number of samples per row as uniform as possible, ensuring an equal loss contribution.

Our internal reward model scores 92.7% on RewardBench (Lambert et al., 2024), and achieves an average
score of 72.3% on RMB (Zhou et al., 2024).

7

3.3 Capabilities

Instruction-Following

3.3.1
Core instruction-following capabilities are crucial for LLMs to solve tasks across areas and domains. We
therefore consider instruction-following a prerequisite for more specific model capabilities focusing on ad-
vanced topics such as code, multilingual, and reasoning. As such, in the Command A post-training recipe, we
teach the model to follow instructions across a wide range of topics and domains, including but not limited to
generalist instruction-following (e.g., factual knowledge requests), formatting, STEM-specific tasks (e.g., tab-
ular reasoning, structured data manipulation), and preamble compliance. Instruction-following capabilities
are acquired both via SFT and offline preference tuning.

3.3.1.1 Data collection
Our data collection approach can be divided based on the post-training method, i.e., SFT or preference
tuning. To collect datasets that serve both of these, we primarily rely on synthetic prompt generation in
conjunction with human annotation, and explore various sampling and filtering techniques (Bartolo et al.,
2021). Specifically, we synthetically generate diverse sets of prompts covering a range of instructions tailored
to individual domains (such domains are mostly enterprise-oriented) and generate two completions per
prompt sampled with different temperatures. We then ask human annotators to provide discrete ratings for
both completions. This process is repeated over multiple turns, resulting in a multi-turn dataset. If the two
completion ratings are not tied and the better completion does not obtain the highest possible rating, we
ask the annotator to improve the better completion.

SFT samples. SFT datasets are constructed using the human rewrites obtained from the process mentioned
above, to ensure the highest completion quality.

Preference pairs. We construct preference pairs directly from the obtained samples by considering com-
pletions with different ratings (including the human rewrites), with ties excluded. It is worth noting that the
obtained preference samples are used to train both Command A and our reward model itself.

Iterative Reward-based Sample Refinement

3.3.1.2
We further experiment with reward-based sample refinement approaches to obtain both SFT and preference
pairs using the synthetically-generated prompts. Similar to Dubey et al. (2024), we use internal reward
models trained on our most recent Command A checkpoints in conjunction with a collection of both human-
written completions and completions generated from different checkpoints under different conditions (e.g.,
varying temperature values) during post-training This implies that the resulting dataset does not contain
purely synthetic completions, and human-written completions are retained for inputs where the models fail
to generate high-quality completions. We approach this in an iterative fashion, where we use the most recent
checkpoints at a given point in time to generate completions, score those completions using our reward
model, create preference pairs and SFT samples using the scores, re-train our models, and repeat.

3.3.1.3 Preambles
A specific focus of the instruction-following post-training of Command A lies in the model’s ability to follow
preamble (or system prompt) requirements. Preambles are designed to contain instructions that should apply
to an entire conversation and potentially multiple model inputs, meaning that instead of having to repeat
instructions in every prompt, they can be defined directly in the preamble. Such system instructions could
specify what language the model should reply in (e.g., “Always respond in French.”), the desired format of
model generations (e.g., “Always use JSON.”, “Do not generate Markdown.”), or the exclusion of specific words
and phrases (e.g., “Do not use the word ‘LLM’ in your response.”). To train Command A to follow preamble
instructions, we develop methods based on synthetic data generation to create diverse preambles that are
attached to prompts flowing into the above-described pipeline. The preambles are then taken into account
when creating the respective completions and preferences, i.e., preamble-augmented data is used during
both SFT and preference tuning. During preamble generation, we aim to maximise instruction diversity to
encourage robustness to a wide range of instructions at inference time.

8

3.3.1.4 Model training
In the context of instruction-following we post-train Command A in sequence with SFT and preference
tuning. For preference tuning, we experiment with a range of methods, including SLiC, IPO, DPO, and
SRPO (for further details, see Section 3.2.2). We find that SRPO performs best across evaluation tasks and
select a checkpoint trained using SRPO for the final Instruct model.

3.3.2 Retrieval Augmented Generation, Tool Use and Agents
Recent breakthroughs have propelled LLMs beyond simple chatbots, transforming them into versatile agents
capable of navigating complex environments. At the heart of this evolution is their ability to use tools
strategically: invoking APIs, analysing results, and iterating dynamically to accomplish goals. This agen-
tic behaviour is pivotal for two key advancements. First, integrating knowledge sources outside of model
parameters (e.g., via Retrieval-Augmented Generation, or RAG) mitigates hallucinations and ensures accu-
rate information beyond the timespan of model training. Second, agentic frameworks empower models to
orchestrate vast action chains, potentially executing hundreds of API calls to automate intricate workflows.
Together, these capabilities expand LLMs’ operational horizons, enabling them to tackle tasks once deemed
beyond their reach.

3.3.2.1 Agentic Tool-Use
Empowering LLMs with Tools. LLMs have demonstrated remarkable proficiency in leveraging external
tools to enhance their capabilities (Schick et al., 2023). By generating API calls, models can execute specific
actions—like performing calculations or retrieving information—to solve tasks more effectively. This process
typically involves providing a set of tool definitions in the model’s preamble. When faced with a task, the
model selects and invokes the appropriate tools, and the results are fed back to inform its final response.

A prime example of this is Retrieval-Augmented Generation (RAG). In RAG (Lewis et al., 2020), the model
has access to a search tool (e.g. a dense embedding index) to answer information-seeking queries. It generates
search queries, retrieves relevant snippets from the selected knowledge source, and uses this context to craft
a well-informed response.

Agents. For more intricate tasks, models may need to orchestrate multiple tools across several steps. This
requires a structured approach to halt generation, extract tool calls, execute them, and reintroduce results
into the model’s workflow—a process repeated until the task is resolved.

We roughly follow the ReAct framework (Yao et al., 2022), a widely adopted method for guiding LLMs
through dynamic problem-solving. ReAct enables models to interleave reasoning and action: they first artic-
ulate their thought process, outlining plans and tool requirements, then either execute a tool (via structured
outputs like JSON) or deliver a final answer. This iterative loop enables adaptive planning, reflection, and
interaction with external systems, making it ideal for complex, multi-step tasks.

3.3.2.2 Data and Training
We train our model on a combination of human-annotated and synthetically generated data. We collect
datapoints in multiple languages to directly supervise on, as well as datapoints with preference judgments
for multiple completions. The data covers areas of code tool execution, user-uploaded documents, and general
API environments. Training consists of an SFT step on agentic datasets followed by offline preference tuning
using the Contrastive Policy Gradient loss (§3.2.2.2).

Data format. Each datapoint contains a user prompt along with a set of available tools and potentially
custom model instructions that the user has provided to the model. The datapoint also contains a reasoning
step, where the model reasons about which tools to use to fulfil the user request and how to fill in the input
parameters of the tools. This is followed by tool calls and tool outputs, which can be concurrent or sequential.
The datapoint concludes with a model response that includes citations to the tool outputs.

Data collection. Annotation is performed by internal annotators specifically trained in ReAct-style data.
All annotated data used for SFT is reviewed multiple times by different annotators to ensure correctness and

9

quality. For preference data, we use a majority vote of at least 3 annotators to collect preference judgments.

Synthetic data. We also generate synthetic training data containing whole trajectories of reasoning and
tool calls. We verify the quality of the trajectories using internal LLMs-as-a-judge.

3.3.3 Multilingual
The ability to communicate in and understand multiple languages is a fundamental component of enterprise
LLMs. Command A is designed to handle a wide array of languages, ensuring that information can be
accessed and shared seamlessly across different linguistic communities. By incorporating solid multilingual
capabilities in 23 languages, Command A enables businesses and individuals to reach a broader audience,
fostering inclusivity and accessibility. Moreover, the multilingual aspect of Command A facilitates better
understanding and collaboration among international teams, driving innovation and efficiency.

3.3.3.1 Data Mixture
We focus our data mixture on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese,
Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Roma-
nian, Greek, Hindi, Hebrew, and Persian. This ensures coverage to expand state-of-the-art language modelling
capabilities to approximately half of the world’s population (Aryabumi et al., 2024; Üstün et al., 2024). Our
multilingual data mixture spans a diverse set of domains and tasks, covering machine translation, multilin-
gual safety, multilingual reasoning, multilingual robust controllability, multilingual RAG, multilingual agents,
etc; ensuring that Command A possesses strong generalisation capabilities across languages. The datasets
are collected through various means including human annotation, multilingual data arbitrage (Odumakinde
et al., 2024; Dang et al., 2024), templated public datasets, or machine translation. Our data mixture is
specifically tailored to handle multilingual learning through SFT and offline preference tuning.

3.3.3.2 Multilingual Data Annotation
Multilingual data annotation is performed by internal and external multilingual annotators who are expertly
trained for annotation within various tasks. It can be divided into two distinct processes, i.e., regionally-
relevant data annotation and complex multilingual task annotation, which cover use cases for both SFT
and preference tuning. For complex tasks such as domain-specific RAG, long-context reasoning, or agentic
tool-use tasks, we conduct human annotation using two different approaches: 1) LLM-generated response
with human post-editing; and 2) manually annotated human data. The prior helps us scale the quantity of
data, while the latter helps develop high-quality multilingual data for tackling complex tasks. We develop
a customised in-house data annotation platform that can support both of these use cases. The high-quality
data generated from human annotations also helps to further improve the quality of the machine-generated
responses providing a positive feedback loop within the annotation process.

Multilingual Best-of-N. To further improve the multilingual quality of Command A, we conduct iterative
synthetic data collection through multilingual best-of-N (Stiennon et al., 2020; Eisenstein et al., 2024). Using
a collection of high-quality prompts, we collect responses from all expert models and score them using our
internal reward models and select the best response to be used in our iterative training. This approach is very
similar to multilingual arbitrage where the model is trained on responses from diverse teacher models. Using
this approach, we observe from human evaluation that LLMs can produce responses that are comparable or
even better than the human-written gold label provided in many multilingual datasets.

3.3.3.3 Training
The multilingual expert model is trained via both SFT and Preference Tuning (full details in Appendix B.2).
We find that training several models with the same configuration (but a different random seed) and uniformly
merging them gives a slight performance boost for the expert at the SFT stage, but does not help at the
preference tuning stage.

10

3.3.4 Code
Generating and understanding code is a fundamental requirement for any enterprise LLM. We invest in
the code capabilities of Command A to assist the software development cycle and improve user coding
experience. Command A’s success in code-oriented tasks is also a precise measurement of capability in
instruction-following, pragmatic inference in interpreting prompts, and procedural reasoning. Our models
excel in challenging business environments, including understanding and translating legacy programs in
COBOL, and using SQL to interface with relational databases.

3.3.4.1 Data
Data Mixture. Our data mix focuses on 8 priority programming languages (Python, Java, C++, Rust,
Go, JavaScript, TypeScript, COBOL) and 5 dialects of SQL (SQLite, MySQL, PostgreSQL, Microsoft T-
SQL, Oracle PL/SQL). Across these languages, we target a wide range of tasks including code generation
(i.e., NL-to-code), code translation, and code optimisation. Within these tasks, we include diverse domains
such as interview-style questions; repository-level queries; and enterprise-specific demands (including high-
dimensional arrays, complex debugging, data processing, and visualisation).

Prompts and completions are sourced from annotation campaigns and synthetic generation pipelines. We
enrich prompt-completion pairs with additional information including execution feedback, explanations, para-
phrases, stack traces, database context (Chang & Fosler-Lussier, 2023), diff patch formats, and unit-testing
requirements. We prioritize candidate data with positive execution-based validation to filter erroneous or
unverifiable code. This includes passing gold-standard unit tests or correct and error-free execution within
a database. We use a multi-language code execution sandbox to evaluate code correctness in an isolated
environment similar to Guo et al. (2024) and Team et al. (2025).

During pretraining, we perform execution-based code data enrichment. We isolate self-contained functions
and files and add print statements of some variables and generate synthetically valid input parameters. The
resulting code is executed in a sandbox and the output is appended to the enriched source code, adding several
billion pretraining tokens. There is the added benefit that a subset of code repositories can be formatted as
a very long document where files are linearised following a graph traversal defined by import links.

In the RL stage, we jointly optimise for code correctness and annotated preferences between code comple-
tions. This approach enhances both the functional accuracy of generated code and reduces edit times of
technically correct but suboptimal or dispreferred generations. We quantify performance using the propor-
tion of unit tests passed in our code execution sandbox, where a reward of 1.0 indicates 100% test success
and 0.2 represents 20% test success. When no valid code block is detected, we assign a reward of −1.0 to
explicitly penalize non-code output. We use synthetic unit-test generation ensuring all code completions have
a minimum of 4 tests per sample. Our synthetic test generation pipeline is similar to Zeng et al. (2025) with
more robust unittest tests over assert statements. The preferred SQL completions are canonicalised using
static query analysis. Beyond verifiable metrics, we incorporate DPO-style preference pairs (Rafailov et al.,
2024) to optimise for code style conventions, documentation structure, and formatting consistency.

Synthetic Data. We experiment with synthetic data pipelines for post-training data enrichment. As a
result, a high proportion of our data are verified synthetic examples in many coding languages. For synthetic
generation sources, we exploit our highest performing models for code, and generalist models for explanations
and reasoning. We experiment with both novel data synthesis and conditional synthetic data augmentation.

Our novel data synthesis efforts include generating examples taking inspiration from concepts, similar to
StarCoder (Wei et al., 2024), and sampling pretraining programming queries. We explore pipelines where
our synthesis is Python-only followed by translating code and unit tests into additional languages, or direct
generation into any programming language. While the former is useful for generating parallel corpora and
targeting Python benchmarks, the latter pipeline proves valuable for problems using absent or uncommon
features of Python (e.g., multithreading or memory management for C++). We use our execution sandbox to
verify all synthetic completions — ensuring that any synthetic example teaches a novel skill via verified code.
This approach to data synthesis only improves performance for small models (i.e., data-based distillation from
larger models). Novel synthesis methods yield negligible improvement for larger models, instead requiring

11

human annotation and synthetic data augmentation to advance our most capable coding experts.

We rely on synthetic data augmentation to diversify the style, syntax, and complexity of our code data. Our
data augmentation pipeline includes prompt paraphrasing, injecting stricter requirements into prompts for
more precise instruction following, and complexifying prompts similar to Magicoder (Wei et al., 2023). In
verifiable scenarios, we use execution feedback to build data for code repair or translation, where iterative
feedback provides guidance until the repaired code passes all tests. In a similar scenario for SQL, a repaired
or translated SQL is adequate when it returns an equivalent answer from a target language database. This
offline pipeline can generate prompt-completion pairs, but we also cast this iterative process into multi-turn
data to simulate conversational code repair.

We also use synthetic augmentation to improve non-verifiable aspects of our data. This includes code ex-
planations, markdown style formatting, technical precision, and global completion structure. We use reward
modelling and majority voting to score these non-verifiable code completions. We also elicit feedback from
human annotators to guide our data synthesis pipeline towards developer preferences for code style, struc-
ture, and explanations. This regularises against overfitting to the preferred style of any LLM judge, and our
generations’ target style and structural features are actually preferred by human raters.

3.3.4.2 Training
The code expert is trained in three stages (hyperparameters and full details in Appendix B.3):

Stage 1 is large-scale supervised learning, with the code data mixture described above. This stage includes
data for all relevant tasks to optimise a high level of coding capability. To mitigate variance in initialisation,
learning trajectory, and performance on small evaluation datasets we use linear merging over the top k
seeds (Izmailov et al., 2018; Team et al., 2024; Yadav et al., 2024; Aakanksha et al., 2024; Khalifa et al.,
2025) where k is typically 2 or 3. We observe that this merged model is a strictly superior initialisation for
continued training with additional fine-tuning or RL.

Stage 2 is supervised learning on only high-quality data. From the first stage fine-tuning, we further
strengthen our code expert with additional fine-tuning on our “highest-quality” code generation datasets.
We define “high-quality” as verifiable human or synthetic data from our best experts, or data rated highly by
internal reward models. As before, we train multiple candidate models and merge across random seeds to pro-
duce the final SFT code expert. This secondary fine-tuning stage increased our key benchmark performance
with negligible regression in tasks only present in stage 1 training (e.g., SQL query optimisation).

Stage 3 is RL over scored or preference-pair data. We train the expert with the offline Contrastive Policy
Gradient algorithm (§3.2.2), to train with execution feedback and DPO-style preference pairs as described
above. To ensure stable RL, we introduce three regularisation schemas. First, we repeat the Stage 2 high-
quality supervised learning and merging process on any non-code expert model (e.g., a merge of multiple
experts). CoPG on a merged checkpoint was strictly more stable and yielded better results than RL on top
of an individual SFT/merge. Second, we introduce a hybrid cross-entropy loss function on top of CoPG to
sample steps of typical supervised learning from the same Stage 2 data mix. Third, we use WARP-style
merging (Ramé et al., 2024) to combine the final model trained with RL to the parent checkpoint. This
hybrid approach ensures stable reinforcement learning optimisation to improve both our code experts for
user preference, and improving our performance on intrinsic code generation capabilities.

3.3.5 Math and Reasoning
Sophisticated reasoning abilities are a necessary competency area for generalisation in LLMs (Guo et al.,
2025; Team et al., 2025; Toshniwal et al., 2024). We focus primarily on core mathematical reasoning as it
is both intrinsically useful (e.g., in financial use cases) and yields out-of-distribution improvements in other
knowledge-intensive tasks such as coding and data manipulation (Islam et al., 2023; Shao et al., 2024).

12

3.3.5.1 Data
We find that training on synthetic data outperforms human-written data, so our approach is heavily weighted
towards the use of synthetic examples. We use carefully-curated seed prompts for few-shot generation of
novel mathematical problems, and LLM-as-judge techniques to determine the correctness of novel problem-
solution pairs. We find that the choices of prompt seeds, correctness validation, and final dataset filtering
have a substantial impact on the quality of our reasoning-expert models.

3.3.5.2 Training
Supervised Fine-Tuning. For SFT, we leverage synthetic mathematical reasoning datasets that have
undergone extensive LLM-driven filtering for solution correctness. We find, similar to Toshniwal et al.
(2024), that strict correctness cut-offs are not needed for optimal SFT performance.

Preference Tuning. We employ preference tuning following SFT across one of two datasets, dependent on
the downstream model training stages: The first dataset is comprised of human-rated preferences on paired
completions to reasoning prompts. The second, fully-synthetic dataset comprises correct and incorrect paired
solutions to reasoning prompts. We find that, unlike in SFT, solution correctness is of critical importance
in preference training (e.g., so that preferences are not accidentally inverted), and in the absence of human-
written ratings, we use a mixture of programmatic and model-driven verifiers to evaluate solution correctness.

Merging. We find that using candidate models exhibiting maximal reasoning performance is sometimes
detrimental to the cross-capability merge under particular merging strategies. We observe that optimal
merged performance is achieved when first merging various reasoning-tuned and instruction-tuned expert
models, and this yields a sufficiently high-signal proxy for selecting Pareto-optimal candidates to merge with
a broader mix of models downstream. We employ this selection for our final set of candidate models, with
the exact selection criteria along the Pareto frontier being dependent on downstream merging strategies.

Training hyperparameters for SFT and preference tuning are in Appendix B.4.

3.3.6 Long Context
Data. Given the complexity of human annotation for long-context tasks, we synthetically generate long-
context examples. We sample from our long-context pretraining dataset and prompt Command R+ Refresh
to generate question-answer pairs based on randomly selected fragments within 8,192 tokens (Xiong et al.,
2024). To ensure high-quality, we use our reward model to select the best generation from a pool of candidates.
The selected question-answer pairs are then concatenated to the original samples to construct our synthetic
data.

Training. We perform one stage of SFT on top of the pretrained model, following a similar approach to our
cooldown phase. We use an interleaved training regime with datasets of 16k and 256k sequence lengths at a
3:1 interleaving ratio. Hyperparameters are in Appendix B.5.

3.3.7 Safety
AI Safety focuses on quantifying and mitigating harms that can occur from using a given AI model, either
to the end user, to the company deploying it, or to society at large. Harms can arise from a single piece of
generated content (e.g. hate speech). They can also be distribution-based, which is the case when the model
is biased towards certain groups. This section focuses on model safety at the instance level, that is, how we
decrease the risks stemming from single generative instances of a given model. We include a distribution-based
evaluation (§4.6) and consider it to be a form of robustness (Seshadri & Goldfarb-Tarrant, 2025).

Cohere’s core Safety behaviour. We focus on practical safety considerations, driven both by model
capabilities and deployment use cases. We consider two main settings in which our models can be deployed:
• The Default setting, in which the model is used entirely outside of Cohere (e.g. open weights release).
In this scenario, we lack control of the preamble structure and deployment context. We ensure that
the model behaves according to Cohere’s Core Safety behaviour in this general setting.

• The Enterprise setting, in which the model is deployed by Cohere to one of our enterprise partners.

13

Here the safety behaviour of the model is controllable by the preamble, to meet different enterprise
needs exceeding Core Safety behaviour. The controllable behaviours are called "Safety modes". There
are currently two safety modes; contextual, and strict.

Our Core Safety behaviour focuses of five key areas where we want to prevent the purposeful propagation of
harmful content online: Violence and hate, Misinformation, Self-harm, Child Sexual Exploitation and Abuse
(CSEA) and Sexual content. In the default setting, we expect the model to be able to answer requests for
information on those topics (covering factual elements such as statistics, educational content); however it
should not generate any unsafe content, that is, supporting, encouraging or otherwise enabling harm. In the
enterprise setting, the contextual mode is similar, but allows sexual content. The model behaviour can be
made stricter by using the strict mode, which prevents the model from covering any topic related to our key
focus areas, as well as from generating profanity.

3.3.7.1 Data
Pretraining. We perform two stages of safety-related pretraining filtering: first, we remove known domains
for CSEA and sexual content, and second, we use a classifier to remove generally low quality content, including
sexual content.

Post-training. In post-training, we use both SFT and preference datasets, with a combination of manual and
automated labels. Safety annotation is performed by internal annotators and specialist external vendors, who
are specifically trained for our Safety concepts and tasks. Our close interaction with internal Safety annotators
provides additional benefits due to the potentially distressing nature of the data. We increase the diversity
of our post-training data via both LLM ‘personas’ and LLM-based reformulations. We generate completions
corresponding to different styles, identities and belief systems via diverse LLM personas. Additionally, we
use our LLM to reformulate content (preserving overall semantics but changing form), thus increasing data
diversity and making sure that the preferred completions are consistent with our refusal policy (in particular,
the model should not apologise for refusing to generate unsafe content, which creates a common dataset
artifact (Chen & Goldfarb-Tarrant, 2025)).

Balancing safety and refusal behaviour. Ensuring that the model cannot produce harmful content
means that a lot of training data shows refusal as the preferred behaviour. It is crucial to balance such data
points with authorised user requests and compliant completions to prevent the model from over-relying on
refusal – as previously referred to in the literature as the balance between harmlessness and helpfulness (Bai
et al., 2022). The balancing prompts can be split into two sets: user requests which are information requests
on safety topics, and benign user requests with similar vocabulary and structure as unsafe prompts.

3.3.7.2 Training
Improving overall model safety means finding a fine balance between over- and under-refusal. We find it
crucial to split datasets in two: namely in their safety-increasing (where the model should refuse) and
helpfulness-inducing (where the model should answer) components. This allows us to balance these aspects
differently during training. We use both SFT and offline preference tuning. We find offline preference tuning
crucial in limiting over-refusal, however, it is less efficient than SFT at making the model safer. We observe
this behaviour both on 8B models and 111B models, with the main difference between the two regimes being
the effect of regularisation, with larger models more prone to overfitting. Overall, the biggest impact on our
model’s ability to respond safely and helpfully is achieved in the polishing process described in Section 3.5.

The Safety expert differs from other experts in that during the preference tuning stage we combine an offline
preference loss with an equally weighted SFT loss. Preference tuning focuses on reinforcing helpfulness via
helpfulness preference pairs, while SFT focuses on reinforcing safety via safety-inducing data. We find that
IPO and DPO perform similarly, with SLiC showing a worse trade-off between over- and under-refusal, so
we use IPO. Full details on SFT and preference hyperparameters are in Appendix B.6.

14

3.4 Merging

3.4.1 Definition
Model merging refers to the process of combining a set of model parameters θi for i ∈ [1, K], into a single
combined model θmerged = f (θ1, ..., θK), where f (·) is some merging function. The merging function can
range in complexity from simple averaging (Izmailov et al., 2018; Wortsman et al., 2022; Li et al., 2022) to
methods based on Fisher information (Matena & Raffel, 2022) and sign agreement between models (Yadav
et al., 2023; Yu et al., 2024). Model merging produces a single set of model parameters, resulting in faster
inference than ensembling and lower memory requirements than runtime query routing.

Figure 4: Model merging allows teams to build domain expert models that excel at different capabilities
independently. These experts are merged into a single model that retains close-to-expert capability levels
across multiple domains or capabilities.

3.4.2 Merging Taxonomy
We here list the different merging techniques, each using different models and having different goals.

Expert merging. Expert merging refers to the process of combining a set of models with different capabili-
ties to produce a single monolithic model with those capabilities. In this setting, the input models will likely
be trained on various datasets and exhibit performance along a single domain only. The aim is to produce
a single set of parameters that preserves as much of the individual ‘expert’ performance as possible. Expert
merging is a core feature of the Command A training pipeline, and we describe it in more detail in §3.4.3.

Merging as Polyak averaging. Merging may be used to achieve a form of Polyak averaging (Ruppert,
1988; Polyak & Juditsky, 1992). Here, the input models are checkpoints from different points along a single
training run, and merging acts as a smoothing operation that reduces the effects of noise inherent in stochastic
gradient-based optimisation.

Seed merging. Merging may also reduce the effects of random seeds (e.g., for initialization or data loader
ordering). Merging the final checkpoints from multiple equivalent training runs with different random seeds
can reduce the risk of overfitting and lead to a more robust model.

Interpolation for capability recovery. We observe multiple instances of capability forgetting Kirkpatrick
et al. (2017), whereby training an expert on one capability degrades performance on other capabilities. This
is a particular issue for long-context abilities since experts are generally trained on top of a long-context
capable model but with training schemes that use short context lengths. In this situation, merging an expert
with the original base model can recover a significant proportion of the original capability while retaining
the new expert capability. This setting is closely related to the WARP approach (Ramé et al., 2024).

15

Expert 1 (Model Checkpoint )Expert 2 (Model Checkpoint )Merged ModelCapability 1Capability 2Capability 1Capability 2Capability 1Capability 2+=3.4.3 Expert Merging
The overall goal for an enterprise-ready LLM is a single monolithic model, with multiple capabilities. These
capabilities can sometimes be orthogonal (e.g., code and safety competencies have very different data distri-
butions) and may involve different scales of training data. For example, it is more straightforward to generate
high volumes of synthetic data in more easily verifiable domains, such as code and reasoning, compared to
domains like safety or RAG, where human-annotated data is more prevalent. These differences introduce
technical and operational challenges: how can we enable asynchronous development of model capabilities, and
jointly optimise for a range of capabilities with highly varied training dynamics?

Model merging enables multiple teams to work asynchronously on improving different capabilities, with
their contributions merged together in parameter space. The capabilities exhibited by Command A cover
a wide range of data scales, that would be non-trivial to combine into a single dataset and optimisation
strategy. Merging allows each team to separately optimise hyperparameters and data scale for peak per-
formance on their capability of interest. Our final model was informed by 500 separate evaluation metrics,
which would have been significantly less practical in a more centralised organisational structure. Merging is
computationally cheap, allowing us to quickly and easily rebalance the capabilities of the final model.

We apply merging at two points in the overall training pipeline: firstly, to combine a set of expert models
trained using SFT into an initial ‘SFT model soup’; secondly, to combine a set of experts that were trained
using offline preference optimisation techniques on top of the SFT soup, giving an ‘off-pref soup’. At both
stages, our aim is to jointly maintain as high a proportion of the expert capability as possible while also
allowing for rebalancing of the overall capabilities of the final model.

3.4.3.1 Linear merging is simple but effective
We employ linear merging (also known as weight averaging), with weights chosen by manual search. We find
that, broadly speaking, the interaction between expert weights and resulting model performance is fairly
intuitive; increasing the weight of a domain expert is likely to increase the performance in that domain.
However, this relationship is not perfect, and the corresponding degradation in performance of other (implic-
itly downweighted2) domains is much less predictable. We therefore search across merging weights using a
combination of heuristics (i.e., upweight experts for domains in which a merge candidate is underperforming)
and brute force search (i.e., perturb the weights for each expert, centred around the current candidate). We
experimented with more complex merging methods (e.g., SLERP (Ramé et al., 2024) and task vectors (Il-
harco et al., 2023)) but found no significant performance improvements, at the cost of increased complexity.
In addition, linear merging is associative, meaning that a linear merge of linear merges can be expressed as
a single merge operation, improving the interpretability of a complex training pipeline.

3.4.3.2 Consistency is more important than optimality
All expert models are initialised from a common ‘general instruction following’ model, for two reasons.
Firstly, some domain experts make use of special tokens (e.g., tool calls) whose embeddings otherwise remain
untrained. We find that selectively merging these embeddings only from checkpoints where they are trained
is beneficial, but suboptimal. Using a shared generalised instruction-following model as initialisation for
each expert and merging the special token embeddings as normal performs much better, even though these
embeddings are likely to be lower quality. Secondly, we find that the post-training process generally degrades
long-context performance, and that this is challenging to recover. Starting from a generalised model that is
‘long-context capable’ preserves long-context performance more easily throughout the training pipeline.

We find it valuable to include ‘leave-one-out’ merges as part of the search process, to reveal instances where
one expert model causes performance degradation of others, or ‘collisions’. To address this, we include a small
amount of cross-domain data in each expert’s training, to act as a regulariser and ensure that each expert
remains ‘compatible’ with the other experts. We also observe that collisions can be caused by small incon-
sistencies in the style or formatting of the common data between experts. In combination this implies that
maintaining some consistency between expert models is more important than absolute expert performance.

2For linear merging, the weights must sum to 1. Increasing the weight of one expert therefore requires reducing the weight

of one or more of the other experts.

16

3.4.3.3 Merging is cheap, evaluation is expensive
We note that most prior work on model merging assumes that the set of input experts is fixed, and seeks to
find a single merging method that optimises some value function, generally a single metric or small number
of metrics (e.g., Wang et al., 2024a). These methods often involve a large number of hyperparameters
(Ilharco et al., 2023) or extensive search over merge weights (Khalifa et al., 2025). By contrast, our goal is
to optimise for a wide range of capabilities and many metrics. This introduces a further challenge generally
not acknowledged by the literature; while model merging is cheap and fast, evaluating each merge requires
significant inference time and compute. Evaluation is therefore a significant bottleneck when applying model
merging in a production context. The set of input models is also not fixed, and a significant portion of the
effort towards successful merging involves making changes to the training scheme used by the experts.

3.5 Polishing
Model merging provides a powerful mechanism for combining a diverse set of experts into a single model.
However, combining experts trained to target specific capabilities does not guarantee the final model’s
alignment with human preferences. To address this, we introduce a polishing phase as the final post-training
step. This phase serves two critical purposes: fixing any artifacts introduced during model merging and
aligning the final model with human preferences.

Unlike other specific capabilities such as coding or instruction-following, human alignment has a cross-
domain effect and influences every aspect of the model’s behaviour. The polishing phase ensures that the
model adheres to human expectations, including tone and style, without sacrificing technical competence.

Polishing is divided into three steps. First, we apply SFT on a subset of our highest quality datasets. Second,
we apply offline preference tuning, and finally, we apply online Reinforcement Learning from Human Feedback
(RLHF). We find that ping-pong-style interleaving of offline and online preference learning helps improve
alignment with human preferences while avoiding regressions and mitigating reward model hacking.

Supervised Fine-Tuning (SFT). We employ a best-of-N SFT approach (Stiennon et al., 2020) where we
synthetically generate four candidate completions for each prompt. We leverage our reward model (§3.2.3)
trained on human preference data to rank these completions. We then apply SFT using the highest-ranked
completions, ensuring that the model learns from the most highly rewarded responses.

Preference Tuning. We use offline preference training to align our model with human preferences. We
select completions with the highest reward scores as preferred completions, and use the completions with
the lowest reward scores as dis-preferred. Additionally, we refine the dataset by filtering out prompts ex-
hibiting a low average reward. To further improve the model’s proficiency in instruction-following, mathe-
matical reasoning, and coding, we incorporate domain-specific preference data into our training mixture. For
instruction-following, completions that correctly adhere to all instructions are considered preferred, while
completions failing to meet all instruction criterion are labelled as dis-preferred. To construct preference
data for mathematical reasoning, we categorise completions that yield correct answers as preferred and those
failing to produce accurate solutions as dis-preferred. Similarly, for code generation tasks, code snippets
passing all unit tests serve as preferred completions, while those failing the tests are used as dis-preferred
completions. We also filter these preference datasets by removing samples for which the preferred completions
are assigned a lower score than the dis-preferred completions by our reward model. We rely again on the
SRPO loss due to its robustness and its self-refinement abilities (§3.2.2.1). In our implementation of SRPO,
following Grinsztajn et al. (2024), we average the log-likelihoods of preferred and dispreferred completions
to control for variations in the completion length.

Reinforcement Learning from Human Feedback (RLHF). To enhance the alignment of our model
with human preferences, we further employ Reinforcement Learning from Human Feedback (RLHF). We
use online CoPG (§3.2.2.2) with two generations per prompt. The prompts used for RLHF training are
derived from a subset of those previously used during SFT, including reasoning, multilingual, coding, and
preference-based tasks prompts. We regularize training using an auxiliary L2 loss with the reference policy,
and an SFT loss using a high-quality subset of post-training data.

17

Area

Benchmarks

Academic, General Knowledge and
Instruction Following (§4.1)

MMLU; MMLU-Pro; GPQA; IFEval; InFoBench

Agents and Tool-Use (§4.2)

TauBench; BFCL.

Multilingual (§4.3)

Code (§4.4)

MMMLU; NTREX; FLoReS; MGSM; mArenaHard (LLM-as-a-Judge); Language
Confusion Benchmark; Al-Qasida; INCLUDE 44; mTauBench.

LBPP; HumanEvalPack; MBPP+; Spider; Bird SQL; RepoQA; LiveCodeBench; Big-
CodeBench; SWE-Bench Diff Generation; Aider Polyglot; internal datasets.

Math and Reasoning (§4.5)

MATH; AIME; LiveBenchMath; Waterloo; OpenQuant; FinanceBench; OmniMath.

Safety (§4.6)

XSTest; internal datasets.

Long-Context (§4.8)

Needle-in-a-Haystack; RULER; RulerQA.

Table 2: Benchmark datasets used to evaluate Command A models, grouped by area.

4 Results
We report results from a diverse and extensive set of evaluations benchmarking the performance of Command
A and Command R7B. We evaluate a broad range of capabilities using public academic datasets and internal
evaluations. Table 2 gives an overview of the capability areas we focus on and the corresponding benchmarks.
We present a snapshot of results on a representative subset of these evaluations in Table 1 opening this report.
Full details for each dataset are available in the corresponding sections.

We compare our models against open and closed models in similar parameter count ranges. Wherever possible,
we show externally reported results with comparable evaluation settings. Where these are not available, we
attempt to internally reproduce these results as faithfully as possible given the information provided publicly.

4.1 Standard Benchmarks
While our primary aim is to build a highly performant model for enterprise use cases (§4.7), we also measure
performance on standard academic datasets to evaluate baseline model knowledge and capabilities. Where
applicable (MMLU, MMLU-Pro, GPQA), we follow the simple-evals implementation, including data, task
settings, prompting, and answer parsing. More details can be found in Appendix B.7.

Model

Command A
GPT-4o
DeepSeek V3
Llama 3.3 70B Instruct
Llama 3.1 405B Instruct
Mistral Large 2
Claude 3.5 Sonnet
Gemini 2.0 Pro

Command R7B
Llama 3.1 8B Instruct
Ministral 8B
Gemma 2 9B Instruct
Gemini 1.5 Flash-8B

MMLU

MMLU-Pro

GPQA

IFEval

InFoBench

85.5
89.2
88.5
86.0
88.6
85.2
89.5
89.3

65.2
71.1
71.1
73.5
74.8

69.6
77.9
75.9
66.0
73.0
67.9
78.0
79.1

42.4
46.5
43.0
50.6
48.4

50.8
53.6
59.1
50.5
49.0
48.6
65.0
64.7

26.3
23.4
23.4
31.3
31.6

90.9
83.8
86.1∗
92.1
88.6
83.8
90.2
87.3

77.9
78.6
59.0
74.4
88.0

94.9
94.0
94.3
92.8
93.9
93.3
93.9
92.2

85.6
90.1
88.3
87.2
88.3

Table 3: Results for Command A and Command R7B on standard academic benchmarks. ∗Note that for
IFEval, Liu et al. (2024a) report only the prompt-level strict accuracy. We report the average of the prompt-
and instruction-level strict accuracies for all other models (see Appendix B.7).

We note that academic benchmarks have various limitations such as saturation, bias and alignment to
real-world performance (Kiela et al., 2021). Human assessment of model capabilities can be undesirably

18

influenced by confounders (Hosking et al., 2024), be subject to idiosyncratic, conversational and demographic
variance (Kirk et al., 2024), and demonstrates imperfect correlation to academic benchmarks (Schaeffer et al.,
2025). Enterprise-relevant capabilities are often not well-represented in these benchmarks, so we augment our
evaluations with enterprise-oriented signal (e.g. §4.2, §4.7), and human annotation based evaluation (§4.11).

Table 3 shows results on these selected benchmarks. Command A is competitive across all benchmarks,
generally outperforming similarly-sized models while remaining competitive with considerably larger and
less-efficient models. On the instruction-following benchmarks, we observe that Command A performs com-
petitively across both IFEval and InFoBench. Specifically, it outperforms all similarly sized models on In-
FoBench and is outperformed only by Llama 3.3 70B Instruct on IFEval. We also note that Command A
represents a substantial improvement over our previous Command R+ Refresh model.

4.2 Agentic Tool Use

Model

Command A
GPT-4o
DeepSeek V3

ChatRAGBench

StrategyQA

Bamboogle

DROP

HotPotQA

72.9
66.6
40.3

76.7
81.2
73.8

76.0
76.0
70.4

91.1
89.5
85.7

92.1
92.1
90.1

Table 4: Standard RAG evaluations. Correctness is determined following the procedure in Verga et al.
(2024) where a panel of LLMs judges the model’s generation against a reference answer.

Model

Command A
Llama 3.3 70B Instruct
Mistral Large 2
Qwen 2.5 72B Instruct
Claude 3.5 Sonnet
Claude 3.7 Sonnet
DeepSeek V3
GPT-4o

Command R7B
Llama 3.1 8B Instruct
Gemma 2 9B Instruct
Ministral 8B
Qwen 2.5 7B Instruct

BFCL Overall

Live AST

Multi-turn

63.8
51.4
58.5
63.5
56.5
58.3
58.6
72.1

52.2
50.9
51.6
51.8
53.7

80.5
62.8
69.9
79.0
78.9
78.4
68.4
79.8

69.2
61.1
68.0
64.9
67.4

25.5
6.9
23.8
24.6
41.0
48.4
18.6
47.6

5.0
9.6
1.6
11.4
7.6

Table 5: BFCL Results. All numbers taken from official leaderboard. Where leaderboard entries exist for
both function calling and prompted, we take the larger of the two reported values.

Model

Taubench Retail

Taubench Airline

Command A
Llama 3.3 70B Instruct
Mistral Large 2
Llama 3.1 405B Instruct
DeepSeek V3
GPT-4o
Claude 3.5 Sonnet

P@1

P@2

P@3

P@4

P@1

P@2

P@3

P@4

60.0
6.2
53.3
29.1
54.8
60.6
69.2

49.8
5.7
37.8
17.5
41.2
49.0
57.6

44.1
5.49
29.0
12.8
34.1
42.4
50.9

40.4
5.3
23.1
10.4
30.4
37.7
46.2

45.3
35.3
27.2
26.0
25.5
43.0
46.0

36.9
33.6
14.2
17.3
14.0
31.8
32.6

32.2
32.4
9.4
13.5
12.0
26.3
26.3

29.0
31.5
7.1
12.0
12.0
22.3
22.5

Table 6: Taubench Results. We follow the original experimental setup from Yao et al. (2024). Pass@k (P@k)
evaluates a model’s consistency; for example, Pass@4 is the probability that a model answers the same
question correctly 4 times. Scores are aggregated over 10 runs.

.

19

Standard RAG Benchmarks. We evaluate on several RAG benchmarks that test the model’s ability to an-
swer questions conditioned on source documents. In DROP (Dua et al., 2019) and HotPotQA-distractor (Yang
et al., 2018), the model is given a question and set of pre-retrieved relevant documents. Bamboogle (Press
et al., 2022) and StrategyQA (Geva et al., 2021) are multi-hop question answering datasets where models
must submit one or more sequential or parallel queries to a search engine to gather documents and arrive at
the answer. Finally, we show results averaged over the ten datasets in ChatRAGBench (Liu et al., 2024d)
that cover a variety of domains situated in a multi-turn conversation. Results are shown in Table 4.

Berkeley Function-Calling Leaderboard (BFCL). BFCL is one of the most widely used evaluations of
LLM tool use / function calling capabilities and maintains an independently run leaderboard (Yan et al.,
2024). Evaluations include simple single step tool calls, measures of tool irrelevance, and a multi-turn subset
which simulates much harder scenarios over long action trajectories. Results are shown in Table 5.

Taubench. Taubench is a complex agentic tool-use benchmark that simulates a customer support agent in
two settings: airline and retail (Yao et al., 2024). The agent model has access to a set of tools for reading
and writing to a provided database and must help a simulated user in accomplishing a given task such as
changing flight or returning a product order. Results are shown in Table 6.

4.3 Multilingual
Command A supports 23 key languages of global business: English, French, Spanish, Italian, German, Por-
tuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indone-
sian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian. We evaluate performance on many of these
languages (and beyond) on both academic and internal enterprise focused benchmarks, as well as public
benchmarks important for business use such as language consistency and steerability, and dialect awareness.

We assess the general multilingual capability of Command A through machine translation via NTREX-
128 (Federmann et al., 2022), which contains human translated news domain documents, FLORES-200 (Team
et al., 2022; Goyal et al., 2022); and multilingual mathematical reasoning (MGSM; Shi et al. (2022)). We
further evaluate Command A’s understanding of regional contexts through INCLUDE (Romanou et al.,
2025), a large-scale region-specific evaluation suite in 44 languages.

Results for machine translation on NTREX are shown in Table 7. We use the COMET-20 metric (Rei et al.,
2020), one of the top performing MT metrics (Freitag et al., 2023). Rather than mark single winning models,
we mark winning clusters of models by taking into account the effect size of the metric. A model is in a
winning cluster if its score difference to the best model is smaller than 1.67 points. This threshold equates
to 75% agreement with humans (Kocmi et al., 2024): humans will agree with automatic metric on 3 out of
4 pairwise system comparisons that have a difference of 1.67 COMET-20. Further academic results (MGSM
and INCLUDE-44) are in Appendix B.2.

To evaluate more general and diverse capabilities, we ran an LLM-as-judge arena-like evaluation of responses
to mArenaHard, a dataset of 500 challenging queries from Chatbot Arena, originally in English, translated
into 23 languages (Dang et al., 2024). As shown in Table 8, Command A is preferred across all 23 languages
versus Llama 3.3 70B Instruct, Llama 3.1 405B Instruct, and DeepSeek V3.

We also conduct a human-annotated arena-like evaluation. Figure 5 shows the results of an internal evaluation
set consisting of 100 translated prompts 3 from English that focus on instruction-following ability. Command
A performs favourably in multilingual head-to-head human evaluations against comparable models across 9
priority languages. Command A outperforms the Llama 3.3 70B Instruct and Llama 3.1 405B Instruct across
all evaluated languages. Versus DeepSeek V3 and Mistral Large 2, Command A is favoured across 8 of 9
languages. Notably, Command A is favoured in Chinese compared to DeepSeek V3 and is favoured in French
compared to Mistral Large 2. It is also favoured against GPT-4o on Arabic and Korean, and is competitive
on Spanish, German, Italian, and Chinese.

3Models commonly had issues generating completions for one of the prompts across different languages, the win/tie/loss

rates for these are based on 99 prompt-completion pairs

20

2
.
1
8

0
.
3
8

8
.
2
8

8
.
2
8

7
.
2
8

3
.
6
7

1
.
9
7

1
.
7
7

8
.
5
7

8
.
9
6

6
.
8
5

0
.
7
7

3
.
6
7

6
.
2
7

8
.
3
6

3
.
2
5

7
.
3
3

S
E
R
O
L
F

.
g
v
A

h
z

i
v

k
u

r
t

u
r

o
r

t
p

l
p

l
n

o
k

a
j

t
i

d
i

i
h

e
h

r
f

a
f

s
e

8
.
8
6

3
.
6
5

7
.
3
6

3
.
8
6

1
.
0
8

2
.
4
6

1
.
4
7

7
.
4
7

3
.
3
7

2
.
3
6

8
.
6
6

6
.
0
6

2
.
4
7

7
.
8
7

8
.
7
6

2
.
6
6

2
.
7
6

3
.
6
5

2
.
4
7

l
e

5
.
9
7

e
d

2
.
0
6

s
c

3
.
4
8

r
a

0
.
0
6

0
.
1
7

9
.
6
5

1
.
6
6

7
.
0
7

9
.
3
8

0
.
5
6

8
.
6
7

8
.
6
7

4
.
5
7

5
.
4
6

9
.
8
6

1
.
4
6

1
.
6
7

2
.
2
8

7
.
0
7

8
.
0
7

6
.
8
6

9
.
7
5

7
.
4
7

0
.
3
8

3
.
1
6

9
.
5
8

8
.
0
6

9
.
5
6

5
.
1
7

8
.
4
8

9
.
5
6

8
.
6
7

8
.
5
7

2
.
5
7

6
.
4
6

0
.
6
6

2
.
2
7

2
.
4
8

7
.
5
6

6
.
7
7

4
.
6
7

7
.
5
7

8
.
3
6

9
.
6
6

2
.
5
6

9
.
2
6

4
.
5
7

6
.
1
8

5
.
1
7

6
.
1
6

9
.
5
7

2
.
1
8

4
.
1
7

3
.
7
6

8
.
8
6

9
.
7
6

6
.
7
5

0
.
5
7

2
.
2
8

7
.
1
6

1
.
5
8

0
.
8
6

2
.
7
5

1
.
4
7

1
.
3
8

3
.
1
6

3
.
5
8

9
.
8
5

4
.
8
5

5
.
0
7

4
.
0
7

2
.
0
7

4
.
6
5

5
.
6
5

8
.
6
5

8
.
9
6

5
.
8
5

8
.
5
6

8
.
5
6

1
.
4
6

5
.
2
6

0
.
6
5

1
.
8
4

3
.
4
5

2
.
3
5

5
.
3
5

5
.
4
5

6
.
7
4

2
.
0
6

3
.
8
5

3
.
8
5

9
.
5
5

0
.
6
4

0
.
9
6

7
.
2
6

8
.
3
6

2
.
1
6

6
.
9
4

7
.
0
4

5
.
5
6

6
.
1
7

6
.
2
8

0
.
3
8

7
.
7
7

5
.
8
6

5
.
0
7

0
.
5
6

1
.
0
5

5
.
5
6

9
.
4
6

9
.
0
6

7
.
0
6

1
.
8
5

3
.
8
5

8
.
9
3

4
.
5
7

6
.
4
7

2
.
3
7

7
.
9
6

1
.
0
7

4
.
6
5

4
.
8
5

3
.
2
7

1
.
4
7

9
.
0
7

6
.
3
7

1
.
9
6

3
.
0
7

9
.
7
6

7
.
7
6

2
.
0
6

6
.
6
4

9
.
2
6

2
.
9
5

4
.
1
6

4
.
7
5

4
.
8
4

3
.
3
6

2
.
3
6

3
.
7
5

8
.
1
5

2
.
2
5

2
.
0
6

8
.
9
5

9
.
8
5

0
.
7
5

7
.
7
5

7
.
0
5

3
.
3
7

8
.
3
7

7
.
0
7

5
.
9
6

4
.
8
6

5
.
8
7

3
.
3
7

6
.
5
7

3
.
4
7

5
.
8
5

8
.
4
6

3
.
1
6

4
.
3
6

6
.
9
4

9
.
5
3

3
.
8
6

9
.
3
6

2
.
3
6

9
.
9
5

0
.
9
1

1
.
5
1
-

6
.
7
6

3
.
4
6

7
.
5
6

7
.
2
6

4
.
3
6

4
.
1
6

9
.
5
5

3
.
5
5

1
.
9
4

5
.
5
4

6
.
3
4

8
.
8
2

8
.
2
3

0
.
3
7

3
.
3
7

4
.
1
7

4
.
1
7

1
.
9
6

2
.
0
7

2
.
9
6

7
.
0
8

7
.
6
7

5
.
1
7

7
.
9
6

7
.
4
6

8
.
1
4

2
.
0
3

0
.
9
5

4
.
9
5

6
.
7
5

0
.
6
5

7
.
4
5

3
.
1
8

7
.
7
7

1
.
6
7

8
.
0
7

8
.
1
6

0
.
6
7

0
.
6
7

3
.
4
7

8
.
3
6

2
.
8
6

4
.
4
6

8
.
4
7

6
.
1
8

9
.
0
7

3
.
5
7

3
.
5
7

0
.
4
6

0
.
9
6

6
.
3
6

7
.
5
7

2
.
1
8

6
.
9
6

2
.
0
7

3
.
7
6

5
.
1
6

1
.
6
8

5
.
9
5

0
.
1
6

0
.
5
8

8
.
9
5

6
.
3
6

9
.
0
5

5
.
1
6

6
.
3
6

1
.
5
7

8
.
9
5

2
.
0
7

3
.
2
7

6
.
7
6

5
.
9
5

1
.
6
5

2
.
1
7

0
.
8
7

8
.
4
6

6
.
1
5

3
.
4
6

7
.
7
4

3
.
1
7

0
.
3
7

1
.
7
5

0
.
2
6

6
.
1
5

7
.
9
5

2
.
9
4

8
.
6
3

0
.
0
2

0
.
1
5

6
.
4
4

6
.
8
3

0
.
7
4

6
.
8
5

5
.
7
5

1
.
1
5

6
.
3
3

1
.
9
3

7
.
3
6

9
.
3
7

5
.
9
5

2
.
0
6

0
.
7
4

7
.
8
3

3
.
5
-

4
.
0
7

8
.
8
4

3
.
3
2

1
.
7
1

9
.
6
5

0
.
7
4

1
.
8
4

2
.
6
2

9
.
7
6

4
.
6
6

5
.
7
5

5
.
4
3

3
.
5
1

7
.
0
7

5
.
6
6

2
.
9
5

7
.
3
6

9
.
3
5

7
.
9
6

9
.
6
7

0
.
2
7

3
.
6
6

3
.
4
6

4
.
7
6

2
.
1
6

6
.
3
5

4
.
1
3

1
.
0
2

2
.
7
5

6
.
2
5

8
.
4
4

0
.
8
3

8
.
3
5

0
.
1
4

6
.
4
3

3
.
1
1

5
.
4
5

9
.
9
6

5
.
2
4

2
.
8
3

7
.
2
3

9
.
3
6

2
.
5
6

4
.
4
5

3
.
6
7

7
.
7
6

5
.
5
5

1
.
6
5

0
.
9
5

0
.
0
6

9
.
6
4

0
.
8
1

1
.
0
5

0
.
6
3

2
.
0
3

2
.
9

9
.
2
1
-

4
.
8
6
-

1
.
2
6

4
.
2
6

7
.
4
5

9
.
6
5

5
.
9
4

2
.
4
4

3
.
0
4

1
.
7
2

3
.
0
1
-

5
.
4
3
-

4
.
8
6

2
.
0
7

8
.
3
6

4
.
5
6

6
.
9
5

9
.
2
6

4
.
1
6

8
.
0
4

9
.
4
3

5
.
7
3
-

8
.
6
5

2
.
9
7

3
.
6
5

1
.
0
5

0
.
2
5

0
.
9
3

0
.
3
7

2
.
1
6

5
.
2
4

5
.
9
2

h
s
a
l
F

0
.
2

i
n
i
m
e
G

o
r
P
5
.
1

i
n
i
m
e
G

t
e
n
n
o
S

7
.
3

e
d
u
a
l
C

3
V
k
e
e
S
p
e
e
D

A
d
n
a
m
m
o
C

o
4
-
T
P
G

0
.
2
5

1
.
2
5

2
.
5
4

3
.
8
4

6
.
0
5

5
.
7
4

4
.
6
4

2
.
2
4

7
.
6
2

2
.
2
1
-

o
b
r
u
T
t
c
u
r
t
s
n
I
B
2
7

5
.
2

n
e
w
Q

t
c
u
r
t
s
n
I
B
5
0
4

1
.
3

a
m
a
l
L

t
c
u
r
t
s
n
I
B
0
7

3
.
3

a
m
a
l
L

2

e
g
r
a
L

l
a
r
t
s
i

M

B
8
-
h
s
a
l
F

5
.
1

i
n
i
m
e
G

u
k
i
a
H
3

e
d
u
a
l
C

t
c
u
r
t
s
n
I
B
9

2

a
m
m
e
G

t
c
u
r
t
s
n
I
B
8

1
.
3

a
m
a
l
L

B
7
R
d
n
a
m
m
o
C

B
8

l
a
r
t
s
i
n
i
M

7
.
2

o
b
r
u
T
t
c
u
r
t
s
n
I
B
7

5
.
2

n
e
w
Q

21

e
h
t

n
i

s
e
r
o
c
s

m
e
t
s
y
S

.
n
w
o
h
s

o
s
l
a

s
i

)
0
2
-
T
E
M
O
C
(

S
E
R
O
L
F

r
e
v
o

e
g
a
r
e
v
A

.

X
E
R
T
N

n
o

s
e
r
o
c
s

)
0
2
-
T
E
M
O
C
(

n
o
i
t
a
l
s
n
a
r
t

e
n
i
h
c
a
M

:
7

e
l
b
a
T

.
e
g
a
u
g
n
a
l

r
e
p

d
l
o
b

e
r
a

r
e
t
s
u
l
c

g
n
i
n
n
i
w

h
z

i
v

k
u

r
t

u
r

o
r

t
p

l
p

l
n

o
k

a
j

t
i

d
i

i
h

e
h

r
f

a
f

s
e

n
e

l
e

e
d

s
c

r
a

.
g
v
A

4
.
4
7

6
.
2
8

0
.
0
8

0
.
9
7

7
.
9
6

0
.
7
7

7
.
6
7

9
.
9
7

8
.
8
7

6
.
4
8

9
.
4
8

9
.
4
7

6
.
1
8

9
.
1
8

0
.
4
8

1
.
6
7

6
.
1
8

4
.
6
7

8
.
2
7

7
.
8
7

4
.
1
8

3
.
7
7

2
.
1
8

8
.
8
7

2
.
8
7

1
.
2
7

6
.
0
7

6
.
3
7

0
.
8
7

5
.
2
8

6
.
7
7

3
.
6
7

3
.
9
7

7
.
2
8

2
.
1
8

7
.
1
8

1
.
1
8

2
.
5
8

3
.
9
7

4
.
8
7

7
.
2
8

6
.
3
7

7
.
8
7

2
.
1
8

4
.
7
7

4
.
0
8

9
.
6
6

2
.
0
7

5
.
4
6

6
.
7
6

2
.
4
6

5
.
5
6

7
.
1
6

9
.
4
6

8
.
4
6

2
.
1
7

6
.
6
6

9
.
2
6

8
.
6
6

6
.
5
6

0
.
8
6

0
.
1
6

6
.
8
6

8
.
3
6

1
.
4
6

5
.
8
6

4
.
4
6

8
.
4
6

4
.
8
6

7
.
4
5

7
.
5
5

2
.
5
5

4
.
0
5

0
.
2
5

9
.
1
5

2
.
4
5

2
.
0
5

7
.
2
5

2
.
3
5

4
.
6
5

8
.
1
5

9
.
3
5

6
.
1
5

0
.
4
5

7
.
3
5

7
.
2
5

7
.
2
5

5
.
2
5

5
.
4
5

9
.
5
5

5
.
4
5

5
.
4
5

9
.
8
7

7
.
8
7

9
.
5
6

4
.
3
5

t
c
u
r
t
s
n
I
B
0
7

3
.
3

a
m
a
l
L

.
s
v
A
d
n
a
m
m
o
C

t
c
u
r
t
s
n
I
B
5
0
4

1
.
3

a
m
a
l
L

.
s
v
A
d
n
a
m
m
o
C

2

e
g
r
a
L

l
a
r
t
s
i

M

.
s
v
A
d
n
a
m
m
o
C

3
V
k
e
e
S
p
e
e
D

.
s
v
A
d
n
a
m
m
o
C

.
s
l
e
d
o
m

s
t
h
g
i
e
w
-
n
e
p
o

t
s
n
i
a
g
a

s
e
g
a
u
g
n
a
l

3
2

n
o

s
e
t
a
r
n
i
w
d
r
a
H
a
n
e
r
A
m
A
d
n
a
m
m
o
C

:
8

e
l

b
a
T

Figure 5: Head-to-head human evaluations against comparable models.

22

0.020.040.060.080.0100.0JA47548AR65.661.0133.33KO612237PT70282ES70291FR62335DE64.652.0233.33IT56341ZH58042Command A Win RatesTie RatesLoss Rates50% ThresholdCommand A vs DeepSeek V30.020.040.060.080.0100.0JA47.57.145.5AR48.03.049.0KO53.50.046.5PT53.046.01.0ES58.040.02.0FR41.01.058.0DE56.01.043.0IT53.03.044.0ZH55.644.40.0Command A Win RatesTie RatesLoss Rates50% ThresholdCommand A vs Gemini 2.0 Pro-exp0.020.040.060.080.0100.0JA69229AR75322KO76123PT58402ES58.5933.338.08FR67132DE60238IT63.642.0234.34ZH70.7128.281.01Command A Win RatesTie RatesLoss Rates50% ThresholdCommand A vs Llama 3.3 70B0.020.040.060.080.0100.0JA64630AR69.233.8526.92KO73.21026.79PT65341ES58.5935.356.06FR58042DE69130IT58.593.0338.38ZH55441Command A Win RatesTie RatesLoss Rates50% ThresholdCommand A vs Llama 3.1 405B0.020.040.060.080.0100.0JA55243AR67.682.0230.03KO62533PT59.639.391.01ES48493FR55243DE55.562.0242.42IT54.55045.45ZH67.6830.032.02Command A Win RatesTie RatesLoss Rates50% ThresholdCommand A vs Mistral Large0.020.040.060.080.0100.0JA37558AR51049KO51049PT39574ES444412FR41.415.0553.54DE47251IT46252ZH47530Command A Win RatesTie RatesLoss Rates50% ThresholdCommand A vs GPT-4o (Nov)Command A
GPT-4o
Gemini 1.5 Pro
Gemini 2.0 Flash
Mistral Large 2

MTaubench Retail

MTaubench Airline

Avg.

34.3
37.3
26.4
26.0
25.8

en

60.0
59.7
49.0
44.1
54.1

fr

36.5
41.7
28.4
29.6
30.0

ar

28.5
29.6
20.3
20.6
18.7

ja

24.4
28.7
16.2
17.1
11.8

ko

Avg.

en

fr

22.0
26.7
18.8
18.8
14.4

43.4
45.2
41.4
33.7
27.0

45.3
47.3
31.7
35.3
28.6

52.7
38.7
36.9
36.0
32.0

ar

47.3
50.7
46.0
34.0
30.9

ja

38.0
41.3
47.6
29.3
21.6

ko

33.8
48.0
45.0
34.0
21.9

Table 9: Multilingual Taubench Results: We follow the original experimental setup from Yao et al.
(2024). We report the per language pass@1 (P@1) score. Scores aggregated over 3 runs.

Command A
Command R+ Refresh
Qwen 2.5 72B Instruct Turbo
Claude 3.7 Sonnet
Llama 3.3 70B Instruct
Gemini 1.5 Pro
DeepSeek V3
GPT-4o
Mistral Large 2

Avg

93.0
95.5
93.0
91.8
91.3
90.6
90.6
88.9
75.9

ar

de

es

fr

hi

id

it

ja

ko

pt

ru

tr

vi

zh

98.2
94.8
96.4
94.5
90.5
91.7
94.9
92.2
85.3

95.5
98.2
94.0
95.5
94.7
94.4
93.8
91.0
64.7

94.8
97.2
94.3
93.2
92.9
94.5
94.2
94.9
83.4

93.5
97.2
93.7
94.2
93.0
92.0
93.5
91.7
78.3

94.6
96.5
94.1
93.6
98.2
93.5
92.2
91.9
86.9

84.2
89.0
86.6
78.9
92.1
82.9
82.6
80.9
65.0

94.9
97.5
94.0
94.0
93.3
93.3
92.8
90.4
69.6

93.2
96.2
91.3
93.6
83.3
86.1
85.5
85.3
82.6

93.4
94.7
93.0
93.9
78.9
90.6
91.5
87.4
76.1

89.6
91.6
87.9
84.1
91.2
86.7
85.3
87.0
75.2

93.7
96.9
95.8
95.9
95.0
93.4
92.9
87.9
75.6

93.6
97.8
95.4
92.6
92.9
94.8
92.3
90.7
69.3

92.2
98.3
95.7
95.3
95.9
95.6
91.8
88.0
73.1

90.3
90.6
89.2
86.2
86.5
79.1
84.3
85.5
77.2

Table 10: Crosslingual line-level pass rate (LPR) from the Language Confusion Benchmark (Marchisio et al.,
2024). Models are prompted in English with an instruction to reply in a different language. LPR measures
the percentage of answers with all lines in the requested language.

Command A
Gemini 1.5 Pro
GPT-4o
Claude 3.7 Sonnet
DeepSeek V3
Llama 3.3 70B Instruct
Qwen 2.5 72B Instruct Turbo
Mistral Large 2
Command R+ Refresh

Monolingual

Crosslingual

24.2
19.3
15.8
8.5
15.7
15.2
9.9
6.9
1.9

33.5
26.4
24.7
23.1
15.7
8.3
9.6
7.9
6.1

Table 11: ADI2 score over monolingual and crosslingual prompts in 4 Arabic dialects (Egyptian, Saudi,
Syrian, Moroccan) from Robinson et al. (2024). Higher scores indicate greater desired dialect adherence.

Beyond instruction-following, agentic capabilities are important for enterprise use. We evaluate Command
A on our own human translated version of τ -bench (Yao et al., 2024).4 As shown in Table 9, Command A
outperforms other widely-adopted LLMs agentic solutions such as Mistral Large 2 and Gemini 1.5 Pro, while
being competitive with GPT-4o.

The Language Confusion Benchmark (Marchisio et al., 2024) measures a model’s ability to appropriately
respond in the desired language of the user. In Table 10, we measure line-level pass-rate (LPR) on crosslingual
prompts. Concretely, models are prompted with an English request and an instruction to reply in another
language. LPR is the percentage of responses where all lines were in the user’s desired language. Command
A and its predecessor, Command R+ Refresh, perform very strongly across languages, with the highest and
second highest aggregate scores.

We measure Command A’s sensitivity to regional dialect in Table 11, which shows ADI2 scores over mono-
lingual and crosslingual prompts in 4 Arabic dialects (Egyptian, Saudi, Syrian, Moroccan) from Robinson
et al. (2024). Higher scores indicate more adherence to the desired Arabic dialect. We observe that Command

4The number may differ slightly from the official implementation due to extensions for our multilingual evaluation pipeline.

23

A strongly outperforms comparison models in its ability to adhere to dialect.

4.4 Code
We evaluate the code capabilities of Command A across code understanding, code editing, and SQL
generation benchmarks.

Python

Multi-language

COBOL

MBPP+ LiveCodeBench BigCodeBench LBPP(All) HE(All) HE →Python

RepoQA

Command A

Command A Expert

Command A Agentic

Command R7B

Command R Refresh

Command R+ Refresh

86.2

87.0

—

72.0

74.3

78.8

Llama 3.3 70B Instruct

86.0 / 81.0

Mistral Large 2

Qwen 2.5 72B Instruct

84.7

88.6

Llama 3.1 405B Instruct

88.6 / 87.0

DeepSeek V3

90.0

26.9

24.9

32.9

9.0

11.0

14.4

32.9

26.7

26.3

29.3

33.5

45.4

47.4
59.7∗

30.9

34.3

25.8

46.9 / 41.9

44.7

45.8 / 43.6

46.2

50.0 / 48.6

51.5

50.8

65.4

21.9

24.7

25.6

47.8

54.0

48.3

52.7

61.5

25.3

29.8

55.7

64.6

76.2

77.5

—

50.7

54.7

54.4

75.5

—

7.0

1.9

2.5

3.2

82.9

10.8

78.5

76.7

6.3

3.2

83.5

15.2

—

35.4

34.2

43.7

46.2

46.8

55.7

59.5

63.3

92.6

91.8

—

69.6

73.2

77.0

85.6

88.0

83.2

90.4

92.2

Table 12: Code Understanding Benchmarks across Python, Multi-language, and COBOL groups re-
porting 1-shot pass@1 and RepoQA reporting match accuracy. HE is HumanEval. All results are internal
reproductions using an identical prompt except where ‘/’ indicates external value first and internal repro-
duction second. Best score ±1% is bolded. ∗For BigCodeBench, we use 3 tool-use execution feedback tests.

Command A
Command A Expert

Command R7B
Command R Refresh
Command R+ Refresh
Llama 3.3 70B Instruct
Mistral Large 2
Qwen 2.5 72B Instruct
Llama 3.1 405B Instruct
DeepSeek V3

SWE-Bench Verified

Aider Polyglot

26.8
23.4

3.6
11.6
17.0
29.4
30.0
33.0
33.4
42.0 / 45.8

14.7
8.9

2.7
1.8
2.2
8.4
16.0
8.0
13.8
49.6 / 51.6

Table 13: Code Editing Benchmarks. All results are internal reproductions using an identical prompt
except where ‘/’ indicates externally reported value first and internal reproduction second.

Code Understanding evaluates code generation across multiple languages. For Python generation, we
report on MBPP+ (Austin et al., 2021; Liu et al., 2024c), LiveCodeBench (Jain et al., 2024, Version 5
10/24-2/25), BigCodeBench (Zhuo et al., 2024, Instruct), and RepoQA (Liu et al., 2024b, 32K context length,
threshold 0.8). For multi-language generation, we report HumanEval (Chen et al., 2021; Muennighoff et al.,
2023) scores in Python, C++, Java, Javascript, Go, and Rust. We also extend our earlier uncontaminated
Python benchmark, Less Basic Python Problems (Matton et al., 2024, LBPP), with parallel versions in
C++, Java, Javascript, Go and Rust for uncontaminated generation evaluation across enterprise-critical
programming languages.5

To assist in future advancements in COBOL understanding, we also develop a parallel version of HumanEval
in COBOL (i.e., HumanEval-COBOL). We evaluate direct generation of COBOL, and translation of COBOL

5We will release this dataset in an update to huggingface.co/datasets/CohereForAI/lbpp

24

Command A

Command A Expert

Command R7B

Command R Refresh

Command R+ Refresh

Llama 3.3 70B Instruct

Mistral Large 2

Qwen 2.5 72B Instruct

Llama 3.1 405B Instruct

DeepSeek V3

Spider

Bird

Internal

Dev

79.5

85.5

78.1

76.5

82.0

81.1

78.8

83.5

83.0

81.7

Test

Dev

Avg.

SQLite

PostgreSQL MySQL

PL/SQL T-SQL

80.2

85.4

77.6

78.1

81.7

84.8

76.3

83.8

59.5

58.5

42.2

47.3

52.7

58.0

50.0

50.1

86.7

59.4

55.3

56.1

34.4

42.8

44.4

45.9

53.3

53.7

49.2

48.7

49.3

27.3

36.7

40.7

41.3

54.0

52.7

54.0

58.0

60.0

36.0

48.0

47.3

48.0

54.7

54.7

58.7

56.0

55.3

34.7

42.7

40.0

43.3

50.7

56.7

50.7

58.7

58.0

38.7

43.3

52.0

50.0

53.3

49.3

34.0

55.3

58.0

35.3

43.3

42.0

46.7

54.0

55.3

48.7

81.7

53.1

60.8

56.7

66.0

60.7

58.7

62.0

Table 14: SQL Generation Benchmarks reporting execution accuracy against gold databases. All results
are internal reproductions using an identical prompt. Avg. is the sample-weighted average across internal
multi-dialect evaluation datasets. Best score ±1% is bolded.

to Python similar to Muennighoff et al. (2023). The translation setting tests model capability to update
legacy codebases into a modern language.

Code Understanding metrics are outlined in Table 12. Sources of externally reported values are in Ap-
pendix B.3. Command A provides strong Python and multi-language performance compared to similar and
larger models. Table 26 details the complete performance for HumanEval and LBPP in all languages, high-
lighting competitive accuracy in many business-critical programming languages. In the hardest benchmarks,
LiveCodeBench and BigCodeBench, Command A surpasses many competitors and can be further improved
with agentic tool-use discussed below. Command A also leads in RepoQA performance compared to all
competitors. Finally, Command A offers state-of-the-art capabilities in COBOL for both direct generation,
via HumanEval-COBOL, and translation from HumanEval-COBOL to HumanEval-Python. These strengths
highlight that Command A offers accurate code understanding in the complex environment of navigating
legacy enterprise codebases.

We also investigate the performance of Command A as a code agent using multi-hop tool use similar
to the setup for RAG in Section 4.2. Command A can now access a code execution tool and receives
feedback on code generation via execution results from gold-standard unit tests similar to Gehring et al.
(2025). We evaluate 3 datasets in this regime: LiveCodeBench, BigCodeBench, and LBPP(all languages). In
LiveCodeBench, we use the public unit tests for execution feedback and private tests for final evaluation.
For BigCodeBench and LBPP, we simulate the unit-test split by using 3 unit tests for execution feedback
and all remaining tests for final evaluation.6 Table 12 shows how using Command A as an agent easily
surpasses direct code generation across all datasets—achieving a pass@1 gain over Command A of +5.9% for
LiveCodeBench, +14.3% for BigCodeBench, and +12.3% for LBPP across all languages. Notably, Command
A achieves 71.4% in LBPP-Python surpassing all competitors by 4.3% and surpasses all other models in the
BigCodeBench leaderboard at the time of publication.7

Code Editing evaluates the model capability to generate precise code line-level changes to edit and update
a codebase. We evaluate our models on the SWEBench Verified Patch Generation task in Python (Jimenez
et al., 2024), and the Aider Polyglot benchmark8 for multi-language code editing in Python, C++, Java,
Javascript, and Rust. Table 13 demonstrates Command A is competitively capable in repository-level under-
standing and solving pull-requests or building code fixes via patch generation. We note that these results are
from post-hoc investigations into code-editing behaviour in our model as we did not target these functions

6As the prompt design for LBPP includes 3 unit-tests, this setup does not leak any further testing requirements to the

model.

7The current best model is GPT-4o-2024-05-13 with 51.1 pass@1 https://bigcode-bench.github.io/
8aider.chat/2024/12/21/polyglot.html

25

Figure 6: Code Performance against Model Size. Command A provides state-of-the-art performance
compared to models of similar size, and often significantly larger models. Command A Agent improves even
further to set a new standard for performance at 111B size with tool-use in code.

in developing Command A. Similar to our investigation into a code agents described above, we share these
results as early signposts for future objectives of code expert development.

SQL Generation evaluates model capability in understanding user requests using a partially observed
database context. Understanding SQL and reasoning with databases is critical for Command A to succeed
as an enterprise model. We evaluate SQLite performance using Spider (Yu et al., 2018, Dev & Test) and the
more recent Bird SQL benchmark (Li et al., 2023a, Dev). To ensure Command A can accurately generate SQL
in an enterprise database context, we also report results for an internal benchmark in SQLite, PostgreSQL,
MySQL, Oracle PL/SQL, and Microsoft T-SQL. Performance on these dialects better reflects real usage of
SQL to access commercial database systems.

Table 14 demonstrates that Command A offers state-of-the-art performance across multiple datasets. Com-
mand A leads in both Spider Dev, and Bird to provide accurate SQL generation to solve challenging queries
in even “dirty” database contexts. Across models of similar size, Command A also demonstrates the strongest
average performance across 5 enterprise-critical SQL dialects in our internal benchmark. This further punc-
tuates the capability of Command A in both academic and enterprise scenarios for SQL.

We highlight the performance benefit of Command A relative to size in Figure 6. Across 3 datasets, Command
A and Command A Code Expert provide best-in-class performance, often surpassing similar and larger
models. Command A offers a unique trade-off for enterprise capability in accurate code and SQL generation.
Using Command A as an agent for code further enhances the model for state-of-the-art capabilities across
challenging benchmarks.

4.5 Math and Reasoning
We evaluate the reasoning capability of our model on key mathematical reasoning benchmarks, and compare
this to publicly-reported metrics (where available) in Table 15. We find that Command A performs especially
well on mathematical benchmarks, and that merging models preserves reasoning performance (compared to
reasoning-expert models) within a few percentage points across most benchmarks (§4.9).

26

20.0%10020030040050060070040.0%30.0%50.0%70.0%60.0%LBPP AllDatasetModelBigCodeBenchBird SQLCode Performance Against Model SizeSize (#Billion Params)Command R RefreshCommand A 111B ExpertCommand R+ RefreshCommand A 111BCommand A 111B AgenticLlama 3.3 70B InstructDeepSeek V3Qwen 2.5 72B InstructMistral Large 2Llama 3.1 405BPerformance (pass@1 / execution acc.)Command A
GPT-4o
Llama 3.3 70B
Llama 3.3 405B
Mistral Large 2

MATH (all)

AIME (2024)

GPQA (Diamond)

80.0
68.5
77.0
73.9
71.3∗

23.3
9.3
20.0∗
20.0∗
11.0

50.8
46.0
50.5
49.0
48.6

Table 15: Reasoning performance of Command A compared to similarly-sized models. Benchmarks are MATH
(Hendrycks et al., 2021), the 2024 AIME mathematics competition, and GPQA Diamond (Rein et al., 2023).
Results for external models are taken from officially-reported sources, unless indicated with an asterisk (*),
which denotes internal evaluation since official public results were not available.

In our qualitative assessments, we also find that reasoning-expert models provide generalised gains in coding
and structured data manipulation tasks, and that these are additive in the final Command A model.

4.6 Safety
Our safety evaluation methodology combines human and automated assessments. Due to speed and cost
considerations, we mainly rely on automated evaluations. We use human annotations as a baseline to ensure
our automated evaluations align with human judgment. These are triply annotated by an internal team of
specialist safety annotators. To further strengthen the reliability of our automated evaluation, we assess the
suitability of evaluators based on their robustness to artifacts (Chen & Goldfarb-Tarrant, 2025).

We measure both absolute and relative safety. Absolute safety evaluation tests models with potentially
eliciting prompts from the categories of our core safety behaviour (§3.3.7), and then computes the rate of
unsafe content in the model output, using an LLM-as-a-judge setup. The absolute safety aggregate score is
the average of each categorical rate, where each category is weighted equally. Relative safety evaluation uses
the same prompts, but considers how the safety of each response compares to the safety of another model’s
response for the same prompt. If both responses are equally safe, the higher quality response is chosen as
the winner. Relative safety is more challenging, so we rely on a jury of LLM evaluators (Verga et al., 2024),
which achieves human agreement scores of 77.7% and Cohen’s Kappa of 0.55 in relative safety evaluations.

We also measure over-refusal rate; how frequently models refuse to answer a prompt that should be an-
swered. These prompts fall into two categories: word sense disambiguation and requests for information
about safety topics. We use an LLM-as-a-judge setup, as we find refusal classification a much easier task
than safety, with very high accuracy and human agreement

4.6.1 Enterprise Safety
In enterprise contexts, safety priorities and risk assessments differ significantly from those in default contexts.
We consider two elements that are of strong concern for Enterprise usage: Controllability, the ability of
the model to be customised for different safety needs, and Demographic Fairness, the robustness of the
model to demographic perturbations in tasks involving real human data.

4.6.1.1 Controllability
In Enterprise Safety, the notion of safety itself is context-dependent. Some core safety behaviour is consistent
across all contexts (§3.3.7.1), but much of it varies between different deployments. The boundaries of content
that an LLM should generate when used as an LLM-editor for a journalist are very different than the content
boundaries of a customer service chatbot. Therefore, we evaluate the model’s ability to accurately condition
on different safety instructions, under our two safety modes: contextual and strict (§3.3.7.1). For each mode
we compose two evaluation sets: one that should always be answered (over-refusal evaluation) and one that
should always be refused (safety mode control), which allows us to optimise the trade-off between these two
scenarios.9 Safety mode accuracy is the mean of these sets for a given mode.

9We note that the over-refusal evaluation set was created by red-teaming Command R+ Refresh.

27

Figure 7: The Pareto frontier between correctly answering and refusing for our enterprise safety modes.

Figure 7 shows that the Command A model is on the Pareto frontier between answering and refusing for both
safety modes. Results for competitor models can be found in appendix Table 27. Each competitor targets
different markets and behaviours, so we consider different modes to have effectively different competitors. In
contextual mode, the relevant competitors are Mistral Large 2, Qwen 2.5 72B Instruct and Llama 3.3 70B
Instruct, while in strict mode the relevant competitors are GPT-4o and Claude 3.5 Sonnet.

4.6.1.2 Demographic Fairness
LLMs are used in various hiring software systems in the market, and we evaluate demographic fairness in
this context. The model is tasked with summarising the suitability of resumes with respect to a given job
description. We follow Seshadri & Goldfarb-Tarrant (2025) for both our method and our metric. We permute
the demographics of the resume and measure meaningful differences in generated summaries for candidates
when their race or gender has changed. A perfect model would have no meaningful differences, i.e. would be
invariant to the perturbation. The bias metric is defined as the proportion of measurements (including reading
ease, subjectivity and regard, as outlined in Seshadri & Goldfarb-Tarrant (2025) for which the null invariance
hypothesis is rejected when comparing the original and perturbed summaries. To account for variability in
generations (Chen & Goldfarb-Tarrant (2025) observed this even at temperature 0), we generate responses
using each model five times per sample and plot the distribution of bias rates across all runs. The results
for gender and race are shown in Figure 8. We report with both Bonferroni (bonf) and Benjamini-Hochberg
(bh) corrections to account for the multiple measurements on the same summaries and to allow the reader
to select whichever correction is more applicable – bonf to minimise false positives (finding a demographic
fairness issue when there is none), and bh to minimise false negatives.

We note two broad patterns across all models: models tend towards much stronger racial bias than gender
bias, and smaller models tend to have greater bias than larger models. In particular, the Command A
models show impressive robustness to demographic perturbations. Command A is entirely robust to gender
perturbations and very resilient to race ones (only 1% failures). Command R7B similarly is entirely robust to
gender in this evaluation, and competitive for a small model at robustness to race, with around 4% failures.

We don’t observe significant gender bias for large models in this domain in our testing setup. Command A,
Llama 3.3 70B Instruct, and Mistral Large 2 all exhibit minimal racial bias, each failing a median of 1% of
invariance tests, while Claude 3.5 Sonnet has the lowest, at 0%. Small models are significantly less robust.

28

75020406080100858090100957502040608010085809010095Llama 3.1 405BCommand AMistral LargeQwen 2.5 72BGPT-4oLlama 3.3 70BDeepSeek V3Claude 3.5 SonnetCommand R+ RefreshStrict ModeCorrectly AnsweringCorrectly RefusingCorrectly RefusingCorrectly AnsweringContextual ModeFigure 8: Boxplots of gender and racial bias rates in model-generated resume summaries for Command A
(left) and Command R7B (right) compared to similarly sized models, respectively, using either Bonferroni
or Benjamini-Hochberg correction. The Command A models show impressive robustness to demographic
perturbations. Command A is robust to gender perturbations and very resilient to race ones (only 1%
failures). Command R7B similarly is robust to gender in this evaluation, and competitive for a small model
at robustness to race, with around 4% failures.

Most small models remain robust to gender, with the exception of Llama 3.1 8B Instruct and Ministral 8B,
which fail 1-5% of invariance tests. Interestingly, Ministral 8B lacks robustness to gender, but is robust to
race, whereas Mixtral lacks robustness to race, but is robust to gender.

Overall, our models offer excellent coverage of robustness across different demographic categories, for multiple
sizes. We note that, though generation does contribute, total demographic fairness in a hiring pipeline is
dominated by the retrieval stage (Seshadri & Goldfarb-Tarrant, 2025). Here we measure only the generation
stage, but our embedding model for the retrieval stage is also the most robust to perturbations.

4.6.2 Default Safety
In the default setting, we evaluate the safety of the model without a system preamble to simulate cases
outside of Cohere’s API or enterprise contexts.

Command A shows strong performance in various categories of unsafe content. As shown in Figure 9, Com-
mand A significantly outperforms all competitors in relative safety evaluations. Additionally, it attains an
absolute safety score of 70.4%, ranking third among large models, closely following Claude 3.5 Sonnet and
Qwen 2.5 72B Instruct (Table 16). It excels at avoiding violence and hate speech, with a 89.7% safe response
rate, and performs well in areas such as not generating CSEA (87.5%) and not promoting misinformation
(67.9%) (Figure 15). While Command R7B shows lower overall performance, it still maintains a notable
presence in certain categories, such as avoiding violence and hate speech (76.3%) and not promoting CSEA
(67.0%) (Figure 16). These results highlight the effectiveness of Command A in mitigating unsafe content
generation even in situations where we cannot add system preamble guardrails.

Although the relative and absolute safety performance of Command A may initially seem contradictory, this

29

0.00.40.20.61.21.00.8GenderMistral LargeLlama 3.3 70BGPT-4oCommand AClaude 3.5 SonnetBonferroniBenjamini-Hochberg012543GenderCommand R7BCommand R RefreshMistal 8BMistral 8x7BLlama 3.1 8BGemma 2 9B02486RaceMistral LargeLlama 3.3 70BGPT-4oCommand AClaude 3.5 Sonnet02.55.020.017.515.012.510.07.5RaceCommand R7BCommand R RefreshMistal 8BMistral 8x7BLlama 3.1 8BGemma 2 9B(a) Large Models

(b) Small Models

Figure 9: Default relative safety performance. Winner is assigned by a panel of LLM judges. When
both responses are equally safe, the winner is chosen based on which response is higher quality.

Relative
Safety(↑)

Absolute
Safety(↑)

Misinfo(↑)

Self-
harm(↑)

CSEA(↑)

Sexual
Content(↑)

Violence &
Hate(↑)

Command A
Claude 3.5 Sonnet
DeepSeek V3
GPT-4o
Llama 3.1 405B
Llama 3.3 70B
Mistral Large 2
Qwen 2.5 72B
Command R+ Refresh

Command R7B
Gemma 2 9B
Llama 3.1 8B
Qwen 2.5 7B
Command R Refresh

49.5
26.4
23.7
26.6
15.9
14.9
22.0
32.4
16.3

49.7
81.3
35.9
59.2
30.4

70.4
80.0
49.7
65.6
41.8
40.5
45.7
71.4
30.2

58.2
87.3
63.6
71.5
31.3

67.9
76.9
50.0
76.9
42.3
50.0
57.7
61.5
42.3

50.0
76.9
57.7
65.4
38.5

61.2
90.4
37.0
69.9
28.8
42.5
37.0
60.3
21.9

50.7
82.2
58.9
50.7
26.0

87.5
98.5
74.1
33.7
63.0
63.0
74.8
86.3
45.9

67.0
94.8
60.7
71.9
37.8

63.1
94.4
34.8
95.5
34.3
6.7
8.0
91.6
5.1

47.2
85.4
66.3
87.6
3.9

89.7
93.1
74.0
84.4
62.2
61.8
71.0
90.0
47.3

76.3
96.9
74.4
82.1
50.4

Table 16: Default safety performance of Command A and Command R7B compared to similarly sized
models across various categories of unsafe content. Relative safety is the winrate vs. Command A. Absolute
safety score is computed as an average of safe response rates for all categories. Large models are shown in
the top half of the table, while small models are shown in the bottom half. The top performing model for
each size category is bolded in each column. As indicated by the upwards-pointing arrows, higher winrates
and higher safe response rates correspond to better performance for each competitor.

occurs because the relative safety evaluation considers the intersection of safety and quality. Critically, in
the event that both models provide a safe response, the relative safety evaluations then consider the winner
to be the model that provides a higher quality response. Rather than simply refusing to answer, Command
A engages meaningfully with queries that relate to potentially unsafe topics. Many other models, such as
Claude 3.5 Sonnet, provide non-specific refusals.

We also measure over-refusal rates for the default setting on the XSTest benchmark (Röttger et al., 2024).
Command A shows refusal rates under 3%, which is considerably better than other closed-source models,
namely Claude 3.5 Sonnet and GPT-4o; and marginally better than open-access models such as Llama 3.3

30

0.020.040.060.080.0100.0Command A Win RatesCompetitor WinClaude 3.5 SonnetDeepSeek V3GPT-4oLlama 3.1 405BLlama 3.3 70BMistral LargeQwen 2.5 72BCommand R+0.020.040.060.080.0100.0Command R7B Win RatesCompetitor WinGemma 2 9BLlama 3.1 8BQwen 2.5 7BCommand RCommand A
Claude 3.5 Sonnet
DeepSeek V3
GPT-4o
Llama 3.1 405B
Llama 3.3 70B
Mistral Large 2
Qwen 2.5 72B
Command R+ Refresh

Command R7B
Gemma 2 9B
Llama 3.1 8B
Qwen 2.5 7B
Command R Refresh

XSTest Refusal (↓)
Partial
Full

Over-Refusal (↓)

XSTest

Internal

1.1
3.6
0.8
5.6
1.2
1.2
0.8
0.4
3.6

4.0
2.8
6.4
0.8
3.2

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
1.2

0.8
4.0
0.0
0.0
0.0

1.1
3.6
0.8
5.6
1.2
1.2
0.8
0.4
4.8

4.8
6.8
6.4
0.8
3.2

7.1
4.1
1.2
7.1
2.4
3.5
2.4
1.2
5.3

11.8
24.1
8.8
9.4
2.4

Table 17: Over-refusal rate of Command A and Command R7B, based on the XSTest benchmark, which
distinguishes between full refusal and partial refusal. We sum both to obtain the over-refusal rate.

Figure 10: Safety scores (rate of safe responses averaged over Misinformation and Violence and Hate) across
9 languages. The dashed lines show the English score.

70B Instruct (see Table 17). As XSTest is saturated, we also report default over-refusal based on our internal
test set from red-teaming our previous model.

4.6.3 Multilingual Safety
We evaluate safety on nine priority languages (results in Figure 10). The safety score is the rate of safe
responses measured on the same set of prompts across all languages, thus allowing for direct comparisons.
This set is the Misinformation and Violence and Hate categories from English, translated automatically,
then corrected and naturalised by multilingual annotators. We use LLM-as-a-judge evaluation. For some
languages, completions are translated into English before being evaluated, and some are retained in the
original language, based on which showed the best performance on a development set.

We also evaluate over-refusal via a prompt set collected through red teaming with the multilingual annotators.
The English over-refusal set was translated into each language, then refined and augmented with more natural

31

0.00.20.10.60.50.40.30.80.7ArabicJapaneseKoreanChineseFrench
LanguageSpanishItalianCommand R+ RefreshLlama 3.3 70BLlama 3.1 405BDeepSeek V3Mistral LargeCommand APortugueseGermanprompts. Command A is on the Pareto frontier between safety and over-refusal in 5 languages (Japanese,
Chinese, German, Korean and Arabic), whilst remaining competitive in all other measured languages.

4.7 Enterprise-specific Benchmarks
Our enterprise evaluation process focuses on testing model capabilities in generative and retrieval-augmented
use cases that mirror typical enterprise applications.

Generative use cases. We use rule-based checks and LLM-as-a-judge evaluations due to the complexity
and nuances of the differing enterprise use cases. We show an example prompt for reference in Figure 11.

Example Prompt

Create a job ad for the role of Social Media Manager at the BuzzMedia, located in New York, NY.
The ad should be creative and engaging to attract the best talent. It should include a catchy and
creative title, a brief role overview, and a list of at least 5 employee benefits offered by the company.
The overview should be at least 50 words.
Format the response as a JSON object in the following format.
{

"title": <title>,
"role_overview": <role_overview>,
"employee_benefits": [

<Benefit 1>,
<Benefit 2>,
<Benefit 3>,
<Benefit 4>,
<Benefit 5>

]

}

Figure 11: Example prompt for an enterprise generative use case.

We break down success criteria into rule-based and LLM-based checks:

• Rule-Based Checks. Checks for attributes like word count, valid JSON output, count of expected

items, and similar.

• LLM-Based Checks. We use a panel of judges, similar to the approach described in Verga et al.

(2024), to evaluate more nuanced criteria like tone and natural language quality.

The final per-example score is an average of the rule- and LLM-based scores. Our generative enterprise
benchmark consists of 22 tasks covering use cases including chat and meeting summarisation, information
extraction, and FAQ generation. A summary of the results across all tasks can be found in Table 18. Command
A achieves the highest pass rate at 94.2% across all generative use cases. Command R7B scores 71.9%, which
is the highest performance among similarly-sized models.

RAG use cases. For enterprise RAG evaluation tasks, we assess question-answering use cases involving
technical documentation and workplace policies, including internal rules and company benefits. These tasks
often involve user queries on long document snippets that can exceed 10,000 tokens. Some questions require
synthesizing information from multiple snippets, while others cannot be directly answered using the given
documents. Our evaluation set includes ground-truth answers annotated by humans.

To assess the performance of our models, we use two key evaluation metrics:

• Correctness: Measured using Llama Index Correctness, this evaluates the validity of a model’s re-
sponse. It ensures that generated answers align with the provided context and are factually correct.
• Answerability Accuracy: Assessed by LLM judges, this metric measures the model’s ability to

discern between answerable and unanswerable questions.

32

Model

Command A

Command R+ Refresh

Claude 3.5 Sonnet v2

DeepSeek V3

GPT-4o

Llama 3.3 70B Instruct

Llama 3.1 405B Instruct

Command R7B

Gemma 2 9B

Llama 3.1 8B

Enterprise Pass Rate (%)

94.2

87.4

84.2

81.3

79.1

65.2

77.8

71.9

65.7

60.4

Table 18: Enterprise generative evaluation performance across all 22 tasks.

Model

Command A

Command R+ Refresh

Claude 3.5 Sonnet v2

DeepSeek V3

GPT-4o

Llama 3.3 70B Instruct

Llama 3.1 405B Instruct

Command R7B

Gemma 2 9B

Llama 3.1 8B

Workplace Policies QA

Technical QA

4.59

4.08

4.63

4.52

4.50

4.45

4.41

4.16

3.23

3.99

4.86

4.42

4.81

4.64

4.81

4.78

4.62

4.48

4.66

4.37

Avg.

4.73

4.25

4.72

4.58

4.66

4.61

4.52

4.32

3.95

4.18

Table 19: Enterprise RAG evaluation performances of Command A models. The table shows the LLama
Index Correctness metric which ranges from 1-5. Best performances are marked in bold.

Model

Command A

Command R+ Refresh

Claude 3.5 Sonnet v2

DeepSeek V3

GPT-4o

Llama 3.3 70B Instruct

Llama 3.1 405B Instruct

Command R7B

Gemma 2 9B

Llama 3.1 8B

Answerable Acc. (%)

Unanswerable Acc. (%)

96

78

89

93

94

95

94

86

90

88

91

95

92

86

88

87

90

76

90

91

Table 20: Enterprise RAG Answerable and Unanswerable Accuracy.

These metrics ensure that the model generates accurate responses while demonstrating robust judgement in
handling questions beyond the scope of the provided context. Table 19 shows the LLama Index Correctness
performance on RAG use cases and Table 20 shows the Answerable and Unanswerable Accuracy across tasks.
The accuracy across answerable and unanswerable questions is typically a trade-off as we do not want the
model to over-refuse when the question is actually answerable, given the context.

Command A has the best Llama Index Correctness Average at 4.73, Answerable Accuracy of 96% and an
Unanswerable Accuracy of 91%, indicating that it responds when intended, while keeping hallucination rates
low for unanswerable questions. Command R7B has the best Llama Index Correctness Average score at 4.32

33

compared to models of similar size, an Answerable Accuracy of 86% and an Unanswerable Accuracy of 76%.

4.8 Long-Context Benchmarks
To assess long-context understanding capability, we employ two extensive long-context benchmark datasets:
RULER (Hsieh et al., 2024) and LongBench-V2 (Bai et al., 2025). RULER comprises 13 distinct tasks,
including retrieval, question-answering, multi-hop tracing and aggregation tasks. It is designed to evaluate a
model’s ability to retrieve and reason over longer context inputs. The evaluation is conducted on sequences of
up to 256k tokens. LongBench-V2 includes a diverse set of question-answering tasks spanning various levels
of difficulty and multiple context types, such as single- and multi-document contexts, multi-turn dialogues,
code repositories, and long structured data.

4k

8k

16k

32k

64k

128k

256k

Avg

wAvg. wAvg.

Command A

Mistral Large 2

Llama 3.1 70B

Command R+ Refresh

GPT-4o(11-20)

Claude 3.5 Sonnet (10-22)

Gemini-1.5-Pro (002)

Gemini-2.0-Flash (exp)

97.2

96.4

96.5

96.0

97.0

96.5

96.2

96.0

96.9

96.3

95.8

95.1

92.1

96.0

96.0

96.0

96.7

95.3

95.4

94.0

89.0

95.7

96.0

95.1

95.9

94.0

94.8

92.4

88.8

95.0

95.8

95.7

93.3

85.9

88.4

85.4

88.4

95.2

93.8

93.7

90.0

48.1

66.6

64.6

-

93.8

91.7

86.0

84.6

-

-

-

-

-

91.6

79.7

(≤128k)

95.0

86.0

89.6

87.9

-

95.4

94.9

93.8

(inc)

93.9

79.5

85.5

83.4

-

95.0

94.2

92.4

(dec)

96.1

92.5

93.7

92.4

-

95.8

95.6

95.1

Table 21: Results on the RULER long context benchmark. Not all models support contexts up to 256k.

Command A

Mistral Large 2

Llama 3.1 70B

Llama 3.1 405B

Command R+ Refresh

GPT-4o(11-20)

Claude 3.5 Sonnet (10-22)

Overall

Short

Medium

Long

Easy

Hard

43.4

34.4

31.8

37.8

27.8

46.0

40.7

44.1

41.7

37.2

39.7

36.7

42.9

44.9

47.4

30.7

28.8

39.0

23.7

52.1

41.3

34.3

29.6

28.7

32.1

21.3

35.3

31.6

45.3

38.0

35.4

39.0

30.2

52.1

45.8

42.3

32.2

29.6

37.0

26.4

42.2

38.6

Table 22: LongBench-V2 results for Command A.

Tables 21 and 22 highlight Command A’s exceptional long-context capabilities. Our hybrid architecture
enables this level of performance while requiring significantly less KV cache memory compared to models
with a full attention architecture. For instance, at an 8k sequence length, Command A requires only 75%
of the KV cache memory used by Llama 3.3 70B Instruct, 23.8% of that used by Llama 3.1 405B Instruct,
and 45.5% of that used by Mistral Large. At a 128k sequence length, these decrease to 32.9%, 10.4%, and
19.9%, respectively. This reduction in KV cache memory usage can significantly decrease latency and memory
consumption while enhancing throughput during inference, particularly for longer contexts.

4.9 Merging

4.9.1 Expert performance is largely preserved
We find that model merging is an effective method for combining capabilities from a set of expert models
into a single model, and that linear merging is sufficient to preserve expert performance with only a 1.8%
average drop. Figure 12a shows the distribution of changes in score for the metrics tracked during merging.
We find that the overwhelming majority of metrics are preserved to within 2.5% of the best expert score, with
some metrics actually improving after merging. Figure 12b shows the average degree of metric preservation
between expert and merge, grouped by domain. RAG and general performance are generally best preserved,
with code performance the least well preserved during merging.

34

(a) Distribution of metric values for the merged
model, as a percentage of the score for the best in-
put expert. The overwhelming majority of metrics
tracked are preserved to within 2.5% of the best ex-
pert score.

(b) Average degree of metric preservation between ex-
pert and merge, by domain. Error bars denote stan-
dard deviation. RAG and general performance are
best preserved, with code the least well preserved.
Note that the higher RAG standard deviation is a
property of RAG metrics (rather than the merge) as
TauBench has very high variance across runs.

Figure 12

We additionally find that, at the 111B scale, model merging is very stable. Large changes to expert weights
result in relatively small (2-3%) changes in domain performance, with very few candidates displaying catas-
trophic failures at any capability.

4.9.2 Merging is a coarse operation
Model merging is a coarse operation, with no guarantee that a merged checkpoint will fall at a local optimum
in the loss landscape. We find that applying a small number of SFT steps with low learning rate10 on top of
a merged checkpoint leads to significant improvements in model capability, with many metrics reaching or
exceeding 100% of the expert performance. Intuitively, model merging appears to be effective at combining
capabilities but may leave them ‘misaligned’ in encoding space; a small number of gradient-based updates
gently adjusts these encodings and allows the capabilities to be fully surfaced.

4.10 Polishing
The polishing effort includes several phases to improve model style and overall alignment with human pref-
erences. We use the same evaluation setting as the multilingual mArenaHard restricted to English (Dang
et al., 2024). In Figure 13, we show Command A win rates against GPT-4o (1120) according to a pool of
LLM judges at each phase of the CoPG/SRPO ping-pong. While this method allows us to achieve high
win rates, we also observe that interleaving the two methods tends to increase overall training stability: a
potential regression occurring at a particular phase is likely to be corrected by the next one.

In addition, polishing helps improve both the overall alignment with human preferences and recovery of any
degraded scores during merging. For some of the metrics, it also improves the score over the expert models.
Figure 14 provides the difference in metrics across many domains between the polished and the RL soup
model, showing that some domains benefit significantly from polishing, with human preferences benefiting
the most.

10Roughly 10-20x lower than was used for the main SFT training.

35

Number of MetricsMerge Score - Expert ScoreMerge - Expert Score, PercentageDomainReasoningSafetyGeneralCodeMultilingualRAGFigure 13: Command A win rates against GPT-4o as the polishing phase progresses.

Figure 14: Performance improvements across various domains during polishing between the inital RL soup
model and the polished model.

4.11 Human Evaluation
While automated evaluation benchmarks provide quick feedback and allow for efficient hill-climbing in specific
task settings, automatically assessing the perceived quality of models remains challenging (Ni et al., 2024).
To validate broader model performance, our most promising model candidates are additionally evaluated by
human annotators. This section describes our human evaluation setup, including details on the curation of
our internal evaluation dataset, an overview of our annotation strategy, as well as the results for Command
A in head-to-head evaluation against competitors.

36

Auto Arena Hard00.20.10.30.40.5SoupBONSRPOOnline CoPGSRPOOnline CoPGSRPOImprovements from PolishingPolished - SoupBFCL MGSMMMLULBPPMATH (all)HumanEvalIFEval(Not) over-refusalHuman Eval Win RatesmArenaHardAuto Arena Hard-55010201525Safety Controllability4.11.1 Evaluation Data
Chatbot Arena (Chiang et al., 2024) is a popular LLM benchmark that involves crowd-sourced human an-
notations. The framework relies on human preference judgments between two model-generated completions
over user-provided prompts, from which the authors then derive Elo scores to build up-to-date leader-
boards (Boubdir et al., 2024). While Chatbot Arena-like evaluation provides an extremely useful quality
signal on user-perceived model quality, it has several drawbacks for the efficient evaluation of internal model
candidates. Two challenges are the long ramp-up time required to provide performance signal on a new
model (often requiring multiple thousands of ratings before producing a reliable Elo score), as well as the
reliance on user-provided prompts that often end up skewing towards simpler topics. In order to provide a
more immediate and targeted feedback signal on human-perceived model performance, we instead curate a
static collection of single-turn prompts for the internal evaluation of our models versus competitors.

Prompts in our internal collection are primarily curated from scratch by our pool of annotators to avoid
accidental contamination for competitor models by re-using existing prompt collections. Instructions are
specifically targeted towards creating complex real-world prompts that span a broad range of typical LLM-
assisted tasks. For our purposes, we define complexity as a function of the number of different asks contained
in a single prompt. As an example, asking for a summary with a certain length requirement would be assigned
a complexity score of 2. The first point for invoking a particular task (summarisation), and the second point
for additionally restricting the output (length constraint). We automatically filter prompts with the help of
Command R+ Refresh along several axes, building up a more comprehensive notion of prompt complexity.
While the filtering may be overly strict in some instances, our findings show that the resulting pool of
prompts is sufficiently difficult even for state-of-the-art models. We prune the final dataset to around 800
prompts, further subset into “general” (350 prompts), “reasoning” (150 prompts), and “code” (300 prompts).

The “general” split focuses on general-purpose tasks that do not require deep technical expertise. This can
include open-ended document generation or idea generation requests, and at most requires basic under-
standing of formatting languages such as CSV or JSON. The “reasoning” split includes more reasoning- and
math-heavy prompts that generally require undergraduate-level understanding of one STEM discipline. For
the “code” split, we specifically instruct annotators to curate prompts across a number of target programming
languages, with a focus on analysis and debugging rather than code generation.

4.11.2 Annotation Methodology
Our annotation process comprises a relatively straightforward pairwise preference evaluation setup. For each
annotation task, we present annotators with one prompt and two completions from competing models.
Annotators are first tasked with evaluating the quality of each completion separately, assigning a score from
1 (flawed) to 5 (perfect). They are then given the option to label common failure modes. Finally, annotators
provide their preference between the two completions. The choices correspond to “Completion A is much
better”, “Completion A is slightly better”, “Tie”, “Completion B is slightly better”, and “Completion B is
much better”. To avoid positional bias, we randomly shuffle the order in which completions are shown to
annotators. We compare different models’ performance based on their win rate versus a fixed competitor
model. We assign the win rate in a pairwise matchup as a single score, distributing ties, as:

winrate =

wins + (0.5 × ties)
wins + ties + losses

We group the strong and weak preferences for wins or losses together and find that computing a win rate
over the 5-point scale does not change model rankings, but helps annotators gain confidence in their given
preference ratings. For a complete pairwise evaluation of 800 samples across all three subsets, on average 65
annotators contribute to a single evaluation run.

4.11.3 Results
Table 23 shows the results for pairwise human annotation runs against different competitor models. 11 Our
results show that Command A is competitive with frontier models on both general and reasoning subsets.

11We obtain completions for GPT-family models directly through the OpenAI API. We use the TogetherAI endpoints for

Llama 3.3 and DeepSeek V3.

37

vs.

GPT-4o

GPT-4.5 Preview

DeepSeek V3

Llama 3.3 70B Instruct

Llama 3.1 405B Instruct

Command A Win Rate (%)

General

Reasoning

50.4

47.2

49.0

68.8

61.6

51.4

30.7

49.3

71.7

64.0

Code

46.8

38.3

54.7

63.4

61.6

Table 23: Win rate of human-annotated pairwise evaluation between Command A and different competitor
models on our internal test set. Win rates are from the perspective of Command A (50% is tie, higher
numbers mean Command A wins by a larger margin).

Interestingly, we also observe that GPT-4.5-preview improves significantly over its predecessor on reasoning-
heavy prompts.

Annotators strongly prefer Command A over Llama 3.3 70B Instruct for all subsets, including code. Our
analysis across evaluation results indicates that humans particularly prefer Command A’s ability to respect
requests for a particular formatting or style.

To further illustrate the importance of polishing as a critical step towards human preference optimisation
(see Section 3.5), we compare the human evaluation results of an earlier internal checkpoint, a result of
the expert merging before polishing. Evaluating both our earlier checkpoint and the final model candidate
against GPT-4o, we see substantial gains as reflected in human preference. For our final model, we manage
an absolute improvement of more than seven percentage points in win rate on the general subset (43.2 →
50.4), 10 points on reasoning (41.4 → 51.4), and almost 17 points (30.0 → 46.8) on code.

Overall, Command A is significantly more preferred by human evaluators to models such as Llama 3.1 405B
Instruct across all subsets, and is competitive with state-of-the-art models such as GPT-4o and DeepSeek
V3 while being considerably more efficient.

5 Conclusion
This technical report detailed the development of Command A, shared extensive performance evaluations
across many domains and languages, and shared additional results for Command R7B. Command A repre-
sents a significant advancement in LLMs for enterprise, achieving best-in-class performance across a wide
range of tasks with optimal efficiency. Our models excel in enterprise-relevant tasks such as agentic workflows,
multilingual understanding and generation, and instruction-following. Key innovations introduced include
data and architectural optimisations, self-refinement algorithms, and a model merging-based approach that
ensures expert-level performance across diverse capabilities within a single model.

Command A outperforms comparable models in both efficiency and computational overhead, requiring fewer
resources for serving, making it easy to deploy on-premises or in private cloud environments on just two A100
or H100 GPUs, and delivering tokens at a higher rate. The release of model weights under a non-commercial
license further facilitates community-based exploration and research. Command A sets a new standard for
LLMs in enterprise applications, balancing performance, efficiency, and versatility — and providing maximum
performance for minimal compute.

38



49

A Authors
Team Cohere: Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Milad Alizadeh, Yazeed Al-
numay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Raphaël Avalos,
Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Björn Bebensee, Neeral Be-
ladia, Walter Beller-Morales, Alexandre Bérard, Andrew Berneshawi, Anna Bialas, Phil Blunsom, Matt
Bobkin, Adi Bongale, Sam Braun, Maxime Brunet, Samuel Cahyawijaya, David Cairuz, Jon Ander Cam-
pos, Cassie Cao, Kris Cao, Roman Castagné, Julián Cendrero, Leila Chan Currie, Yash Chandak, Diane
Chang, Giannis Chatziveroglou, Hongyu Chen, Claire Cheng, Alexis Chevalier, Justin T. Chiu, Eugene
Cho, Eugene Choi, Eujeong Choi, Tim Chung, Volkan Cirik, Ana Cismaru, Pierre Clavier, Henry Conklin,
Lucas Crawhall-Stein, Devon Crouse, Andres Felipe Cruz-Salinas, Ben Cyrus, Daniel D’souza, Hugo Dalla-
Torre, John Dang, William Darling, Omar Darwiche Domingues, Saurabh Dash, Antoine Debugne, Théo
Dehaze, Shaan Desai, Joan Devassy, Rishit Dholakia, Kyle Duffy, Ali Edalati, Ace Eldeib, Abdullah Elkady,
Sarah Elsharkawy, Irem Ergün, Beyza Ermis, Marzieh Fadaee, Boyu Fan, Lucas Fayoux, Yannis Flet-Berliac,
Nick Frosst, Matthias Gallé, Wojciech Galuba, Utsav Garg, Matthieu Geist, Mohammad Gheshlaghi Azar,
Ellen Gilsenan-McMahon, Seraphina Goldfarb-Tarrant, Tomas Goldsack, Aidan Gomez, Victor Machado
Gonzaga, Nithya Govindarajan, Manoj Govindassamy, Nathan Grinsztajn, Nikolas Gritsch, Patrick Gu,
Shangmin Guo, Kilian Haefeli, Rod Hajjar, Tim Hawes, Jingyi He, Sebastian Hofstätter, Sungjin Hong, Sara
Hooker, Tom Hosking, Stephanie Howe, Eric Hu, Renjie Huang, Hemant Jain, Ritika Jain, Nick Jakobi,
Madeline Jenkins, JJ Jordan, Dhruti Joshi, Jason Jung, Trushant Kalyanpur, Siddhartha Rao Kamalakara,
Julia Kedrzycki, Gokce Keskin, Edward Kim, Joon Kim, Wei-Yin Ko, Tom Kocmi, Michael Kozakov, Wo-
jciech Kryściński, Arnav Kumar Jain, Komal Kumar Teru, Sander Land, Michael Lasby, Olivia Lasche,
Justin Lee, Patrick Lewis, Jeffrey Li, Jonathan Li, Hangyu Lin, Acyr Locatelli, Kevin Luong, Raymond Ma,
Lukáš Mach, Marina Machado, Joanne Magbitang, Brenda Malacara Lopez, Aryan Mann, Kelly Marchisio,
Olivia Markham, Alexandre Matton, Alex McKinney, Dominic McLoughlin, Jozef Mokry, Adrien Morisot,
Autumn Moulder, Harry Moynehan, Maximilian Mozes, Vivek Muppalla, Lidiya Murakhovska, Heman-
gani Nagarajan, Alekhya Nandula, Hisham Nasir, Shauna Nehra, Josh Netto-Rosen, Daniel Ohashi, James
Owers-Bardsley, Jason Ozuzu, Dennis Padilla, Gloria Park, Sam Passaglia, Jeremy Pekmez, Laura Penstone,
Aleksandra Piktus, Case Ploeg, Andrew Poulton, Youran Qi, Shubha Raghvendra, Miguel Ramos, Ekagra
Ranjan, Pierre Richemond, Cécile Robert-Michon, Aurélien Rodriguez, Sudip Roy, Sebastian Ruder, Laura
Ruis, Louise Rust, Anubhav Sachan, Alejandro Salamanca, Kailash Karthik Saravanakumar, Isha Satyakam,
Alice Schoenauer Sebag, Priyanka Sen, Sholeh Sepehri, Preethi Seshadri, Ye Shen, Tom Sherborne, Sylvie
Shang Shi, Sanal Shivaprasad, Vladyslav Shmyhlo, Anirudh Shrinivason, Inna Shteinbuk, Amir Shukayev,
Mathieu Simard, Ella Snyder, Ava Spataru, Victoria Spooner, Trisha Starostina, Florian Strub, Yixuan Su,
Jimin Sun, Dwarak Talupuru, Eugene Tarassov, Elena Tommasone, Jennifer Tracey, Billy Trend, Evren
Tumer, Ahmet Üstün, Bharat Venkitesh, David Venuto, Pat Verga, Maxime Voisin, Alex Wang, Donglu
Wang, Shijian Wang, Edmond Wen, Naomi White, Jesse Willman, Marysia Winkels, Chen Xia, Jessica Xie,
Minjie Xu, Bowen Yang, Tan Yi-Chern, Ivan Zhang, Zhenyu Zhao, and Zhoujie Zhao.

A.1 Acknowledgements
We would also like to acknowledge the contributions of the following people who helped make this work
possible: Robert Li, Olivier Pietquin, Karina Waluk, Bill Wu, and James Zhou. We would also like to thank
our talented team of internal annotators and model trainers.

This technical report was written with the assistance of the models described in this technical report.

50

B Appendices

Instruction-Following

B.1
We use the following custom preamble override for internal IFEval evaluation for Command A:

You are in non-interactive mode. Please be as comprehensive and accurate as possible and do not
introduce your response and / or ask follow-up questions.

B.2 Multilingual

B.2.1 Expert Training Considerations
Supervised Fine-Tuning (SFT). We employ all SFT datasets as mentioned in Section 3.3.3.1. To train
the model, we use the Adam optimiser with a peak learning rate of 2.5 × 10−5, cosine decay to 1.25 × 10−5,
β1 = 0.9, β2 = 0.95 and a weight decay of 0.1. We merge several models with the same configuration (but a
different random seed) at this stage.

Preference Tuning. Following the SFT stage, we perform offline preference tuning using both human-
annotated and synthetically generated multilingual preference data (with best-of-N or machine translation).
We use DPO with the Adam optimiser, a peak learning rate of 1.25 × 10−5 decaying to 1.25 × 10−6, β = 0.2
and use SFT regularisation with the same data mixture we used in the SFT stage. We do not merge over
multiple seeds at this stage.

B.2.2 Results
We show additional results on MGSM in Table 24, and INCLUDE-44 in Table 25 — highlighting the highly
competitive multilingual capabilities of our models.

Command A

DeepSeek V3

Claude 3.7 Sonnet

Llama 3.3 70B Instruct

Qwen 2.5 72B Instruct Turbo

Mistral Large 2

Gemini 2.0 Flash

GPT-4o

Gemini 1.5 Pro

Llama 3.1 405B Instruct

Command R7B

Gemma 2 9B Instruct

Gemini 1.5 Flash-8B

Claude 3 Haiku

Ministral 8B

Llama 3.1 8B Instruct

Qwen 2.5 7B Instruct Turbo

Avg.

90.1

92.3

92.1

91.5

90.5

90.1

90.0

89.6

88.4

74.1

75.0

80.7

80.3

76.9

76.1

70.4

70.3

de

92.0

92.4

93.6

92.8

92.0

90.0

90.8

92.0

91.2

89.8

77.1

83.1

79.9

77.9

81.0

73.9

81.9

es

92.4

96.0

96.4

91.6

94.4

90.4

92.8

90.8

93.2

82.4

78.3

85.9

80.7

77.9

80.6

67.1

75.9

fr

85.1⋆
90.4

85.5

90.0

87.6

85.9

83.1

83.1

77.5

7.7

71.5

76.7

76.7

73.5

74.6

65.9

27.3

ja

87.1

89.6

89.2

89.6

87.1

90.3

87.1

87.1

86.7

85.8

66.7

72.3

79.1

74.7

64.4

62.7

72.7

ru

94.0

94.0

95.6

95.2

92.4

92.8

95.2

92.0

92.4

90.3

79.9

86.3

84.3

80.3

81.1

77.9

82.3

zh

90.0

91.6

92.4

90.0

89.6

91.5

90.8

92.8

89.6

88.6

76.3

79.9

80.7

77.1

74.7

75.1

81.5

Table 24: MGSM scores using simple-evals. ⋆: we did not train with the simple-evals template and our French
outputs sometimes contain a comma (used as a decimal separator), which simple-evals counts as wrong. With
our internal implementation of MGSM, we score 90.0% on French.

51

Command A

Claude 3.7 Sonnet

Gemini 2.0 Flash

GPT-4o

DeepSeek V3

Gemini 1.5 Pro

Llama 3.1 405B Instruct

Llama 3.3 70B Instruct

Qwen 2.5 72B Instruct Turbo

Mistral Large 2

Command R7B

Gemini 1.5 Flash-8B

Claude 3 Haiku

Gemma 2 9B Instruct

Qwen 2.5 7B Instruct Turbo

Llama 3.1 8B Instruct

Ministral 8B

Avg.

74.3

81.0

78.8

78.7

76.9

76.4

75.3

75.0

72.6

72.0

55.8

66.0

64.0

61.7

59.9

54.9

50.2

ar

73.2

79.2

79.0

79.3

73.9

72.5

70.7

71.7

70.5

68.3

58.0

61.4

65.9

57.1

57.1

55.1

43.8

de

67.6

66.9

69.1

68.3

64.7

66.2

71.7

65.5

64.0

66.2

51.8

54.7

50.4

58.3

53.2

53.2

42.4

el

es

fa

fr

he

hi

id

it

ja

ko

nl

pl

pt

ru

68.0

74.7

79.3

85.1

61.3

73.4

77.6

84.2

75.5

87.8

81.3

87.1

78.6

87.0

77.5

80.9

69.2

78.8

64.6

63.3

63.5

65.3

60.0

59.5

57.1

55.3

41.4

79.5

82.3

83.1

79.4

77.4

75.2

77.3

76.6

58.7

83.3

83.1

80.2

78.2

77.8

78.5

70.5

74.9

53.6

67.6

77.3

80.4

76.4

75.0

77.1

73.9

72.4

71.5

69.3

47.2

72.7

81.6

86.5

91.4

77.8

78.2

76.5

76.0

76.4

76.4

73.1

72.2

58.4

89.8

89.2

88.5

87.2

76.8

89.2

85.8

87.0

69.2

86.8

93.4

91.2

93.6

91.8

90.8

88.5

85.2

86.0

85.8

61.9

72.4

73.6

79.4

73.8

71.4

72.3

68.4

67.0

70.6

65.0

58.2

86.0

85.8

83.7

86.2

85.8

83.5

84.0

82.4

59.5

49.3

69.4

68.9

65.4

67.8

79.4

77.4

64.0

74.6

46.9

44.9

43.4

43.8

38.7

67.1

64.0

63.5

59.2

57.0

68.0

66.5

58.4

54.5

49.5

60.1

58.1

56.7

49.2

46.6

67.1

63.6

63.5

60.5

49.5

76.6

72.4

72.8

70.6

57.5

75.0

73.7

71.9

58.3

57.3

62.8

56.2

61.4

50.2

51.0

72.6

70.2

69.7

63.7

58.4

71.8

72.5

66.5

70.7

62.2

64.7

57.1

66.0

41.3

61.7

54.2

55.3

49.3

33.5

41.6

83.1

83.1

82.5

79.6

81.4

78.0

76.9

79.5

64.0

71.2

71.5

65.5

65.8

62.7

59.5

85.0

82.7

80.3

80.4

84.7

86.5

71.9

74.3

71.0

67.5

69.3

64.1

59.5

55.1

55.8

79.1

78.9

77.9

75.7

75.8

77.9

76.2

75.3

49.9

67.3

64.1

65.0

61.5

59.5

49.2

73.7

74.8

72.8

71.7

72.0

69.6

69.4

68.3

47.3

62.1

62.3

57.8

52.5

49.1

51.1

tr

67.7

68.4

66.2

66.2

66.8

67.7

71.9

73.0

64.8

65.3

62.0

63.0

58.6

59.3

52.6

56.0

46.7

uk

vi

76.5

87.1

70.7

80.5

80.4

82.5

77.8

78.5

76.5

74.5

71.3

71.8

58.9

66.5

69.3

66.0

57.1

54.7

54.0

77.5

79.5

78.0

75.1

77.6

76.4

70.0

66.2

52.2

63.1

56.0

60.2

56.4

55.5

44.5

zh

75.8

80.7

78.3

79.3

83.9

77.0

76.9

74.9

84.4

71.0

51.9

65.9

61.3

56.5

72.8

53.6

50.6

Table 25: INCLUDE-44 scores for individual languages.

B.3 Code

B.3.1 Expert Training Considerations
Stage 1 large-scale supervised learning. This expert is trained using Adam optimisation, a learning
rate peak of 5×10−5, cosine decay to 5×10−6, beta values (β1 = 0.9, β2 = 0.95), weight decay factor of 0.1,
and gradient norm clipping at peak 1.0. Our regularisation is inspired by similar code expert training such
as Qwen 2.5 Coder (Hui et al., 2024).

Stage 2 supervised learning on only high-quality data. We follow a similar schedule for fine-tuning
of the merged model described in Stage 1.

Stage 3 RL over scored or preference-pair data. We use a constant learning rate of 2 × 10−6, a
regularisation parameter β of 0.06, and otherwise match the hyperparameters used for SFT.

B.3.2 Results
We additionally show a breakdown of results across programming languages for HumanEval and Less Basic
Python Problems (LBPP) in Table 26.

HumanEval

C++ Rust
73.2
81.1

Less Basic Python Problems (LBPP)
JavaScript

Java

COBOL Avg. Python C++ Rust
41.6

25.3

51.5

58.4

50.3

Command A

Command A Expert

Command A Agentic

Command R7B

Command R Refresh

Command R+ Refresh

Llama 3.3 70B Instruct

Mistral Large 2

Qwen 2.5 72B Instruct

Llama 3.1 405B Instruct

DeepSeek V3

Avg.

76.2

77.5

—

50.7

54.7

54.4

75.5

82.9

78.5

76.7

83.5

Python

85.4

86.6

—

61.0

67.1

72.0

80.5 / 85.4

94.5
86.6 / 86.6

89.0 / 88.4

Java

JavaScript

86.0

76.2

—

52.4

59.2

67.1

83.4

84.8

82.9

87.8
85.4

64.6

79.9

—

53.7

64.6

62.8

81.7

87.2
81.1

79.3

85.4

Go

72.0

71.3

—

43.3

33.5

19.5

62.2

72.6

71.3

69.5

77.4

76.8

—

54.9

56.1

54.3

75.0

86.0

76.8

81.7

74.4
—

39.0

47.6

50.6

63.4

72.6

72.0

53.7

70.7

29.8
—

7.0

1.9

2.5

3.2

10.8

6.3

3.2

15.2

50.8

65.4
21.9

24.7

25.6

47.8

54.0

48.3

52.7

61.5

59.6

71.4
26.1

33.5

38.5

55.3

62.7

59.6

59.6

67.1

51.6

39.6

68.8
21.1

57.7
13.4

16.1

21.5

32.9

36.2

32.9

38.9

25.5

26.7

42.2

53.4

46.6

50.9

59.0

50.0

53.2

63.9

23.4

22.2

30.4

56.3

51.3

49.4

57.6

58.2

54.3

68.0
29.4

26.8

28.1

56.2

52.9

60.8

61.4

66.0

92.1

90.2

57.1

67.1

Go

50.3

46.6

62.7
17.4

23.6

8.1

43.5

42.2

39.8

47.2

52.8

Table 26: Full pass@1 results for HumanEval and LBPP across Python, C++, Rust Java, Javascript, Go.
and COBOL. All results are internal reproductions using an identical prompt except where ‘/’ indicates an
external value first, and footnote citation, and the internal reproduction second. Average (Avg.) results are
a sample-weighted average across Python, C++, Rust Java, Javascript and Go languages.

Sources of externally reported numbers in all code-related tables in this report are as follows:

Code Understanding Benchmarks (Table 12)

• Llama 3.3 70B Instruct: https://github.com/meta-llama/llama-models/blob/main/models/lla

52

ma3_3/MODEL_CARD.md, https://bigcode-bench.github.io/

• Mistral Large 2: https://livecodebench.github.io/
• Qwen 2.5 72B Instruct: https://bigcode-bench.github.io/
• Llama 3.1 405B Instruct: https://github.com/meta-llama/llama-models/blob/main/models/lla

ma3_3/MODEL_CARD.md

• DeepSeek V3: https://livecodebench.github.io/, https://bigcode-bench.github.io/

Code Editing Benchmarks (Table 13)

• DeepSeek V3: https://www.deepseek.com/

HumanEval and LBPP (Table 26)

• Llama 3.3 70B Instruct and Llama 3.1 405B Instruct: https://github.com/meta-llama/llama-mod

els/blob/main/models/llama3_3/MODEL_CARD.md

• Qwen 2.5 72B Instruct: From Hui et al. (2024).

B.4 Reasoning
Supervised Fine-Tuning. We train using the Adam optimiser with peak learning rate of 2.5 × 10−5, cosine
decay to 2.5 × 10−6, β1 = 0.9, β2 = 0.95, weight decay of 0.01, and gradient norm clipping peak at 1.0.

Preference Tuning. We train using CoPG with the Adam optimiser, using a learning rate of 2 × 10−6 with
no decay, β1 = 0.9, β2 = 0.95, and gradient norm clipping peak at 1.0.

B.5 Long Context
Training is conducted using the Adam optimiser with a peak learning rate of 2.5 × 10−5, cosine decay to
2.5 × 10−6, β1 = 0.9, β2 = 0.95, weight decay of 0.01, and gradient norm clipping peak at 1.0.

B.6 Safety

Command A
Claude 3.5 Sonnet
DeepSeek V3
GPT-4o
Llama 3.1 405B
Llama 3.3 70B
Mistral Large Latest
Qwen 2.5 72B
Command R+ Refresh

Command R7B
Gemini 2.0 Flash
Gemma 2 9B
Llama 3.1 8B
Qwen 2.5 7B
Command R Refresh

Contextual
Over-Refusal

Contextual
Accuracy

Strict
Over-Refusal

Strict Accuracy

8.3
10.1
3.6
21.3
5.9
3.6
8.9
10.1
8.9

14.2
26.6
39.1
4.1
18.3
9.5

75.4
82.5
66.6
77.8
70.3
60.9
70.6
72.2
61.8

60.9
69.7
72.5
70.7
70.3
63.1

10.2
10.4
1.7
10.4
5.8
4.0
8.7
1.2
12.7

7.5
14.7
14.5
6.9
8.7
10.4

87.5
89.6
68.7
86.9
80.5
67.9
81.1
78.3
70.0

62.3
79.0
81.8
81.0
74.7
68.0

Table 27: Safety mode performance compared to similarly sized models. We pass the instructions for each
safety mode to competitors as system message or first message to allow for the mode comparison. Note
that the over-refusal set was developed by red-teaming Command R+ Refresh, and therefore is specifically
challenging for Command A models. The top performing model for each size category is bolded in each
column. Higher accuracy and lower over-refusal rates correspond to better performance.

53

Figure 15: Absolute safety performance for large models in the default setting.

Figure 16: Absolute safety performance for small models in the default setting.

B.6.1 Expert Training Considerations
Supervised fine-tuning. During the SFT stage, we train the safety expert using the Adam optimiser with
β1 = 0.9, β2 = 0.95, and a peak learning rate of 10−4 decayed to 10−5 with a cosine schedule. Gradient norm
is clipped to 1, and weight decay is weighted by 10−3.

Offline preference tuning. We train the Safety expert using the same hyper-parameters as above, except
the peak learning rate is 10−6 decayed to 10−7. We use IPO with a KL regularisation parameter, β = 0.03.

B.6.2 Results
Table 27 provides additional results for the safety mode performance of the Command A models, while
Figures 15 and 16 show absolute safety performance for large and small models respectively in the default
safety setting, further highlighting our models’ competitive safety performance.

B.7 Evaluation on Standard Benchmarks
We provide further details about how we measure performance on standard benchmarks:

• MMLU (Hendrycks et al., 2021) measures university-level academic knowledge across a diverse range

of subjects. We run 5-shot, with Chain-of-Thought (CoT).

• MMLU-Pro (Wang et al., 2024b) is an enhanced version of MMLU, designed to evaluate knowledge
across a diverse range of professional and academic domains, including law, medicine, and engineering.

54

040208060100MisinfoSelf-harmCSEASexual ContentViolence & HateClaude 3.5 SonnetDeepSeek V3GPT-4oLlama 3.1 405BLlama 3.3 70BMistral Large LatestQwen 2.5 72BCommand R+Command A040208060100MisinfoSelf-harmCSEASexual ContentViolence & HateGemini 1.5 FlashGemini 2.0 FlashGemma 2 9BLlama 3.1 8BQwen 2.5 7BCommand R7BCommand RWe run 5-shot, with CoT.

• GPQA (Rein et al., 2023) measures graduate-level academic knowledge in specialized STEM topics.

We run on 0-shot, with CoT, and we report results only on the diamond subset.

• IFEval (Zhou et al., 2023) measures instruction-following ability across 25 types of verifiable instruc-
tions (e.g. output length, keyword inclusion/exclusion, formatting). We compute the average of the
prompt-level strict accuracy (i.e., the fraction of dataset prompts where all verifiable instructions are
followed) and the instruction-level strict accuracy (i.e., the fraction of verifiable instructions that are
followed, this allows partial credit as most prompts include multiple instructions).

• InFoBench (Qin et al., 2024) also measures instruction-following across five broad types of instruc-
tions: content, linguistic, style, format, and number. Each prompt in InFoBench is paired with a set
of yes-no evaluation questions, and we use GPT-4o to answer these questions. We compute the overall
average accuracy across the fraction of correctly answered questions per prompt.

55

