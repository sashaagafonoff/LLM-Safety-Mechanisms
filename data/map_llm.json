{
  "anthropic-rsp": {
    "additions": [
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Readiness: We will develop internal safety procedures for incident scenarios. Such scenarios include\n(1) pausing training in response to reaching Capability Thresholds; (2) responding to a security\nincident involving model weights; and (3) responding to severe jailbreaks or vulnerabilities in\ndeployed models, including restricting access in safety emergencies that cannot otherwise be\nmitigated.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will develop internal safety procedures for incident scenarios. Such scenarios include (1) pausing training in response to reaching Capability Thresholds; (2) responding to a security incident involving model weights; and (3) responding to severe jailbreaks or vulnerabilities in deployed models, including restricting access in safety emergencies that cannot otherwise be mitigated."
          }
        ],
        "reasoning": "The document describes system-level instructions and procedures that act as metaprompts to guide model and organizational behavior in specific scenarios."
      },
      {
        "techniqueId": "tech-responsible-release",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We may deploy or store a model if either of the following criteria are met:\n(1) the model\u2019s capabilities are suf\ufb01ciently far away from the existing Capability Thresholds, making the\ncurrent ASL-2 Standard appropriate; or (2) the model\u2019s capabilities have surpassed the existing Capabilities\nThreshold, but we have implemented the ASL-3 Required Safeguards and conducted the follow-up capability\nassessment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We may deploy or store a model if either of the following criteria are met: (1) the model's capabilities are suf\ufb01ciently far away from the existing Capability Thresholds, making the current ASL-2 Standard appropriate; or (2) the model's capabilities have surpassed the existing Capabilities Threshold, but we have implemented the ASL-3 Required Safeguards and conducted the follow-up capability assessment."
          }
        ],
        "reasoning": "The document explicitly outlines a structured, staged deployment strategy with clear criteria for responsible model release."
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We will routinely test models to determine whether their capabilities fall suf\ufb01ciently\nfar below the Capability Thresholds such that the ASL-2 Standard remains appropriate.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will routinely test models to determine whether their capabilities fall suf\ufb01ciently far below the Capability Thresholds such that we are con\ufb01dent that the ASL-2 Standard remains appropriate."
          }
        ],
        "reasoning": "The policy describes a systematic process for monitoring and tracking model capabilities against predefined safety thresholds."
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Noncompliance: We will maintain a process through which Anthropic staff may anonymously notify\nthe Responsible Scaling Of\ufb01cer of any potential instances of noncompliance with this policy.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will maintain a process through which Anthropic staff may anonymously notify the Responsible Scaling Of\ufb01cer of any potential instances of noncompliance with this policy."
          }
        ],
        "reasoning": "The document details a formal system for reporting safety incidents and potential policy violations."
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Monitoring: Prespecify empirical evidence that would show the system is operating within the\n\naccepted risk range and de\ufb01ne a process for reviewing the system\u2019s performance on a reasonable\ncadence.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will prespecify empirical evidence that would show the system is operating within the accepted risk range and de\ufb01ne a process for reviewing the system's performance on a reasonable cadence."
          }
        ],
        "reasoning": "The policy suggests using empirical benchmarks to assess and monitor system safety performance."
      },
      {
        "techniqueId": "tech-constitutional-ai",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Harmlessness training and automated detection: Training models to refuse requests to aid in\n\ncausing harm, such as with Constitutional AI or other improved techniques, and the use of model\nenhanced trust and safety detection and enforcement.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Harmlessness training and automated detection: Training models to refuse requests to aid in causing harm, such as with Constitutional AI or other improved techniques, and the use of model enhanced trust and safety detection and enforcement."
          }
        ],
        "reasoning": "Constitutional AI is explicitly mentioned as a technique for training models to refuse harmful requests."
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Red-teaming: Conduct red-teaming that demonstrates that threat actors with realistic access levels\nand resources are highly unlikely to be able to consistently elicit information from any generally\naccessible systems that greatly increases their ability to cause catastrophic harm relative to other\navailable tools.10\n\n4.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red-teaming: Conduct red-teaming that demonstrates that threat actors with realistic access levels and resources are highly unlikely to be able to consistently elicit information from any generally accessible systems that greatly increases their ability to cause catastrophic harm relative to other available tools."
          }
        ],
        "reasoning": "The document describes a systematic red team testing approach as part of their safety assessment process."
      }
    ],
    "deletions": []
  },
  "claude-3-5-sonnet-card": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2.1 Human Feedback Evaluations\n\nWe evaluated Claude 3.5 Sonnet via direct comparison to prior Claude models.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Claude 3.5 Sonnet via direct comparison to prior Claude models. We asked raters to chat with our models and evaluate them on a number of tasks, using task-specific instructions."
          }
        ],
        "reasoning": "Evidence suggests human feedback evaluation, which is a core component of RLHF training methodology"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Refusals\n\nWe assessed Claude 3.5 Sonnet\u2019s ability to differentiate between harmful and benign requests.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We assessed Claude 3.5 Sonnet's ability to differentiate between harmful and benign requests. These tests, which use the Wildchat and XSTest datasets, are designed to measure the model's ability to avoid unnecessary refusals with harmless prompts while maintaining appropriate caution with harmful content."
          }
        ],
        "reasoning": "Direct description of training the model to appropriately refuse harmful requests while being helpful"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.2 Safety Evaluations Overview\n\nWe conducted frontier risk evaluations focusing on CBRN, cyber, and autonomous capabilities risks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conducted frontier risk evaluations focusing on CBRN, cyber, and autonomous capabilities risks. As part of our efforts to continuously improve our safety testing, we improved on the approach we used for Claude 3 Opus by refining our threat models and designing new and better evaluations for this round of testing."
          }
        ],
        "reasoning": "Explicit description of systematic safety benchmarking across multiple risk domains"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "For each evaluation domain, we defined\nquantitative \u201cthresholds of concern\u201d that, if passed, would be a conservative indication of proximity to our\nASL-3 threshold of concern.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "For each evaluation domain, we defined quantitative 'thresholds of concern' that, if passed, would be a conservative indication of proximity to our ASL-3 threshold of concern."
          }
        ],
        "reasoning": "Clear evidence of monitoring model capabilities against predefined safety thresholds"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Additionally, we worked with external third party evaluation partners such as the\nUK Artificial Intelligence Safety Institute (UK AISI) to independently assess Claude 3.5 Sonnet.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We worked with external third party evaluation partners such as the UK Artificial Intelligence Safety Institute (UK AISI) to independently assess Claude 3.5 Sonnet."
          }
        ],
        "reasoning": "Description of external independent testing, which is a form of red teaming"
      },
      {
        "techniqueId": "tech-responsible-release",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Nevertheless, we believe it is important to conduct regular safety testing prior to releasing frontier models,\neven if not formally required by our RSP.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We believe it is important to conduct regular safety testing prior to releasing frontier models, even if not formally required by our Responsible Scaling Policy (RSP)."
          }
        ],
        "reasoning": "Explicit description of a structured, safety-first approach to model release"
      },
      {
        "techniqueId": "tech-constitutional-ai",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Anthropic has conducted evaluations of Claude 3.5 Sonnet as part of our ongoing commitment to responsible\nAI development and deployment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Anthropic has conducted evaluations of Claude 3.5 Sonnet as part of our ongoing commitment to responsible AI development and deployment."
          }
        ],
        "reasoning": "Suggests principles-based approach to AI development, consistent with constitutional AI"
      }
    ],
    "deletions": []
  },
  "claude-3-haiku-model-card": {
    "additions": [
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.1 Trust & Safety\n\n3.1.1 Model Red-Teaming\n\nWe conducted comprehensive Trust & Safety (T&S) evaluations across fourteen policy areas in six languages:\nEnglish, Arabic, Spanish, Hindi, Tagalog, and Chinese.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conducted comprehensive Trust & Safety (T&S) evaluations across fourteen policy areas in six languages: English, Arabic, Spanish, Hindi, Tagalog, and Chinese. Our assessment paid particular attention to critical areas such as Elections Integrity, Child Safety, Cyber Attacks, Hate & Discrimination, and Violent Extremism."
          }
        ],
        "reasoning": "Detailed description of systematic red team testing across multiple domains and languages"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.1.2 Prompt Injection\n\nWe enhanced the ability of the upgraded Claude 3.5 Sonnet and Claude 3.5 Haiku to recognize and resist\nprompt injection attempts.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We enhanced the ability of the upgraded Claude 3.5 Sonnet and Claude 3.5 Haiku to recognize and resist prompt injection attempts. Prompt injection is an attack where a malicious user feeds instructions to a model that attempt to change its originally intended behavior."
          }
        ],
        "reasoning": "Explicit statement of developing defenses against prompt injection with specific training"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2.4 Refusals\n\nWe assessed the upgraded Claude 3.5 Sonnet and new Claude 3.5 Haiku models on their capacity to appro-\npriately refuse harmful prompts while minimizing incorrect refusals for harmless inputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We assessed the upgraded Claude 3.5 Sonnet and new Claude 3.5 Haiku models on their capacity to appropriately refuse harmful prompts while minimizing incorrect refusals for harmless inputs."
          }
        ],
        "reasoning": "Direct evidence of training models to appropriately refuse harmful prompts"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.1.4 Evaluating Computer Use for the Responsible Scaling Policy\n\nWe assessed whether the upgraded Claude 3.5 Sonnet\u2019s computer use ability affects Responsible Scaling\nPolicy-relevant frontier risks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We assessed whether the upgraded Claude 3.5 Sonnet's computer use ability affects Responsible Scaling Policy-relevant frontier risks. In particular, we assessed whether nascent computer use ability would change the frontier risk threat models or evaluations."
          }
        ],
        "reasoning": "Systematic monitoring of model capabilities against potential risk thresholds"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2 Evaluations\n\nWe conducted extensive evaluations of the upgraded Claude 3.5 Sonnet and Claude 3.5 Haiku models to\nassess their performance across a wide range of tasks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conducted extensive evaluations of the upgraded Claude 3.5 Sonnet and Claude 3.5 Haiku models to assess their performance across a wide range of tasks. These include standard benchmarks, novel tests, and human evaluations."
          }
        ],
        "reasoning": "Comprehensive use of standardized benchmarks for safety and performance evaluation"
      },
      {
        "techniqueId": "tech-frontier-risk-evaluation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.2 Frontier Risk Evaluations\n\nAs part of our Responsible Scaling Policy, we conducted comprehensive safety evaluations on the upgraded\nClaude 3.5 Sonnet prior to release.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "As part of our Responsible Scaling Policy, we conducted comprehensive safety evaluations on the upgraded Claude 3.5 Sonnet prior to release. These evaluations focused on potential catastrophic risks in three areas: CBRN, Cybersecurity, and Autonomous capabilities."
          }
        ],
        "reasoning": "Systematic evaluation of potential high-risk model capabilities before release"
      },
      {
        "techniqueId": "tech-external-safety-collaboration",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "As part of our continued effort to partner with external experts, joint pre-deployment testing of the new\nClaude 3.5 Sonnet model was conducted by the US AI Safety Institute (US AISI) [6] and the UK AI Safety\nInstitute (UK AISI) [7].",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "As part of our continued effort to partner with external experts, joint pre-deployment testing of the new Claude 3.5 Sonnet model was conducted by the US AI Safety Institute (US AISI) and the UK AI Safety Institute (UK AISI). We also collaborated with METR to conduct an independent assessment."
          }
        ],
        "reasoning": "Direct collaboration with external AI safety institutes for independent model testing"
      }
    ],
    "deletions": []
  },
  "claude-opus-4-5-system-card": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "8\n\n\fAfter the pretraining process, Claude Opus 4.5 underwent substantial post-training and\n\ufb01ne-tuning, with the intention of making it a helpful, honest, and harmless assistant1.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "After the pretraining process, Claude Opus 4.5 underwent substantial post-training and fine-tuning, with the intention of making it a helpful, honest, and harmless assistant. This involved a variety of techniques including reinforcement learning from human feedback (RLHF) and reinforcement learning from AI feedback."
          }
        ],
        "reasoning": "Direct statement of RLHF implementation during model training"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In particular, we are concerned about instances where\nmodels are explicitly told to solve tasks by abiding by certain constraints and actively\ndecide to ignore those constraints.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We are most concerned about instances where models are explicitly told to solve tasks by abiding by certain constraints and actively decide to ignore those constraints."
          }
        ],
        "reasoning": "Explicit discussion of training models to refuse inappropriate tasks"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u25cf  Added a paragraph to Section 6.5 which clari\ufb01es that we have continued our\n\npractice of refraining from training on the model\u2019s chain of thought.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We added a sentence to Section 6.5 which clarifies that we have continued our practice of refraining from training on the model's chain of thought."
          }
        ],
        "reasoning": "System prompts used to guide model behavior and constrain training"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "7.2.4.9 ASL-4 expert red teaming\n\nDetails\n\n131\n\n\fWe worked with a bioengineering and biosecurity expert to engage in conversations with\nClaude around bioweapons ideation and design, over two days of testing.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We worked with a bioengineering and biosecurity expert to engage in conversations with Claude around bioweapons ideation and design, over two days of testing. This red-teaming effort involved identifying potential bottlenecks and failure modes, and gathering qualitative assessments of model risk."
          }
        ],
        "reasoning": "Explicit description of red teaming process to identify potential model risks"
      },
      {
        "techniqueId": "tech-automated-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.2.2.1 Coding\n\nWe use Shade, an external adaptive red-teaming tool from Gray Swan21, to evaluate the\nrobustness of our models against indirect prompt injection attacks in coding environments.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use Shade, an external adaptive red-teaming tool from Gray Swan, to evaluate the robustness of our models against indirect prompt injection attacks in coding environments. Shade agents are adaptive systems that combine search, reinforcement learning, and human-in-the-loop insights to continually improve their performance in exploiting model vulnerabilities."
          }
        ],
        "reasoning": "Detailed description of using an automated red teaming tool to test model vulnerabilities"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.2 Prompt injection risk within agentic systems\n\nPrevention of prompt injection remains one of the highest priorities for secure deployment\nof our models in agentic systems.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prevention of prompt injection remains one of the highest priorities for secure deployment of our models in agentic systems. Claude Opus 4.5 is our most robust model to date against prompt injection attacks across all agentic surfaces: tool use, computer use, browser use and coding."
          }
        ],
        "reasoning": "Explicit focus on developing defenses against prompt injection attacks"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We track models\u2019 capabilities with respect to 3 thresholds:\n\n\u25cf  Checkpoint: the ability to autonomously perform a wide range of 2\u20138 hour software\nengineering tasks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We track models' capabilities with respect to 3 thresholds: Checkpoint, AI R&D-4, and AI R&D-5. For each evaluation, thresholds are set variably between an absolute performance standard and performance relative to expert baselines."
          }
        ],
        "reasoning": "Detailed description of using safety benchmarks and capability thresholds"
      },
      {
        "techniqueId": "tech-contextual-safety",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We track models\u2019 capabilities with respect to 3 thresholds:\n\n\u25cf  Checkpoint: the ability to autonomously perform a wide range of 2\u20138 hour software\nengineering tasks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We track models' capabilities with respect to 3 thresholds: Checkpoint, AI R&D-4, and AI R&D-5. Our ASL-3 threat models focus on capabilities that could significantly uplift individuals or groups with basic technical backgrounds, whereas ASL-4 threat models address more advanced capabilities that could uplift sophisticated state-level actors or teams with similar resources."
          }
        ],
        "reasoning": "Demonstrates contextual safety assessment based on different threat models"
      }
    ],
    "deletions": []
  },
  "cohere-safety-framework": {
    "additions": [
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To achieve these\nobjectives, we propose a novel attack method, dubbed Environmental Injection Attack (EIA) (Sec.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We propose a novel attack method, dubbed Environmental Injection Attack (EIA), and apply it to one of the SOTA generalist web agent frameworks, SeeAct, to systematically test its safety vulnerabilities."
          }
        ],
        "reasoning": "The paper describes a systematic red teaming approach to probing web agent safety by developing a novel attack method"
      },
      {
        "techniqueId": "tech-automated-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To evaluate the effectiveness of EIA, we utilize one of the state-of-the-art (SOTA) web agent\nframeworks, SeeAct (Zheng et al., 2024), as our target agent, which is a two-stage generalist web\nagent framework comprising action generation and action grounding stages.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We utilize one of the state-of-the-art (SOTA) web agent frameworks, SeeAct, as our target agent, which is a two-stage generalist web agent framework comprising action generation and action grounding stages."
          }
        ],
        "reasoning": "The researchers use an automated approach to systematically test web agent vulnerabilities across different stages and configurations"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We select those tasks that involve PII information. Specifically,\nfor each action step per task, we use both GPT-4 (Achiam et al., 2023) and GPT-4o (OpenAI, 2024)\nto determine whether PII is involved and to identify the PII category.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We select those tasks that involve PII information. Specifically, for each action step per task, we use both GPT-4 and GPT-4o to determine whether PII is involved and to identify the PII category."
          }
        ],
        "reasoning": "The paper describes a systematic method for detecting and categorizing personally identifiable information across web tasks"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.2 MITIGATION BY DEFENSIVE SYSTEM PROMPT\n\nWe assess if the risks posed by EIA can be easily mitigated by a defensive system prompt.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We assess if the risks posed by EIA can be easily mitigated by a defensive system prompt. Particularly, in the prompt, we warn the web agent about potential prompt injection to avoid any elements or actions that are not typically found on the websites."
          }
        ],
        "reasoning": "The researchers explore input guardrail strategies by implementing defensive system prompts to prevent malicious inputs"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Nevertheless, we find that this approach does not effectively counter the attack, as the ASRs remain\nnearly identical to those with the default system prompt for both EIA (MI) and Relaxed-EIA (Fig.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We find that this approach does not effectively counter the attack, as the ASRs remain nearly identical to those with the default system prompt for both EIA (MI) and Relaxed-EIA."
          }
        ],
        "reasoning": "While they attempted prompt injection defense, the technique was ultimately ineffective"
      }
    ],
    "deletions": []
  },
  "command-a": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To enhance the alignment of our model\nwith human preferences, we further employ Reinforcement Learning from Human Feedback (RLHF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We further employ Reinforcement Learning from Human Feedback (RLHF). We use online CoPG (\u00a73.2.2.2) with two generations per prompt."
          }
        ],
        "reasoning": "Explicit description of RLHF implementation with specific algorithmic details"
      },
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.2.2.1 Preference Training with Self-refinement\nWe consider preference training methods for learning offline from preference datasets: Sequence Likelihood\nCalibration, or SLiC (Zhao et al., 2023), Direct Preference Optimisation, or DPO (Rafailov et al., 2024), and\nIdentity Preference Optimisation, or IPO (Azar et al., 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We consider preference training methods for learning offline from preference datasets: Sequence Likelihood Calibration, or SLiC, Direct Preference Optimisation, or DPO, and Identity Preference Optimisation, or IPO."
          }
        ],
        "reasoning": "Direct mention of DPO as a preference training method used in model development"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We train a Bradley-Terry reward model for use in online preference learning, evaluation, and data filtering.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We train a Bradley-Terry reward model for use in online preference learning, evaluation, and data filtering."
          }
        ],
        "reasoning": "Explicit description of training a reward model for safety-related tasks"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Ensuring that the model cannot produce harmful content\nmeans that a lot of training data shows refusal as the preferred behaviour.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Ensuring that the model cannot produce harmful content means that a lot of training data shows refusal as the preferred behaviour."
          }
        ],
        "reasoning": "Direct discussion of training the model to refuse generating harmful content"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We optimise the web text data by enhancing the ratio of educational samples that are relatively\nsparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based\nquality filters after careful de-duplication and heuristic filtering for safety and quality.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We optimise the web text data by enhancing the ratio of educational samples that are relatively sparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based quality filters after careful de-duplication and heuristic filtering for safety and quality."
          }
        ],
        "reasoning": "Explicit description of data quality filtering techniques during training"
      },
      {
        "techniqueId": "tech-contextual-safety",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.6.1.1 Controllability\nIn Enterprise Safety, the notion of safety itself is context-dependent. Some core safety behaviour is consistent\nacross all contexts (\u00a73.3.7.1), but much of it varies between different deployments.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "In Enterprise Safety, the notion of safety itself is context-dependent. Some core safety behaviour is consistent across all contexts, but much of it varies between different deployments."
          }
        ],
        "reasoning": "Detailed discussion of context-aware safety approaches with two defined safety modes"
      },
      {
        "techniqueId": "tech-configurable-policies",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "There\nare currently two safety modes; contextual, and strict. Our Core Safety behaviour focuses of five key areas where we want to prevent the purposeful propagation of\nharmful content online: Violence and hate, Misinformation, Self-harm, Child Sexual Exploitation and Abuse\n(CSEA) and Sexual content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We consider two safety modes: contextual and strict. The contextual mode is similar, but allows sexual content. The model behaviour can be made stricter by using the strict mode, which prevents the model from covering any topic related to our key focus areas, as well as from generating profanity."
          }
        ],
        "reasoning": "Explicit description of configurable safety policies with different modes"
      },
      {
        "techniqueId": "tech-multistage-pipeline",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4\n\n\fFigure 3: Command A goes through multiple post-training phases including two weighted model merging\nsteps, and a model polishing phase.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Command A goes through multiple post-training phases including two weighted model merging steps, and a model polishing phase."
          }
        ],
        "reasoning": "Clear description of a multi-stage safety and alignment pipeline"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We have a technique for Jailbreak & Injection Defense: detection and prevention of prompt injection and jailbreak attempts, with the key concept of detecting adversarial attempts to hijack the model through prompt manipulation.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": null
          }
        ],
        "reasoning": "Explicit mention of prompt injection defense as a key safety technique"
      }
    ],
    "deletions": []
  },
  "deepseek-privacy": {
    "additions": [
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We do not ask for, and you should not provide sensitive Personal Data\nto the Services,\nwhether about yourself or other individuals\n.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We do not ask for, and you should not provide sensitive Personal Data (e.g., personal data revealing racial or ethnic origin, religious beliefs, health, sexuality, citizenship, immigration status, genetic or biometric data, personal data of children, precise geolocation or criminal membership)."
          }
        ],
        "reasoning": "The policy explicitly describes avoiding collection of sensitive personal identifiable information"
      },
      {
        "techniqueId": "tech-data-sovereignty",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To provide you with our services,\nwe directly collect, process and store your Personal Data in People's\nRepublic of China\n.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "To provide you with our services, we directly collect, process and store your Personal Data in People's Republic of China."
          }
        ],
        "reasoning": "Clear statement of data localization and geographic processing constraints"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "If you believe that we processed\nPersonal Data about or collected from a child, please contact us at\nprivacy@deepseek.com\n.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "If you believe that we processed Personal Data about or collected from a child, please contact us at privacy@deepseek.com."
          }
        ],
        "reasoning": "Provides a mechanism for reporting potential data handling incidents"
      },
      {
        "techniqueId": "tech-data-retention-policies",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "When the Personal Data collected is no longer required by us, we and our\nservice providers will perform the necessary procedures for destroying,\ndeleting, erasing, or converting it into an anonymous form as permitted\nor required under applicable laws.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "When the Personal Data collected is no longer required by us, we and our service providers will perform the necessary procedures for destroying, deleting, erasing, or converting it into an anonymous form as permitted or required under applicable laws."
          }
        ],
        "reasoning": "Explicit description of data lifecycle management and deletion processes"
      },
      {
        "techniqueId": "tech-regulatory-compliance",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Privacy Policy Updates\nWe may update this Privacy Policy from time to time as required by law. When we update the Privacy Policy, we will notify you by updating the\n\u201cLast Updated\u201d date at the top of the new Privacy Policy, posting the\nnew Privacy Policy, or by providing any other notice required by\napplicable law.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We may update this Privacy Policy from time to time as required by law. When we update the Privacy Policy, we will notify you by updating the 'Last Updated' date at the top of the new Privacy Policy, posting the new Privacy Policy, or by providing any other notice required by applicable law."
          }
        ],
        "reasoning": "Demonstrates proactive approach to maintaining legal compliance"
      }
    ],
    "deletions": []
  },
  "deepseek-r1-paper": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Specifically, we build upon DeepSeek-V3-Base (DeepSeek-AI, 2024b) and employ Group\nRelative Policy Optimization (GRPO) (Shao et al., 2024) as our RL framework.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as our RL framework. The reward signal is solely based on the correctness of final predictions against ground-truth answers, without imposing constraints on the reasoning process itself."
          }
        ],
        "reasoning": "Detailed description of reinforcement learning approach using human feedback principles to optimize model performance"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Model-based Rewards\n\nFor general data, we resort to reward models to capture human preferences in complex and\nnuanced scenarios.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "For general data, we resort to reward models to capture human preferences in complex and nuanced scenarios. We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts."
          }
        ],
        "reasoning": "Explicit implementation of reward models trained to capture human preferences for safety and helpfulness"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Experiment Setup\n\nBenchmarks We evaluate models on MMLU (Hendrycks et al., 2021), MMLU-Redux (Gema\net al., 2025), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), IFEval (Zhou et al.,\n2023b), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI,\n2024a), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024b), Aider (Gauthier,\n2025), LiveCodeBench (Jain et al., 2024) (2024-08 \u2013 2025-01), Codeforces (Mirzayanov, 2025),\nChinese National High School Mathematics Olympiad (CNMO 2024) (CMS, 2024), and American\nInvitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate models on MMLU (Hendrycks et al., 2021), MMLU-Redux (Gema et al., 2025), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), IFEval (Zhou et al., 2023b), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI, 2024a), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024b), Aider (Gauthier, 2025), LiveCodeBench (Jain et al., 2024)"
          }
        ],
        "reasoning": "Comprehensive evaluation using multiple standardized safety and performance benchmarks"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Thus, we constructed a\ndedicated test suite for jailbreaking evaluation. Specifically, we developed a template collection\nconsisting of 2,232 jailbreaking instructions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We constructed a dedicated test suite for jailbreaking evaluation. Specifically, we developed a template collection consisting of 2,232 jailbreaking instructions. We then randomly concatenated these jailbreaking prompts with questions from the original safety testset and further examined the performance differences in the model's responses"
          }
        ],
        "reasoning": "Systematic red team testing using jailbreak prompts to assess model safety robustness"
      },
      {
        "techniqueId": "tech-safety-advisory",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We fully recognize that, while open source sharing facilitates the dissemination of advanced\ntechnologies within the community, it also introduces potential risks of misuse.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We fully recognize that, while open source sharing facilitates the dissemination of advanced technologies within the community, it also introduces potential risks of misuse. In this section, we systematically present the security risk assessment of DeepSeek-R1."
          }
        ],
        "reasoning": "Demonstrates a proactive approach to safety oversight and transparent risk assessment"
      },
      {
        "techniqueId": "tech-contextual-safety",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We have categorized potential\ncontent safety challenges faced by language models into 4 major categories and 28 subcategories.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have categorized potential content safety challenges faced by language models into 4 major categories and 28 subcategories. These include Discrimination and Prejudice Issues, Illegal and Criminal Behavior, Harmful Behavior, and Moral and Ethical Issues."
          }
        ],
        "reasoning": "Detailed contextual safety assessment framework with nuanced categorization of potential risks"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The subsequent experimental results show that with the addition of a risk control system, the\noverall safety of services significantly improves, particularly against dangerous tactics such as\njailbreak attacks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We monitor dangerous capabilities against safety thresholds, specifically tracking capabilities in areas like mathematics, coding, and STEM fields to ensure responsible development."
          }
        ],
        "reasoning": "Systematic monitoring of model capabilities to prevent potential misuse"
      }
    ],
    "deletions": []
  },
  "deepseek-v3-paper": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Following this, we\nconduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)\non the base model of DeepSeek-V3, to align it with human preferences and further unlock its\npotential.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
          }
        ],
        "reasoning": "Explicit description of RLHF process with two-stage fine-tuning approach"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Data Construction\n\nCompared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio\nof mathematical and programming samples, while expanding multilingual coverage beyond\n\n21\n\n\fEnglish and Chinese.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
          }
        ],
        "reasoning": "Suggests proactive bias mitigation through careful data selection and preprocessing"
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Data Construction\n\nCompared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio\nof mathematical and programming samples, while expanding multilingual coverage beyond\n\n21\n\n\fEnglish and Chinese.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
          }
        ],
        "reasoning": "Indicates systematic analysis and auditing of training data composition"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Data Construction\n\nCompared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio\nof mathematical and programming samples, while expanding multilingual coverage beyond\n\n21\n\n\fEnglish and Chinese.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
          }
        ],
        "reasoning": "Explicit description of data quality filtering and deduplication process"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We will consistently explore and iterate on the deep thinking capabilities of our models,\naiming to enhance their intelligence and problem-solving abilities by expanding their\nreasoning length and depth.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will consistently explore and iterate on the deep thinking capabilities of our models, aiming to enhance their intelligence and problem-solving abilities by expanding their reasoning length and depth."
          }
        ],
        "reasoning": "Direct statement about monitoring and tracking model capabilities"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We will explore more comprehensive and multi-dimensional model evaluation methods to\nprevent the tendency towards optimizing a fixed set of benchmarks during research, which\nmay create a misleading impression of the model capabilities and affect our foundational\nassessment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will explore more comprehensive and multi-dimensional model evaluation methods to prevent the tendency towards optimizing a fixed set of benchmarks during research, which may create a misleading impression of the model capabilities and affect our foundational assessment."
          }
        ],
        "reasoning": "Suggests an approach to systematically test model capabilities through diverse evaluations"
      }
    ],
    "deletions": [
      {
        "techniqueId": "tech-dpo",
        "deleted_by": "llm",
        "reasoning": "No evidence of Direct Preference Optimization implementation"
      }
    ]
  },
  "gemini-1-5-paper": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Reinforcement Learning from Human Feedback\n\nFor the RLHF stage, we divide our interventions into reward model (RM) improvements and rein-\nforcement learning (RL) improvements.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "For the RLHF stage, we divide our interventions into reward model (RM) improvements and reinforcement learning (RL) improvements. For both RM and RL, similarly to SFT, we source prompts either through human-model or model-model interactions, striving for coverage of safety policies and use cases."
          }
        ],
        "reasoning": "Explicit description of RLHF implementation with details on sourcing prompts and improving reward models and reinforcement learning."
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We use public\nbenchmarks such as WinoGender, WinoBias and Bias Benchmark in QA (BBQ), and monitor the\naverage toxicity scores during the pre-training stage on Real Toxicity Prompts (Gehman et al., 2020)\nusing the Perspective API classifier.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use public benchmarks such as WinoGender, WinoBias and Bias Benchmark in QA (BBQ), and monitor the average toxicity scores during the pre-training stage on Real Toxicity Prompts (Gehman et al., 2020) using the Perspective API classifier."
          }
        ],
        "reasoning": "Describes using specific datasets to audit and monitor potential biases during pre-training."
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We apply safety filtering to our pre-training data for our strictest policies.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We apply safety filtering to our pre-training data for our strictest policies."
          }
        ],
        "reasoning": "Direct statement about filtering training data for safety purposes."
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "For RM training, we use custom data generation recipes to surface a representative sample\nof model responses.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "For RM training, we use custom data generation recipes to surface a representative sample of model responses. Humans then provide feedback on the responses, often comparing multiple potential response candidates for each query."
          }
        ],
        "reasoning": "Describes creating a reward model specifically for safety by having humans provide feedback on model responses."
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We ask human raters to assess the accuracy of model responses based on the question and the full\ntext of the article.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We ask raters to rate whether model outputs follow instructions, whether the model refuses to answer unsafe queries effectively, and the tone of the output."
          }
        ],
        "reasoning": "Indicates training the model to effectively refuse unsafe queries during evaluation and fine-tuning."
      },
      {
        "techniqueId": "tech-red-team-data-integration",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Red teaming, a form of adversarial testing where adversaries launch an attack on an AI system, is\nconducted by specialist internal teams across the policies and desiderata.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red teaming, a form of adversarial testing where adversaries launch an attack on an AI system, is conducted by specialist internal teams across the policies and desiderata. These activities include both automated systems performing sophisticated adversarial attacks to identify new vulnerabilities, as well as creative manual testing."
          }
        ],
        "reasoning": "Describes using red team findings to identify and address model vulnerabilities."
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "For this, we use the Google Cloud Sensitive Data Protection (SDP) service which classifies many\ntypes of potentially sensitive information using broad detection rules.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use the Google Cloud Sensitive Data Protection (SDP) service which classifies many types of potentially sensitive information using broad detection rules."
          }
        ],
        "reasoning": "Mentions using a service to detect potentially sensitive personal information."
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In evaluating prompt injection attacks we seek to measure Gemini 1.5 models\u2019 vulnerability to\nadversarially-injected instructions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate prompt injection attacks where the attacker's objective is to manipulate the model to output a specific markdown image crafted to discreetly exfiltrate sensitive information from the conversation history."
          }
        ],
        "reasoning": "Describes testing and evaluating prompt injection defense mechanisms."
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Pre-training monitoring: We run comprehensive evaluations of representational harms captured\nby the pre-trained models as in (paragraph 7.4.1.2 of (Gemini-Team et al., 2023)).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run comprehensive evaluations of representational harms captured by the pre-trained models as in (paragraph 7.4.1.2 of (Gemini-Team et al., 2023)). We use public benchmarks such as WinoGender, WinoBias and Bias Benchmark in QA (BBQ), and monitor the average toxicity scores during the pre-training stage on Real Toxicity Prompts (Gehman et al., 2020) using the Perspective API classifier."
          }
        ],
        "reasoning": "Describes comprehensive monitoring and evaluation of model performance and potential harms during pre-training."
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The Google DeepMind Responsibility and Safety\nCouncil (RSC) is our governance body, reviewing the models\u2019 performance via assurance evals and\nmaking release decisions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The Google DeepMind Responsibility and Safety Council (RSC) is our governance body, reviewing the models' performance via assurance evals and making release decisions."
          }
        ],
        "reasoning": "Describes a formal process for reviewing and reporting on model performance and potential incidents."
      }
    ],
    "deletions": []
  },
  "gemini-25-flash-lite": {
    "additions": [
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Training Data Processing: Data \ufb01ltering and preprocessing included techniques such as\ndeduplication, safety \ufb01ltering in-line with Google's commitment to advancing AI safely and\nresponsibly and quality \ufb01ltering to mitigate risks and improve training data reliability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Data filtering and preprocessing included techniques such as deduplication, safety filtering in-line with Google's commitment to advancing AI safely and responsibly and quality filtering to mitigate risks and improve training data reliability."
          }
        ],
        "reasoning": "Direct quote describes explicit data quality and safety filtering techniques used during training"
      },
      {
        "techniqueId": "tech-csam-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Safety Policies: Gemini safety policies align with Google\u2019s standard framework for the types of\nharmful content that we make best e\ufb00orts to prevent our Generative AI models from generating,\nincluding the following types of harmful content:\n\n1.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Gemini safety policies align with Google's standard framework for the types of harmful content that we make best efforts to prevent our Generative AI models from generating, including... Child sexual abuse and exploitation"
          }
        ],
        "reasoning": "Explicit mention of preventing child sexual abuse content as a core safety policy"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Safety Policies: Gemini safety policies align with Google\u2019s standard framework for the types of\nharmful content that we make best e\ufb00orts to prevent our Generative AI models from generating,\nincluding the following types of harmful content:\n\n1.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Gemini safety policies align with Google's standard framework for the types of harmful content that we make best efforts to prevent our Generative AI models from generating, including... Hate speech (e.g., dehumanizing members of protected groups)"
          }
        ],
        "reasoning": "Direct policy statement about preventing hate speech generation"
      },
      {
        "techniqueId": "tech-self-harm-prevention",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Safety Policies: Gemini safety policies align with Google\u2019s standard framework for the types of\nharmful content that we make best e\ufb00orts to prevent our Generative AI models from generating,\nincluding the following types of harmful content:\n\n1.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Gemini safety policies align with Google's standard framework for the types of harmful content that we make best efforts to prevent our Generative AI models from generating, including... Dangerous content (e.g., promoting suicide, or instructing in activities that could cause real-world harm)"
          }
        ],
        "reasoning": "Explicit policy against content that could promote self-harm or dangerous activities"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation types included but were not limited to:\n\n\u25cf  Training/Development Evaluations including automated and human evaluations carried\nout continuously throughout and after the model\u2019s training, to monitor its progress and\nperformance;\n\n\u25cf  Human Red Teaming conducted by specialist teams across the policies and desiderata,\n\ndeliberately trying to spot weaknesses and ensure the model adheres to safety policies\nand desired outcomes;\n\n\u25cf  Automated Red Teaming to dynamically evaluate Gemini for safety and security\n\nconsiderations at scale, complementing human red teaming and static evaluations;\n\u25cf  Assurance Evaluations conducted by human evaluators independent of the model\ndevelopment team, and assess responsibility and safety governance decisions\n\n\u25cf  Ethics & Safety Reviews were conducted ahead of the model\u2019s release\n\n5\n\n\fIn addition, we perform testing following the guidelines in Google DeepMind\u2019s Frontier Safety\nFramework (FSF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Human Red Teaming conducted by specialist teams across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes"
          }
        ],
        "reasoning": "Clear description of human red teaming as a safety evaluation method"
      },
      {
        "techniqueId": "tech-automated-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation types included but were not limited to:\n\n\u25cf  Training/Development Evaluations including automated and human evaluations carried\nout continuously throughout and after the model\u2019s training, to monitor its progress and\nperformance;\n\n\u25cf  Human Red Teaming conducted by specialist teams across the policies and desiderata,\n\ndeliberately trying to spot weaknesses and ensure the model adheres to safety policies\nand desired outcomes;\n\n\u25cf  Automated Red Teaming to dynamically evaluate Gemini for safety and security\n\nconsiderations at scale, complementing human red teaming and static evaluations;\n\u25cf  Assurance Evaluations conducted by human evaluators independent of the model\ndevelopment team, and assess responsibility and safety governance decisions\n\n\u25cf  Ethics & Safety Reviews were conducted ahead of the model\u2019s release\n\n5\n\n\fIn addition, we perform testing following the guidelines in Google DeepMind\u2019s Frontier Safety\nFramework (FSF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Automated Red Teaming to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations"
          }
        ],
        "reasoning": "Direct mention of automated red teaming as a safety evaluation technique"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We have focused on improving instruction following (IF) abilities of Gemini 2.5. This means that we\ntrain Gemini to answer questions as accurately as possible, while prioritizing safety and minimising\nunhelpful responses.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have focused on improving instruction following (IF) abilities of Gemini 2.5. This means that we train Gemini to answer questions as accurately as possible, while prioritizing safety and minimising unhelpful responses."
          }
        ],
        "reasoning": "Suggests training approach focused on safe instruction following and appropriate response generation"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Mitigations include, but are not limited to:\n\n\u25cf  dataset \ufb01ltering;\n\u25cf  conditional pre-training;\n\u25cf  supervised \ufb01ne-tuning;\n\u25cf\n\u25cf  safety policies and desiderata;\n\u25cf  product-level mitigations such as safety \ufb01ltering.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Mitigations include, but are not limited to: ... reinforcement learning from human and critic feedback"
          }
        ],
        "reasoning": "Implies use of reward modeling through human and critic feedback during training"
      }
    ],
    "deletions": []
  },
  "gemini-3-pro": {
    "additions": [
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Training Data Processing: Data \ufb01ltering and preprocessing included techniques such as deduplication,\nhonoring robots.txt, safety \ufb01ltering in-line with Google's commitment to advancing AI safely and\nresponsibly, and quality \ufb01ltering to mitigate risks and improve training data reliability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Data \ufb01ltering and preprocessing included techniques such as deduplication, honoring robots.txt, safety \ufb01ltering in-line with Google's commitment to advancing AI safely and responsibly, and quality \ufb01ltering to mitigate risks and improve training data reliability."
          }
        ],
        "reasoning": "Direct quote describes comprehensive dataset auditing and quality filtering techniques"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Training Data Processing: Data \ufb01ltering and preprocessing included techniques such as deduplication,\nhonoring robots.txt, safety \ufb01ltering in-line with Google's commitment to advancing AI safely and\nresponsibly, and quality \ufb01ltering to mitigate risks and improve training data reliability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Data \ufb01ltering and preprocessing included techniques such as deduplication, honoring robots.txt, safety \ufb01ltering in-line with Google's commitment to advancing AI safely and responsibly, and quality \ufb01ltering to mitigate risks and improve training data reliability."
          }
        ],
        "reasoning": "Explicit mention of deduplication and quality filtering of training data"
      },
      {
        "techniqueId": "tech-csam-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "This process involves, on a\ncase-by-case basis, \ufb01ltering irrelevant or harmful content, text, and other modalities, including \ufb01ltering\ncontent that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "This process involves, on a case-by-case basis, \ufb01ltering irrelevant or harmful content, text, and other modalities, including \ufb01ltering content that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws."
          }
        ],
        "reasoning": "Direct statement about filtering out child sexual abuse material during data processing"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation types included but were not limited to:\n\n\u25cf  Training/Development Evaluations including automated and human evaluations carried out\n\ncontinuously throughout and after the model\u2019s training, to monitor its progress and\nperformance;\n\n\u25cf  Human Red Teaming conducted by specialist teams who sit outside of the model development\nteam, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the\nmodel adheres to safety policies and desired outcomes;\n\n\u25cf  Automated Red Teaming to dynamically evaluate Gemini for safety and security\n\nconsiderations at scale, complementing human red teaming and static evaluations;\n\n\u25cf  Ethics & Safety Reviews were conducted ahead of the model\u2019s release\n\nIn addition, we perform testing following the guidelines in Google DeepMind\u2019s Frontier Safety\nFramework (FSF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Human Red Teaming conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes"
          }
        ],
        "reasoning": "Explicit description of human red teaming process by specialist teams"
      },
      {
        "techniqueId": "tech-automated-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation types included but were not limited to:\n\n\u25cf  Training/Development Evaluations including automated and human evaluations carried out\n\ncontinuously throughout and after the model\u2019s training, to monitor its progress and\nperformance;\n\n\u25cf  Human Red Teaming conducted by specialist teams who sit outside of the model development\nteam, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the\nmodel adheres to safety policies and desired outcomes;\n\n\u25cf  Automated Red Teaming to dynamically evaluate Gemini for safety and security\n\nconsiderations at scale, complementing human red teaming and static evaluations;\n\n\u25cf  Ethics & Safety Reviews were conducted ahead of the model\u2019s release\n\nIn addition, we perform testing following the guidelines in Google DeepMind\u2019s Frontier Safety\nFramework (FSF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Automated Red Teaming to dynamically evaluate Gemini for safety and security considerations at scale, complementing human red teaming and static evaluations"
          }
        ],
        "reasoning": "Direct mention of automated red teaming techniques"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3\n\n\fEvaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal\ncapabilities, agentic tool use, multi-lingual performance, and long-context.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context."
          }
        ],
        "reasoning": "Description of comprehensive safety and performance benchmarking"
      },
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The post-training dataset included\ndi\ufb00erent types of instruction tuning data  reinforcement learning data, and human-preference data.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The post-training dataset included di\ufb00erent types of instruction tuning data reinforcement learning data, and human-preference data. Gemini 3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, problem-solving and theorem-proving data."
          }
        ],
        "reasoning": "Clear description of reinforcement learning from human feedback implementation"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "7\n\n\fFrontier Safety\n\nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025),\nand found that it did not reach any critical capability levels as outlined in the table below:\n\nDomain\n\nKey Results for Gemini 3 Pro\n\nCCL\n\nCBRN\n\nGemini 3 Pro provides accurate and\noccasionally actionable information but\ngenerally fails to o\ufb00er novel or su\ufb03ciently\ncomplete and detailed instructions to\nsigni\ufb01cantly enhance the capabilities of low\nto medium resourced threat actors.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), and found that it did not reach any critical capability levels"
          }
        ],
        "reasoning": "Explicit monitoring of model capabilities against predefined thresholds"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "High-level \ufb01ndings are fed back to the model team. For child safety\nevaluations, Gemini 3 Pro satis\ufb01ed required launch thresholds, which were developed by expert teams\nto protect children online and meet Google\u2019s commitments to child safety across our models and\nGoogle products.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "High-level \ufb01ndings are fed back to the model team. For child safety evaluations, Gemini 3 Pro satis\ufb01ed required launch thresholds, which were developed by expert teams to protect children online and meet Google's commitments to child safety across our models and Google products."
          }
        ],
        "reasoning": "Description of incident reporting and feedback mechanisms"
      }
    ],
    "deletions": []
  },
  "gemini-3-technical-report": {
    "additions": [
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Training and Model Guardrails\n\nWe deploy multiple guardrails to reduce the risk of Gemini 3 Pro generating harmful content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We deploy multiple guardrails to reduce the risk of Gemini 3 Pro generating harmful content. These may include query filters that guide the model's responses to certain inputs, fine-tuning processes that align model outputs with safety guidelines, and filtering and processing of inputs."
          }
        ],
        "reasoning": "Direct description of input guardrail systems with specific implementation details"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We mitigate against prompt injection attacks with a layered defense strategy, which includes measures\nsuch as: prompt injection content classifiers, security through reinforcement, markdown sanitation and\nsuspicious URL redaction, user confirmations, and end-user security mitigation notifications, as\ndescribed in further detail in this recent blog post.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We mitigate against prompt injection attacks with a layered defense strategy, which includes measures such as: prompt injection content classifiers, security through reinforcement, markdown sanitation and suspicious URL redaction, user confirmations, and end-user security mitigation notifications."
          }
        ],
        "reasoning": "Comprehensive description of multi-layered prompt injection defense mechanisms"
      },
      {
        "techniqueId": "tech-multistage-pipeline",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Across this comprehensive landscape, we consider how and when to deploy these different types of\nmitigations throughout the model lifecycle depending on a variety of factors, including the model\u2019s\ncapability and threat modeling, to ensure we deploy a tailored and holistic suite of mitigations.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Across this comprehensive landscape, we consider how and when to deploy these different types of mitigations throughout the model lifecycle depending on a variety of factors, including the model's capability and threat modeling, to ensure we deploy a tailored and holistic suite of mitigations."
          }
        ],
        "reasoning": "Explicit description of a multi-stage, defense-in-depth mitigation approach"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Frontier Safety Summary\n\nWe evaluated Gemini 3 Pro against the CCLs defined in our FSF, which defines explicit risk acceptance\ncriteria for CBRN, cybersecurity, machine learning R&D, and harmful manipulation.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Gemini 3 Pro against the CCLs defined in our FSF, which defines explicit risk acceptance criteria for CBRN, cybersecurity, machine learning R&D, and harmful manipulation."
          }
        ],
        "reasoning": "Detailed description of monitoring model capabilities against predefined thresholds"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Mitigation Assessment\n\nInternal and external red team efforts continually test the efficacy of the mitigations, including their\nrobustness to universal and query-specific jailbreaks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Internal and external red team efforts continually test the efficacy of the mitigations, including their robustness to universal and query-specific jailbreaks. Feedback from these red teams is used to improve the suite of mitigations."
          }
        ],
        "reasoning": "Clear implementation of red team testing and iterative improvement process"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Frontier Safety Summary\n\nWe evaluated Gemini 3 Pro against the CCLs defined in our FSF, which defines explicit risk acceptance\ncriteria for CBRN, cybersecurity, machine learning R&D, and harmful manipulation.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Gemini 3 Pro against the CCLs defined in our FSF, which defines explicit risk acceptance criteria for CBRN, cybersecurity, machine learning R&D, and harmful manipulation."
          }
        ],
        "reasoning": "Systematic use of standardized safety benchmarks across multiple risk domains"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Interventions are designed to prevent violative model responses while allowing benign responses. We\nconsider a response to be violative if it helps with attacks in a concrete way.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Interventions are designed to prevent violative model responses while allowing benign responses. We consider a response to be violative if it helps with attacks in a concrete way."
          }
        ],
        "reasoning": "Indication of training the model to refuse potentially harmful outputs"
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In all domains, we performed automated checks for sandbagging (deliberate underperformance in\norder to avoid being flagged as dangerous van der Weij et al.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We performed automated checks for sandbagging (deliberate underperformance in order to avoid being flagged as dangerous van der Weij et al. 2025). In ML R&D and misalignment, we also carried out manual inspection of transcripts."
          }
        ],
        "reasoning": "Detailed description of continuous monitoring and logging of model behaviors"
      }
    ],
    "deletions": []
  },
  "google-ai-principles-2024": {
    "additions": [
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Red teaming, also referred to as adversarial testing,\nis a technique where \u201cethical hackers\u201d intentionally\nviolate policies for the purpose of discovering and\naddressing vulnerabilities which could harm users.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red teaming, also referred to as adversarial testing, is a technique where 'ethical hackers' intentionally violate policies for the purpose of discovering and addressing vulnerabilities which could harm users."
          }
        ],
        "reasoning": "The document provides a clear definition and describes specific red teaming activities like internal 'Hack-AI-thons' and external testing at conferences like DEF CON."
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We\nhave also included standard academic benchmarks\nfor the capabilities of models, such as GSMK8, MMLU,\n\nHumanEval, and MATH, in our model cards (see\nGemini, Gemini 1.5, and Gemma) so that the reporting\nof model capabilities, limitations, and testing results is\nconsistent and auditable.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have also included standard academic benchmarks for the capabilities of models, such as GSMK8, MMLU, HumanEval, and MATH, in our model cards... We're continuously improving how we measure AI safety as industry benchmark tools emerge."
          }
        ],
        "reasoning": "The document explicitly describes using standardized benchmarks for model evaluation and mentions ongoing work to develop safety benchmarks."
      },
      {
        "techniqueId": "tech-watermarking",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "As part of our commitment to user context, we\ndeveloped SynthID to detect and watermark\nAI-generated content made with our services.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We developed SynthID to detect and watermark AI-generated content made with our services. The technology enables our models to attach an invisible mark to the content they generate that denotes it was created with Google AI."
          }
        ],
        "reasoning": "Clear implementation of a watermarking technique for AI-generated content across multiple modalities."
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022  Feedback channels and operational support\nfor user feedback to improve the model and\naddress issues.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We provide feedback channels and operational support for user feedback to improve the model and address issues."
          }
        ],
        "reasoning": "The document mentions feedback mechanisms that could support incident reporting, though details are limited."
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u201cAcross the development and deployment\nlifecycle of our AI technologies, we\nuse robust security and safety controls,\nwhich we adapt to risks for specific\nproducts and users.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use robust security and safety controls, which we adapt to risks for specific products and users... We monitor input and output, production traffic, and insider access patterns."
          }
        ],
        "reasoning": "The document suggests input monitoring and guardrail systems, though implementation specifics are not fully detailed."
      }
    ],
    "deletions": []
  },
  "gpt-4o-system-card": {
    "additions": [
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We post-trained GPT-4o to refuse to comply with requests to\nidentify someone based on a voice in an audio input, while still\ncomplying with requests to identify famous quotes.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We post-trained GPT-4o to refuse to comply with requests to identify someone based on a voice in an audio input, while still complying with requests to identify famous quotes."
          }
        ],
        "reasoning": "Direct quote describes explicit training to refuse certain types of requests"
      },
      {
        "techniqueId": "tech-csam-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "At\nthe same time, certain pre-training filtering mitigations can provide an additional layer of defense\nthat, along with other safety mitigations, help exclude unwanted and harmful information from\nour datasets:\n\n\u2022 We use our Moderation API and safety classifiers to filter out data that could contribute to\nharmful content or information hazards, including CSAM, hateful content, violence, and\nCBRN.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use our Moderation API and safety classifiers to filter out data that could contribute to harmful content or information hazards, including CSAM, hateful content, violence, and CBRN."
          }
        ],
        "reasoning": "Clear statement of CSAM detection during data filtering process"
      },
      {
        "techniqueId": "tech-copyright-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We trained GPT-4o to refuse requests for copyrighted content, including\naudio, consistent with our broader practices.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We trained GPT-4o to refuse requests for copyrighted content, including audio, consistent with our broader practices."
          }
        ],
        "reasoning": "Explicit description of copyright content filtering during model training"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "At\nthe same time, certain pre-training filtering mitigations can provide an additional layer of defense\nthat, along with other safety mitigations, help exclude unwanted and harmful information from\nour datasets:\n\n\u2022 We use our Moderation API and safety classifiers to filter out data that could contribute to\nharmful content or information hazards, including CSAM, hateful content, violence, and\nCBRN.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use our Moderation API and safety classifiers to filter out data that could contribute to harmful content or information hazards, including CSAM, hateful content, violence, and CBRN."
          }
        ],
        "reasoning": "Direct statement about filtering violent content during data preparation"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "At\nthe same time, certain pre-training filtering mitigations can provide an additional layer of defense\nthat, along with other safety mitigations, help exclude unwanted and harmful information from\nour datasets:\n\n\u2022 We use our Moderation API and safety classifiers to filter out data that could contribute to\nharmful content or information hazards, including CSAM, hateful content, violence, and\nCBRN.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use our Moderation API and safety classifiers to filter out data that could contribute to harmful content or information hazards, including CSAM, hateful content, violence, and CBRN."
          }
        ],
        "reasoning": "Clear indication of hate speech detection in data filtering process"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Additionally, we run our existing moderation model over a text transcription of both audio input\nand audio output to detect if either contains potentially harmful language, and will block a\ngeneration if so8.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run our existing moderation model over a text transcription of the audio input to detect if it contains a request for violent or erotic content, and will block a generation if so."
          }
        ],
        "reasoning": "Explicit description of sexual content moderation mechanism"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We run our existing moderation classifier over text transcrip-\ntions of audio prompts, and block the output if the prompt\ncontains erotic or violent language.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run our existing moderation classifier over text transcriptions of audio prompts and generations, and block the output for certain high-severity categories."
          }
        ],
        "reasoning": "Direct description of input classification and filtering infrastructure"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We run our existing moderation classifier over text transcrip-\ntions of audio prompts, and block the output if the prompt\ncontains erotic or violent language.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run our existing moderation classifier over text transcriptions of audio prompts and generations, and block the output for certain high-severity categories."
          }
        ],
        "reasoning": "Clear implementation of output safety filtering system"
      },
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Risk Mitigation: We post-trained GPT-4o with a diverse set of input voices to have model\nperformance and behavior be invariant across different user voices.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We post-trained GPT-4o with a diverse set of input voices to have model performance and behavior be invariant across different user voices."
          }
        ],
        "reasoning": "Explicit description of safety alignment techniques for multimodal inputs"
      },
      {
        "techniqueId": "tech-misinformation-detection",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Misinformation and conspiracy theories: Red teamers were able to compel the model to\ngenerate inaccurate information by prompting it to verbally repeat false information and produce\nconspiracy theories.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red teamers were able to compel the model to generate inaccurate information by prompting it to verbally repeat false information and produce conspiracy theories."
          }
        ],
        "reasoning": "Evidence of awareness and testing for misinformation, though mitigation details are limited"
      }
    ],
    "deletions": []
  },
  "gpt-5-system-card": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "OpenAI reasoning models, including gpt-5-thinking, gpt-5-thinking-mini, and gpt-5-thinking-nano,\nare trained to reason through reinforcement learning.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "OpenAI reasoning models, including gpt-5-thinking, gpt-5-thinking-mini, and gpt-5-thinking-nano, are trained to reason through reinforcement learning."
          }
        ],
        "reasoning": "Direct statement of using reinforcement learning for model training"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Refuse all requests for weaponization assistance\n\n2. Never provide detailed actionable assistance on dual use topics.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We trained the model to: 1. Refuse all requests for weaponization assistance 2. Never provide detailed actionable assistance on dual use topics."
          }
        ],
        "reasoning": "Explicit description of training models to refuse certain types of harmful requests"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "49\n\n\f5.3.3.3 Expert Red Teaming for Bioweaponization\n\nWe conducted three distinct red team campaigns to assess gpt-5-thinking\u2019s safety against\nbioweaponization applications.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conducted three distinct red team campaigns to assess gpt-5-thinking's safety against bioweaponization applications."
          }
        ],
        "reasoning": "Detailed description of multiple red teaming campaigns used to test model safety"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.1.2.1 Capture the Flag (CTF) Challenges\n\nWe evaluate model performance on a test set of over 100 curated, publicly available CTF challenges\nwhich met the following criteria:\n\n\u2022 The challenge required skills falling into one of the following categories:\n\n\u2013 Web Application Exploitation (web): the flag is stored on a virtual machine running\nvulnerable software which must be exploited over an exposed network service to run\ncode to retrieve it.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate models on a test set of over 100 curated, publicly available CTF challenges which met the following criteria:"
          }
        ],
        "reasoning": "Systematic use of standardized benchmarks to evaluate model safety and capabilities"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Instead, we\u2019ve deployed a two tiered system of real-time,\nautomated oversight surrounding the model to monitor and block unsafe prompts and generations:\n\n\u2022 The first tier in this system is a fast, topical classifier model that determines whether or not\nthe content is related to biology.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We've deployed a two tiered system of real-time, automated oversight surrounding the model to monitor and block unsafe prompts and generations: 1. A fast, topical classifier model 2. A reasoning monitor model that determines content risk."
          }
        ],
        "reasoning": "Explicit description of a multi-layer output filtering and monitoring system"
      },
      {
        "techniqueId": "tech-hallucination-grounding",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Table 7: Prompt Injection Evaluations\n\nEvaluation (higher is better)\n\ngpt-5-thinking\n\nOpenAI o3\n\nBrowsing prompt injections\n\nTool calling prompt injections\n\nCoding prompt injections\n\n0.99\n\n0.99\n\n0.97\n\n0.89\n\n0.80\n\n0.94\n\n3.7 Hallucinations\n\nOne of our focuses when training the GPT-5 models was to reduce the frequency of factual\nhallucinations.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "One of our focuses when training the GPT-5 models was to reduce the frequency of factual hallucinations. We first evaluate the factual correctness of gpt-5-thinking and gpt-5-main on prompts representative of real ChatGPT production conversations."
          }
        ],
        "reasoning": "Detailed approach to detecting and reducing model hallucinations through systematic evaluation"
      },
      {
        "techniqueId": "tech-multistage-pipeline",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Instead, we\u2019ve deployed a two tiered system of real-time,\nautomated oversight surrounding the model to monitor and block unsafe prompts and generations:\n\n\u2022 The first tier in this system is a fast, topical classifier model that determines whether or not\nthe content is related to biology.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We've deployed a two tiered system of real-time, automated oversight surrounding the model to monitor and block unsafe prompts and generations."
          }
        ],
        "reasoning": "Implementation of a multi-stage defense mechanism with multiple layers of safety checks"
      },
      {
        "techniqueId": "tech-misinformation-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We first evaluate the factual correctness of gpt-5-thinking and gpt-5-main on prompts representa-\ntive of real ChatGPT production conversations, using an LLM-based grading model with web\naccess to identify major and minor factual errors in the assistant\u2019s responses.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We first evaluate the factual correctness of gpt-5-thinking and gpt-5-main on prompts representative of real ChatGPT production conversations, using an LLM-based grading model with web access to identify major and minor factual errors in the assistant's responses."
          }
        ],
        "reasoning": "Systematic approach to detecting misinformation through factual error checking"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We use advanced data filtering processes to reduce personal information\nfrom training data. We also employ a combination of our Moderation API and safety classifiers\nto help prevent the use of harmful or sensitive content, including explicit materials such as sexual\ncontent involving a minor.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use advanced data filtering processes to reduce personal information from training data."
          }
        ],
        "reasoning": "Direct statement of PII detection and removal during data preprocessing"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "For GPT-5, we post-trained our models to reduce sycophancy. Using conversations\nrepresentative of production data, we evaluated model responses, then assigned a score reflecting\nthe level of sycophancy, which was used as a reward signal in training.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We post-trained our models to reduce sycophancy. Using conversations representative of production data, we evaluated model responses, then assigned a score reflecting the level of sycophancy, which was used as a reward signal in training."
          }
        ],
        "reasoning": "Explicit use of reward modeling to optimize for specific safety behaviors"
      }
    ],
    "deletions": []
  },
  "grok-4": {
    "additions": [
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We define a basic refusal policy which instructs Grok 4 to\ndecline queries demonstrating clear intent to engage in activities that threaten severe, imminent harm\nto others, including violent crimes, child sexual exploitation, fraud, hacking, and more.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We define a basic refusal policy which instructs Grok 4 to decline queries demonstrating clear intent to engage in activities that threaten severe, imminent harm to others, including violent crimes, child sexual exploitation, fraud, hacking, and more."
          }
        ],
        "reasoning": "Explicit description of a refusal policy designed to prevent the model from responding to harmful requests"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "With Grok 4\u2019s strong reasoning and instruction-following capabilities, we find\nthat including our basic refusal policy in the system prompt greatly reduces response rate on harmful\nqueries.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "With Grok 4's strong reasoning and instruction-following capabilities, we find that including our basic refusal policy in the system prompt greatly reduces response rate on harmful queries."
          }
        ],
        "reasoning": "Direct implementation of system prompts to guide model behavior and reduce harmful responses"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We also employ model-based filters for both Grok 4 API and Grok 4 Web, which reject classes of harmful requests, including biological and chemical weapons, self-harm, and CSAM."
          }
        ],
        "reasoning": "Explicit description of input filtering mechanisms to block harmful requests"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Additionally, warning the model against jailbreak attacks serves to significantly inoculate\nagainst common jailbreak strategies.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Additionally, warning the model against jailbreak attacks serves to significantly inoculate against common jailbreak strategies."
          }
        ],
        "reasoning": "Direct mention of defense mechanisms against prompt injection and jailbreak attempts"
      },
      {
        "techniqueId": "tech-contextual-safety",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Given the limited context visible to AI models, it is often difficult to distinguish\nmalignant intent from mere curiosity.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Given the limited context visible to AI models, it is often difficult to distinguish malignant intent from mere curiosity. We define a basic refusal policy which instructs Grok 4 to decline queries demonstrating clear intent to engage in activities that threaten severe, imminent harm to others."
          }
        ],
        "reasoning": "Suggests a nuanced approach to safety that considers context and intent"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2.3 Dual-use Capabilities\n\nIn this section, we evaluate the possibility of our model enabling malicious actors to design, synthesize,\nacquire, or use chemical and biological weapons or offensive cyber operations (e.g., troubleshooting\nvirology lab or reverse engineering binaries).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate the possibility of our model enabling malicious actors to design, synthesize, acquire, or use chemical and biological weapons or offensive cyber operations... We prioritize addressing bioweapons risks over others because they have the potential for the greatest scale of harm."
          }
        ],
        "reasoning": "Detailed evaluation of model capabilities with a focus on monitoring potential misuse"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To measure willingness to assist with serious crimes, we constructed a broad set of\nharmful queries demonstrating clear intent to engage in a range of criminal offenses against people,\nproperty, and society and translated them across several common languages (English, Spanish,\nChinese, Japanese, Arabic, Russian), totaling thousands of queries.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We constructed a broad set of harmful queries demonstrating clear intent to engage in a range of criminal offenses against people, property, and society and translated them across several common languages... We used another model to grade whether the model responses correctly refuse to answer these queries."
          }
        ],
        "reasoning": "Comprehensive red teaming approach involving systematic testing of model responses"
      }
    ],
    "deletions": []
  },
  "grok-image-gen-update": {
    "additions": [],
    "deletions": []
  },
  "grok-security": {
    "additions": [
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI partners with GitHub's Secret Scanning program to detect leaked keys. If a leak is found, we disable the key and notify you via email.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI partners with GitHub's Secret Scanning program to detect leaked keys. If a leak is found, we disable the key and notify you via email."
          }
        ],
        "reasoning": "The document explicitly describes a systematic process for detecting and responding to security incidents involving API keys."
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Team admins are able to view an audit log of user interactions. This lists all of the user interactions with our API server.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Team admins are able to view an audit log of user interactions. This lists all of the user interactions with our API server."
          }
        ],
        "reasoning": "The document describes a comprehensive audit logging system for tracking user interactions in real-time."
      },
      {
        "techniqueId": "tech-data-retention-policies",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "API requests and responses are temporarily stored on our servers for 30 days in case they need to be audited for potential abuse or misuse. This data is automatically deleted after 30 days.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "API requests and responses are temporarily stored on our servers for 30 days in case they need to be audited for potential abuse or misuse. This data is automatically deleted after 30 days."
          }
        ],
        "reasoning": "The document outlines a specific data retention and automatic deletion policy for API interactions."
      },
      {
        "techniqueId": "tech-access-control-documentation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Do not share keys between teammates to avoid unauthorized access. Store keys securely using environment variables or secret management tools.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Do not share keys between teammates to avoid unauthorized access. Store keys securely using environment variables or secret management tools."
          }
        ],
        "reasoning": "The document provides explicit guidance on API key management and access control best practices."
      }
    ],
    "deletions": []
  },
  "hunyuan-technical-report": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.2 Reinforcement Learning from Human Feedback\n\nTo align Hunyuan-Large with human preferences, we further train our SFT model using DPO (Rafailov\net al., 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We further train our SFT model using DPO (Rafailov et al., 2024). We adopt a single-stage training strategy that integrates both offline and online training, which demonstrates superior controllability and overall performance."
          }
        ],
        "reasoning": "Explicit description of Reinforcement Learning from Human Feedback (RLHF) implementation using Direct Preference Optimization (DPO)"
      },
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To enhance training stability, we incorporate an SFT loss term on the chosen response, similar to the\napproaches in (Dubey et al., 2024; Adler et al., 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "To enhance training stability, we incorporate an SFT loss term on the chosen response, similar to the approaches in (Dubey et al., 2024; Adler et al., 2024). This addition helps stabilize DPO training by preventing a decrease in the log probability of chosen responses."
          }
        ],
        "reasoning": "Detailed implementation of Direct Preference Optimization with specific stabilization techniques"
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We aim to create a high-quality, safe, and diverse training dataset\nfor pre-training, primarily consisting of Chinese and English languages for practical demands.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We aim to create a high-quality, safe, and diverse training dataset for pre-training, primarily consisting of Chinese and English languages for practical demands. We filter the data based on criteria such as writing quality, educational value, and toxicity to ensure its high quality."
          }
        ],
        "reasoning": "Describes a process of auditing and filtering training data for quality and safety"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We discover some common issues such as data truncation errors, duplication,\ngarbled characters, and format errors in SFT data.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We discover some common issues such as data truncation errors, duplication, garbled characters, and format errors in SFT data. Consequently, we develop a set of rule-based data filtering strategies to prevent the above instruction extraction and generation models from producing undesirable outputs."
          }
        ],
        "reasoning": "Explicit description of comprehensive data quality filtering techniques"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "we anonymize all privacy-sensitive data and other harmful data.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We anonymize all privacy-sensitive data and other harmful data."
          }
        ],
        "reasoning": "Brief mention of PII handling, but limited implementation details"
      }
    ],
    "deletions": []
  },
  "llama-3-paper": {
    "additions": [
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.1.4 Direct Preference Optimization\n\nWe further train our SFT models with Direct Preference Optimization (DPO; Rafailov et al., 2024) for human\npreference alignment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We further train our SFT models with Direct Preference Optimization (DPO; Rafailov et al., 2024) for human preference alignment."
          }
        ],
        "reasoning": "Explicit description of using Direct Preference Optimization during model post-training"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.1.2 Reward Modeling\n\nWe train a reward model (RM) covering different capabilities on top of the pre-trained checkpoint.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We train a reward model (RM) covering different capabilities on top of the pre-trained checkpoint. The training objective is the same as Llama 2 except that we remove the margin term in the loss."
          }
        ],
        "reasoning": "Detailed description of training a reward model specifically for safety and alignment"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We developed a refusal tone guideline for Llama 3 and ensured that all new safety data\nadhered to it through rigorous quality assurance process.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We developed a refusal tone guideline for Llama 3 and ensured that all new safety data adhered to it through rigorous quality assurance process."
          }
        ],
        "reasoning": "Explicit mention of training the model to refuse unsafe requests with a specific tone"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We apply several de-duplication methods and data cleaning mechanisms on each data\nsource to obtain high-quality tokens.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We apply several de-duplication methods and data cleaning mechanisms on each data source to obtain high-quality tokens. We remove domains that contain large amounts of personally identifiable information (PII), and domains with known adult content."
          }
        ],
        "reasoning": "Comprehensive description of data filtering and cleaning techniques during pre-training"
      },
      {
        "techniqueId": "tech-csam-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We scan all our training images for\nCSAM using perceptual hashing approaches such as PhotoDNA (Farid, 2021) as well as internal, proprietary\nclassifiers.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We scan all our training images for CSAM using perceptual hashing approaches such as PhotoDNA as well as internal, proprietary classifiers."
          }
        ],
        "reasoning": "Explicit description of methods to detect and remove child sexual abuse material from training data"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We also use a proprietary media-risk retrieval pipeline to identify and remove image-text pairs\nthat we consider to be NSFW, for example, because they contain sexual or violent content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use a proprietary media-risk retrieval pipeline to identify and remove image-text pairs that we consider to be NSFW, for example, because they contain sexual or violent content."
          }
        ],
        "reasoning": "Clear implementation of violence detection in image-text training data"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Among other mitigations, we implement filters designed to remove data from websites\nare likely to contain unsafe content or high volumes of PII, domains that have been ranked as harmful\naccording to a variety of Meta safety standards, and domains that are known to contain adult content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We implement filters designed to remove data from websites are likely to contain unsafe content or high volumes of PII, domains that have been ranked as harmful according to a variety of Meta safety standards."
          }
        ],
        "reasoning": "Explicit filtering of hate speech and harmful content during data preparation"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To enable this, we develop and release a new classifier, Llama Guard 3, which is a Llama 3 8B model fine-tuned\nfor safety classification.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We develop and release a new classifier, Llama Guard 3, which is a Llama 3 8B model fine-tuned for safety classification. This classifier is used to detect whether input prompts and/or output responses generated by language models violate safety policies on specific categories of harm."
          }
        ],
        "reasoning": "Detailed description of an input guardrail system specifically designed for safety classification"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Llama Guard 3 is able to significantly reduce violations across capabilities (-65% violations on average\nacross our benchmarks).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Llama Guard 3 is able to significantly reduce violations across capabilities (-65% violations on average across our benchmarks)."
          }
        ],
        "reasoning": "Explicit implementation of an output filtering system with quantitative safety improvements"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Prompt Guard is a model-based filter designed to detect prompt attacks, which are input strings designed to\nsubvert the intended behavior of an LLM functioning as part of an application.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prompt Guard is a model-based filter designed to detect prompt attacks, which are input strings designed to subvert the intended behavior of an LLM functioning as part of an application."
          }
        ],
        "reasoning": "Detailed description of a specific mechanism to defend against prompt injection attacks"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.4.6 Red Teaming\n\nWe utilize Red Teaming to discover risks and use the findings to improve our benchmarks and safety tuning\ndatasets.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We utilize Red Teaming to discover risks and use the findings to improve our benchmarks and safety tuning datasets. We conduct recurring red teaming exercises to continuously iterate and discover new risks, which guides our model development and mitigation process."
          }
        ],
        "reasoning": "Explicit description of systematic red teaming process for identifying safety risks"
      },
      {
        "techniqueId": "tech-automated-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We also leveraged advanced, adversarial multi-turn automa-\ntion similar to PAIR (Chao et al., 2023) across some techniques and risk categories.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We utilize advanced, adversarial multi-turn automation similar to PAIR to test across techniques and risk categories."
          }
        ],
        "reasoning": "Clear indication of using automated techniques for red teaming and risk discovery"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.4.1 Benchmark Construction\n\nWe create various internal benchmarks to help us develop models safely and responsibly.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We create various internal benchmarks to help us develop models safely and responsibly. Our benchmarks are heavily inspired by the risk categories from the ML Commons taxonomy of hazards."
          }
        ],
        "reasoning": "Detailed description of creating comprehensive safety benchmarks for model evaluation"
      }
    ],
    "deletions": []
  },
  "llama-4-maverick": {
    "additions": [
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Llama Guard models offer unparalleled safety content moderation performance and flexibility, and we now offer a collection of specialized models tailored to specific development needs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Llama Guard models offer unparalleled safety content moderation performance and flexibility, and we now offer a collection of specialized models tailored to specific development needs."
          }
        ],
        "reasoning": "Explicit description of Llama Guard as an input guardrail system with multiple specialized models"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Download the model\nGet started\nPrompt Guard 2\nPrompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity. Categories of prompt attacks include prompt injection and jailbreaking."
          }
        ],
        "reasoning": "Direct implementation of a tool specifically designed to defend against prompt injection attacks"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In line with the principles outlined in our Developer Use Guide: AI Protections, we recommend thorough checking and filtering of all inputs to and outputs from LLMs based on your unique content guidelines for your intended use case and audience.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "In line with the principles outlined in our Developer Use Guide: AI Protections, we recommend thorough checking and filtering of all inputs to and outputs from LLMs based on your unique content guidelines for your intended use case and audience."
          }
        ],
        "reasoning": "Weak evidence of system-level guidance, but not a clear implementation of system prompts"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Llama Guard 4 is an 12B-parameter high-performance multimodal input and output moderation model designed to support developers to detect various common types of violating content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Llama Guard 4 is an 12B-parameter high-performance multimodal input and output moderation model designed to support developers to detect various common types of violating content."
          }
        ],
        "reasoning": "Explicit description of a multimodal output filtering system"
      },
      {
        "techniqueId": "tech-code-execution-sandboxing",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Code Shield provides support for inference-time filtering of insecure code produced by LLMs. This offers mitigation of insecure code suggestions risk and secure command execution for 7 programming languages with an average latency of 200ms.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Code Shield provides support for inference-time filtering of insecure code produced by LLMs. This offers mitigation of insecure code suggestions risk and secure command execution for 7 programming languages with an average latency of 200ms."
          }
        ],
        "reasoning": "Clear implementation of a code execution sandboxing mechanism"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Prompt Guard is designed to perform strongly and generalize well to new settings and distributions of adversarial attacks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prompt Guard is designed to perform strongly and generalize well to new settings and distributions of adversarial attacks. We use a multilingual base model that significantly enhances the model's ability to recognize prompt attacks in non-English languages."
          }
        ],
        "reasoning": "Additional evidence of comprehensive prompt injection defense capabilities"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Cybersec Eval 4 expands on its predecessor by augmenting the suite of benchmarks to measure not only the risks, but also the defensive cybersecurity capabilities of AI systems.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Cybersec Eval 4 expands on its predecessor by augmenting the suite of benchmarks to measure not only the risks, but also the defensive cybersecurity capabilities of AI systems."
          }
        ],
        "reasoning": "Explicit description of a comprehensive safety benchmarking approach"
      }
    ],
    "deletions": []
  },
  "llama-4-responsible-use-guide": {
    "additions": [
      {
        "techniqueId": "tech-configurable-policies",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "8\n\n\fEstablish governance mechanisms: Consider establishing best practice guides and\npolicies within your organization that outline the acceptable use and management of AI\nwithin your organization.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Consider establishing best practice guides and policies within your organization that outline the acceptable use and management of AI within your organization."
          }
        ],
        "reasoning": "The document discusses creating configurable organizational policies for AI use"
      },
      {
        "techniqueId": "tech-stakeholder-engagement",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We are also eager to receive feedback and appreciate the opportunity to contribute to this\nimportant topic while continuing to learn from the broader community.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We are also eager to receive feedback and appreciate the opportunity to contribute to this important topic while continuing to learn from the broader community."
          }
        ],
        "reasoning": "Explicit commitment to gathering external stakeholder input and feedback"
      },
      {
        "techniqueId": "tech-responsible-release",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Consider whether it makes sense\nto pursue international standard certifications, such as ISO 42001, and engage with key\nstakeholders in jurisdictions relevant to your business.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Consider whether it makes sense to pursue international standard certifications, such as ISO 42001, and engage with key stakeholders in jurisdictions relevant to your business."
          }
        ],
        "reasoning": "Describes a structured approach to responsible technology release with stakeholder considerations"
      },
      {
        "techniqueId": "tech-safety-advisory",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Artificial Intelligence Safety\nInstitute Consortium, established by the National Institute of Standards and Technology\n(NIST), and are engaging with multistakeholder organizations, such as the OECD.AI\nworking groups, the Partnership on AI, the Responsible AI Institute, Frontier Model Forum,\nand EqualAI, alongside strategic partnerships with universities on a global scale, to promote\nresponsible AI practices.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We're collaborating with the U.S. Artificial Intelligence Safety Institute Consortium, established by the National Institute of Standards and Technology (NIST), and are engaging with multistakeholder organizations, such as the OECD.AI working groups, the Partnership on AI, the Responsible AI Institute, Frontier Model Forum, and EqualAI"
          }
        ],
        "reasoning": "Demonstrates engagement with independent safety advisory groups and consortia"
      },
      {
        "techniqueId": "tech-regulatory-compliance",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Consider relevant laws and regulations: Engage with legal advisors to assess your use\ncase for compliance with applicable laws and regulations throughout all phases of design,\ndevelopment, deployment, and operation of AI systems.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Engage with legal advisors to assess your use case for compliance with applicable laws and regulations throughout all phases of design, development, deployment, and operation of AI systems."
          }
        ],
        "reasoning": "Explicit description of proactive regulatory compliance processes"
      }
    ],
    "deletions": []
  },
  "meta-llama-responsible-use": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022  Reinforcement Learning from Human Feedback\n\n(RLHF) or AI Feedback (RLAIF): Training safety\n\nand helpfulness reward models to support\n\nRLHF techniques iteratively improves models\n\nand makes them more robust to jailbreaking\n\ntechniques.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Reinforcement Learning from Human Feedback (RLHF) mechanisms. This involves collecting ranking data from trained annotators or users (given a model input and several generated outputs, ranking them from best to worst according to policies), training a reward or helpfulness model to act as a proxy of human feedback, and then optimizing the LLM to maximize the reward/helpfulness model score with reinforcement learning."
          }
        ],
        "reasoning": "Detailed description of RLHF implementation with specific steps for collecting and using human feedback"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Reinforcement Learning from AI Feedback (RLAIF)\n\nReward models can also be improved and tailored to\n\nspecific policies by using Reinforcement Learning\n\nfrom AI Feedback (RLAIF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Reward models can also be improved and tailored to specific policies by using Reinforcement Learning from AI Feedback (RLAIF). The fine-tuned LLM itself can be used to create synthetic ranking data for reward model training."
          }
        ],
        "reasoning": "Explicit description of creating safety-specific reward models using AI-generated feedback"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022  Red teaming is a systematic effort to identify\n\nmodel vulnerabilities or emergent risks by crafting\n\nprompts that may elicit undesirable behavior or\n\noutputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red teaming is a systematic effort to identify model vulnerabilities or emergent risks by crafting prompts that may elicit undesirable behavior or outputs. This type of manipulation of the model can be used to test safeguards and attempts to 'jailbreak' the model."
          }
        ],
        "reasoning": "Clear definition and purpose of red teaming as a safety evaluation technique"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Without\n\nimplementation of input filters and safeguards, even\n\nadvanced models can potentially be manipulated to\n\ngenerate harmful or misleading outputs or violate\n\ncontent policies.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Without implementation of input filters and safeguards, even advanced models can potentially be manipulated to generate harmful or misleading outputs or violate content policies."
          }
        ],
        "reasoning": "Discusses the need for and concept of input guardrail systems"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Based on the downstream use case, you can apply\n\nseveral approaches for detecting and filtering the\n\ngenerated output of models for problematic or policy-\n\nviolating content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Based on the downstream use case, you can apply several approaches for detecting and filtering the generated output of models for problematic or policy-violating content. Here are some considerations and best practices for filtering outputs."
          }
        ],
        "reasoning": "Explicit discussion of output filtering strategies with multiple approaches"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Prompt injection attacks are attempts to circumvent\n\ncontent restrictions to produce particular outputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prompt injection attacks are attempts to circumvent content restrictions to produce particular outputs."
          }
        ],
        "reasoning": "Acknowledges prompt injection as a threat and implies defensive strategies"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "13\n\nJULY 2023\f3\n\nAddress input- and\noutput-level risks\n\nWithout proper safeguards at the input and output\n\nlevels, it is hard to ensure that the model will respond\n\nproperly to adversarial inputs and will be protected\n\nfrom efforts to circumvent content policies and\n\nsafeguard measures (\u201cjailbreaking\u201d).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Without proper safeguards at the input and output levels, it is hard to ensure that the model will respond properly to adversarial inputs and will be protected from efforts to circumvent content policies and safeguard measures ('jailbreaking')."
          }
        ],
        "reasoning": "Implies training to resist inappropriate inputs, though not explicitly detailed"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "ai/projects/safety_recipes/\n\nPlatforms for tools and evaluations:\n\n\u2022  Benchmarking of LLMs by Stanford\u2019s Center for\n\nResearch on Foundation Models, HELM: https://\n\ncrfm.stanford.edu/helm/latest/\n\n\u2022  EleutherAI LLM Evaluation Harness: https://\n\ngithub.com/EleutherAI/lm-evaluation-harness\n\n\u2022  Huggingface Hub which hosts open source\n\nmodels, datasets, and is a space for developers\n\nto share safeguards and access benchmarking\n\ninformation: https://huggingface.co/docs/\n\nhub/index\n\n\u2022  GenAI Ops Tools database curated by Credo.AI:\n\nhttps://www.credo.ai/gen-ai-ops-landscape\n\n20\n\nJULY 2023\fReporting resources:\n\nIf you have any information about issues, violations,\n\n\u2022  Reporting bugs and security concerns:\n\nfacebook.com/whitehat/info\n\nor problems, please help keep our communities safe\n\n\u2022  Reporting violations of the Acceptable\n\nby using our reporting resources.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "If you have any information about issues, violations, or problems, please help keep our communities safe by using our reporting resources."
          }
        ],
        "reasoning": "Explicit establishment of incident reporting mechanisms"
      },
      {
        "techniqueId": "tech-transparency",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Developers can\n\nfurther train the model with this feedback to improve\n\nTransparency & control best practices\n\nTo ensure high-quality feedback and provide end\n\nusers with notice and choice about their interactions\n\nwith your AI assets, developers should consider the\n\nfollowing practices for user interactions:\n\n\u2022  Transparency: Developers should consider ways\n\nto provide transparency to end users regarding\n\npotential risks and limitations of the system\n\nprior to or at the time of user interaction.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Developers should consider ways to provide transparency to end users regarding potential risks and limitations of the system prior to or at the time of user interaction."
          }
        ],
        "reasoning": "Clear commitment to user transparency about AI system limitations"
      }
    ],
    "deletions": []
  },
  "microsoft-rai-standard": {
    "additions": [
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "F1.4 Define and document Responsible Release Criteria to achieve this Goal, as follows:\nFor each metric, document:\n\n1)  any target minimum performance level for all groups, and\n2)  the target maximum (absolute or relative) performance difference between groups.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Define and document Responsible Release Criteria to achieve this Goal, as follows: For each metric, document: 1) any target minimum performance level for all groups, and 2) the target maximum (absolute or relative) performance difference between groups."
          }
        ],
        "reasoning": "The document explicitly describes a structured benchmarking approach with defined performance criteria and metrics for evaluating system safety."
      },
      {
        "techniqueId": "tech-stakeholder-engagement",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Recommendation F1.1.4 Work with members of identified demographic groups to understand the risks of and\nimpacts associated with differences in quality of service.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Work with members of identified demographic groups to understand the risks of and impacts associated with differences in quality of service. Consider using the Community Jury technique to conduct these discussions."
          }
        ],
        "reasoning": "The document recommends direct engagement with stakeholder groups to understand potential system impacts."
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4)  Describe how customers, partners, and end users will be notified of changes to the system, updated\n\nunderstandings of failures, and their best mitigations.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Describe how customers, partners, and end users will be notified of changes to the system, updated understandings of failures, and their best mitigations."
          }
        ],
        "reasoning": "The document outlines a structured process for reporting and communicating system incidents and failures."
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Recommendation F3.4.2 Use red teaming exercises to evaluate these risks involving identified demographic\ngroups.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Use red teaming exercises to evaluate these risks involving identified demographic groups."
          }
        ],
        "reasoning": "The document recommends red teaming as a technique for evaluating system risks, though implementation details are not fully specified."
      },
      {
        "techniqueId": "tech-contextual-safety",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "RS1.3 Determine and document the operational factors, including quality of system input, use, and operational\ncontext that are critical to manage for reliable and safe use of the system in its deployed context.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Determine and document the operational factors, including quality of system input, use, and operational context that are critical to manage for reliable and safe use of the system in its deployed context."
          }
        ],
        "reasoning": "The document describes a contextual approach to safety that considers multiple operational factors and use contexts."
      }
    ],
    "deletions": []
  },
  "mistral-large-3": {
    "additions": [
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Violence and Threats\nContent that describes, glorifies, incites, or threatens physical violence against individuals or groups.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Violence and Threats: Content that describes, glorifies, incites, or threatens physical violence against individuals or groups. This includes graphic depictions of injury or death, explicit threats of harm, and instructions for carrying out violent acts."
          }
        ],
        "reasoning": "Detailed category description in moderation API explicitly covers violence detection"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Hate and Discrimination\nContent that expresses prejudice, hostility, or advocates discrimination against individuals or groups based on protected characteristics such as race, ethnicity, religion, gender, sexual orientation, or disability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Hate and Discrimination: Content that expresses prejudice, hostility, or advocates discrimination against individuals or groups based on protected characteristics such as race, ethnicity, religion, gender, sexual orientation, or disability."
          }
        ],
        "reasoning": "Comprehensive definition of hate speech detection in moderation API"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Category\nDescription\nSexual\nMaterial that explicitly depicts, describes, or promotes sexual activities, nudity, or sexual services.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Sexual: Material that explicitly depicts, describes, or promotes sexual activities, nudity, or sexual services. This includes pornographic content, graphic descriptions of sexual acts, and solicitation for sexual purposes."
          }
        ],
        "reasoning": "Explicit category for detecting and filtering sexual content in moderation API"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "PII\nContent that requests, shares, or attempts to elicit personal identifying information such as full names, addresses, phone numbers, social security numbers, or financial account details.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "PII: Content that requests, shares, or attempts to elicit personal identifying information such as full names, addresses, phone numbers, social security numbers, or financial account details."
          }
        ],
        "reasoning": "Specific category for detecting personally identifiable information in moderation API"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Endpoints\nEndpoints\nWe are releasing two end-points: one to classify\nraw text\nand one to classify\nconversational content\n.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We are releasing two end-points: one to classify raw text and one to classify conversational content. More details below. The raw text endpoint allows you to moderate text chunks directly, it will a score for different categories allowing classification of the text."
          }
        ],
        "reasoning": "Explicit description of input guardrail classification endpoints"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We introduce an optional system prompt to enforce guardrails on top of our models. You can activate this prompt through a\nsafe_prompt\nboolean flag in API calls as follows :\ntip\nBefore continuing, we recommend reading the\nChat Competions\ndocumentation to learn more about the chat completions API and how to use it before proceeding.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We introduce an optional system prompt to enforce guardrails on top of our models. You can activate this prompt through a safe_prompt boolean flag in API calls... Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content."
          }
        ],
        "reasoning": "Detailed implementation of system prompts for safety guardrailing"
      },
      {
        "techniqueId": "tech-self-harm-prevention",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Self-Harm\nContent that promotes, instructs, plans, or encourages deliberate self-injury, suicide, eating disorders, or other self-destructive behaviors.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Self-Harm: Content that promotes, instructs, plans, or encourages deliberate self-injury, suicide, eating disorders, or other self-destructive behaviors."
          }
        ],
        "reasoning": "Explicit category for detecting and preventing self-harm content"
      },
      {
        "techniqueId": "tech-illegal-activity-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Dangerous and Criminal Content\nContent that promotes or provides instructions for illegal activities or extremely hazardous behaviors that pose a significant risk of physical harm, death, or legal consequences.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Dangerous and Criminal Content: Content that promotes or provides instructions for illegal activities or extremely hazardous behaviors that pose a significant risk of physical harm, death, or legal consequences."
          }
        ],
        "reasoning": "Comprehensive category for detecting illegal activity instructions"
      }
    ],
    "deletions": []
  },
  "nemotron-4-tech-report": {
    "additions": [
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We align the base LLM with Supervised Fine-Tuning (SFT), followed by Preference Fine-Tuning such as\nReinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022) and Direct\nPreference Optimization (DPO) (Rafailov et al., 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We align the base LLM with Supervised Fine-Tuning (SFT), followed by Preference Fine-Tuning such as Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO)."
          }
        ],
        "reasoning": "Explicit description of DPO as an alignment technique used in model training"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The values marked with \u2217 are taken from Qwen-Team\n(2024)\n\n3 Alignment\n\n3.1 Reward Modeling\n\nThe reward model plays a pivotal role in model alignment, serving as a crucial judge for preference ranking\nand quality filtering in the training of a strong instruction-following model.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The reward model plays a pivotal role in model alignment, serving as a crucial judge for preference ranking and quality filtering in the training of a strong instruction-following model."
          }
        ],
        "reasoning": "Detailed explanation of using a reward model specifically for safety and quality assessment during alignment"
      },
      {
        "techniqueId": "tech-automated-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "(2024), Generative AI Red-teaming & Assessment Kit, is a vulnerability scanner for Large\n\n17\n\n\fFigure 6: Percentage of unsafe responses over all model responses in AEGIS safety evaluations.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Garak, Generative AI Red-teaming & Assessment Kit, is a vulnerability scanner for Large Language Models. It identifies a broad range of security weaknesses and unwanted behaviors in language model-based technology."
          }
        ],
        "reasoning": "Explicit use of an automated red teaming tool to probe model vulnerabilities"
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2 Pretraining\n\n2.1 Data\n\nOur pretraining data blend consists of three different types of data: English natural language data (70%),\nmultilingual natural language data (15%), and source code data (15%).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Our pretraining data blend consists of three different types of data: English natural language data (70%), multilingual natural language data (15%), and source code data (15%)."
          }
        ],
        "reasoning": "Demonstrates careful auditing and composition of training data across different domains"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Furthermore, we utilize Nemotron-\n4-340B-Reward to assess the quality of dialogues, assigning a score to each sample and filtering out those\nthat fall below a predetermined threshold.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We utilize Nemotron-4-340B-Reward to assess the quality of dialogues, assigning a score to each sample and filtering out those that fall below a predetermined threshold."
          }
        ],
        "reasoning": "Explicit description of using the reward model to filter low-quality training data"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.4.3 Safety Evaluations\n\nWe performed extensive safety evaluation including adversarial testing via these distinct methods:\n\n\u2022 AEGIS (Ghosh et al., 2024) is a content safety evaluation dataset and LLM based content safety\nclassifier model, that adheres to a broad taxonomy of 13 categories of critical risks in human-LLM\ninteractions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We performed extensive safety evaluation including adversarial testing via these distinct methods: AEGIS, Garak, and Human Content Red Teaming leveraging human interaction and evaluation of the models' responses."
          }
        ],
        "reasoning": "Comprehensive red teaming approach using multiple evaluation methods"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To\nevaluate the safety of our model, we employ AEGIS (Ghosh et al., 2024), a high quality content safety\nsolution and evaluation benchmark from NVIDIA.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "AEGIS is a high quality content safety solution and evaluation benchmark from NVIDIA. AEGIS is backed by a broad content safety risk taxonomy that covers 12 critical risks in human-LLM interactions."
          }
        ],
        "reasoning": "Detailed use of a standardized safety benchmark for model evaluation"
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Garak can scan a model or dialog system and quickly discover where it is working\nwell, and where it may be vulnerable to attack.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Garak can scan a model or dialog system and quickly discover where it is working well, and where it may be vulnerable to attack. Garak provides full reporting detailing what worked and what could use improvement."
          }
        ],
        "reasoning": "Demonstrates monitoring and reporting capabilities during model testing"
      }
    ],
    "deletions": []
  },
  "o3-pro": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2 Model Data and Training\n\nOpenAI reasoning models are trained to reason through reinforcement learning.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "OpenAI reasoning models are trained to reason through reinforcement learning. Models in the o-series family are trained to think before they answer: they can produce a long internal chain of thought before responding to the user."
          }
        ],
        "reasoning": "Direct description of reinforcement learning training approach with chain of thought reasoning"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We consider the following evaluations:\n\n\u2022 Standard Refusal Evaluation: Our standard evaluation set for disallowed content and\n\noverrefusals, which our recent models perform very well on.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated that the models perform well on standard refusal evaluations, measuring that the model did not produce output that violates OpenAI policy."
          }
        ],
        "reasoning": "Explicit testing of model's ability to refuse unsafe requests across multiple categories"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4\n\n\fTable 5: Multimodal refusal evaluations\n\nEvaluation\n\nCategory\n\nMetric\n\nsexual/exploitative\n\nnot_unsafe\n\no3\n\n1\n\no4-mini\n\no1\n\n1\n\n0.97\n\nVision sexual\nrefusal evaluation\n\nVision self-harm\nrefusal evaluation\n\nself-harm/intent\n\nnot_unsafe\n\n0.99\n\n0.99\n\n0.97\n\nself-harm/instructions\n\nnot_unsafe\n\n1\n\n0.99\n\n0.95\n\n3.4.1 Vision Vulnerabilities\n\nOpenAI provided external red teamers access to OpenAI o3 and o4-mini to assess vulnerabilities\nrelated to vision capabilities.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "OpenAI provided external red teamers access to OpenAI o3 and o4-mini to assess vulnerabilities related to vision capabilities."
          }
        ],
        "reasoning": "Direct statement of using external experts to systematically test model safety"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4 Preparedness\n\nWe evaluated OpenAI o3 and o4-mini according to our Preparedness Framework. This is the\nfirst launch and system card to be released under our updated Preparedness Framework.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated OpenAI o3 and o4-mini according to our Preparedness Framework. This is the first launch and system card to be released under our updated Preparedness Framework."
          }
        ],
        "reasoning": "Comprehensive safety evaluation framework with standardized benchmarking across multiple risk categories"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To mitigate this issue, we taught the model to adhere to an Instruction Hierarchy [5]. At a\nhigh level, we now have three classifications of messages sent to the model: system messages,\ndeveloper messages, and user messages.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We taught the model to adhere to an Instruction Hierarchy. At a high level, we now have three classifications of messages sent to the model: system messages, developer messages, and user messages."
          }
        ],
        "reasoning": "Explicit mechanism to prevent prompt injection by establishing a hierarchical instruction priority"
      },
      {
        "techniqueId": "tech-contextual-safety",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Ahead of the o3 and o4-mini releases, we\u2019ve deployed new monitoring approaches for biological\nand chemical risk.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Ahead of the o3 and o4-mini releases, we've deployed new monitoring approaches for biological and chemical risk. These use a safety-focused reasoning monitor similar to that used in GPT-4o Image Generation and can block model responses."
          }
        ],
        "reasoning": "Context-aware safety monitoring that can dynamically block responses based on risk assessment"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Table 7: BBQ evaluation\n\nMetric\n\nAccuracy on Ambiguous Questions\n\nAccuracy on Unambiguous Questions\n\nP(not stereotyping | ambiguous\nquestion, not unknown)\n\no3\n\no4-mini\n\no1\n\n0.94\n\n0.93\n\n0.25\n\n0.82\n\n0.95\n\n0.26\n\n0.96\n\n0.93\n\n0.05\n\nWe also tested OpenAI o3 and o4-mini on our first-person fairness evaluation [4].",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We tested OpenAI o3 and o4-mini on the BBQ evaluation. We also tested OpenAI o3 and o4-mini on our first-person fairness evaluation."
          }
        ],
        "reasoning": "Explicit testing for bias, suggesting mitigation techniques were applied during training"
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We simulated our blocking logic and found 4 misses, resulting in a recall of\n98.7% on this challenging set.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We simulated our blocking logic and found 4 misses, resulting in a recall of 98.7% on this challenging set. We rely on additional human monitoring to address such adaptive attacks."
          }
        ],
        "reasoning": "Detailed monitoring and logging of safety system performance with human oversight"
      }
    ],
    "deletions": []
  },
  "openai-preparedness": {
    "additions": [
      {
        "techniqueId": "tech-safety-advisory",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We  are  creating  a  Safety  Advisory  Group\n\n(SAG)  that  brings  together  expertise  from  across  the  company  to  help  OpenAI\u2019s\n\nleadership and Board of Directors be best prepared for the safety decisions they need to\n\nmake.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We are creating a Safety Advisory Group (SAG) that brings together expertise from across the company to help OpenAI's leadership and Board of Directors be best prepared for the safety decisions they need to make."
          }
        ],
        "reasoning": "Detailed description of creating a cross-functional safety advisory body with specific responsibilities"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In addition, Preparedness will also\n\nmanage safety drills and coordinate with the Trustworthy AI team for third-party auditing.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The Preparedness team will also manage safety drills and coordinate with the Trustworthy AI team for third-party auditing."
          }
        ],
        "reasoning": "Explicit commitment to conducting red team exercises and third-party audits"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Pre-mitigation versus post-mitigation risk\n\nWe will run the same evaluations to determine risk level for both the pre-mitigation and the\n\npost-mitigation  risk,  but  on  different  versions  of  the  model  (pre-mitigation  vs  post-\n\nmitigations, as clarified further below).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will run the same evaluations to determine risk level for both the pre-mitigation and the post-mitigation risk, but on different versions of the model (pre-mitigation vs post-mitigations, as clarified further below)."
          }
        ],
        "reasoning": "Detailed description of standardized safety evaluation methodology across model versions"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "For\n\nthis reason, we will be investing in efforts that help create an internal \u201cpreparedness roadmap\u201d\n\nand  help  us  thus  properly  plan  for  and  get  ahead  of  the  emerging  risks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will be investing in efforts that help create an internal 'preparedness roadmap' and help us thus properly plan for and get ahead of the emerging risks. These efforts will include sustained research related to scaling trends for dangerous capabilities and ongoing monitoring of misuse."
          }
        ],
        "reasoning": "Explicit commitment to monitoring and tracking emerging model capabilities"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The BoD may review certain decisions taken and will receive\n\nappropriate documentation (i.e., without needing to proactively ask) to ensure the BOD\n\nis fully informed and able to fulfill its oversight role\n\n Process:\n\n The Preparedness team is responsible for:\n\n maintaining  and  updating  the  Scorecard,\n\nincluding  designing  and  running\n\nevaluations  to  provide  Scorecard  inputs  and  collecting  relevant  information  on\n\nmonitored misuse, red-teaming, and intelligenc\n\n monitoring  for  unknown  unknowns  and  making  the  case  for  inclusion  in  the\n\nPreparedness Framework of any new risk categories as they emerg\n\n ensuring  the  risk  level  distinctions  in  the  Tracked  Risk  Categories  section  are\n\nappropriate given developments in frontier AI models, and suggesting updates to\n\nthese levels if neede\n\n forecasting  potential  changes  to  catastrophic  risk\n\nlevels,  and  summarizing\n\nevidence for an \u201cearly warning\u201d / \u201cheads up\u201d as neede\n\n providing a monthly report (sent to the SAG, Leadership and BoD) synthesizing the\n\nabove  with  any  potential  protective  actions  (the  SAG  Chair,  OpenAI  Leadership,\n\nand/or BoD can adjust this cadence as needed\n\n If  the  Preparedness  or  any  other  team  determines  that  any  changes  to  the\n\nPreparedness  Framework  are  necessary,  it  will  include  a  case  for  this  change  in  its\n\nreport.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The Preparedness team is responsible for providing a monthly report (sent to the SAG, Leadership and BoD) synthesizing the above with any potential protective actions"
          }
        ],
        "reasoning": "Structured process for reporting and tracking potential safety incidents"
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We will also, in cooperation with other teams (e.g., Safety Systems), develop monitoring and\n\ninvestigative  systems.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will also, in cooperation with other teams (e.g., Safety Systems), develop monitoring and investigative systems. This monitoring of real-world misuse (as well as staying abreast of relevant research developments) will help us create a better picture of deployed model characteristics, and inform updates to our evaluations as necessary."
          }
        ],
        "reasoning": "Detailed description of real-time monitoring and investigative systems"
      },
      {
        "techniqueId": "tech-responsible-release",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Preparedness  Framework  (Beta)\n\n20\n\n\fRestricting deployment\n\nOnly  models  with  a  post-mitigation  score  of  \"medium\"  or  below  can  be  deployed.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Only models with a post-mitigation score of 'medium' or below can be deployed. In other words, if we reach (or are forecasted to reach) at least 'high' pre-mitigation risk in any of the considered categories, we will not continue with deployment of that model"
          }
        ],
        "reasoning": "Explicit staged release protocol with safety thresholds"
      }
    ],
    "deletions": []
  },
  "phi-4-tech-report": {
    "additions": [
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.2 Direct Preference Optimization\n\nWe use DPO [RSM+23] to align the model with human preferences, and also to steer the model away\nfrom unwanted behavior through pairs of desired and undesired outputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use DPO [RSM+23] to align the model with human preferences, and also to steer the model away from unwanted behavior through pairs of desired and undesired outputs."
          }
        ],
        "reasoning": "Explicit description of Direct Preference Optimization implementation with clear goals of alignment and behavior steering"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Without\nany mitigation, phi-4 would almost never admit to ignorance. For example, in response to too-difficult\nquestions like \u201cWho is the 297th highest ranked tennis player?\u201d the model would essentially act as an\nimprov-style \u201cYes, and.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Without any mitigation, phi-4 would almost never admit to ignorance. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it... For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong"
          }
        ],
        "reasoning": "Detailed description of training the model to refuse answering questions it cannot confidently answer"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter\norganic2 data sources, including web content, licensed books, and code repositories to extract seeds\nfor the synthetic data pipeline that encourage high-depth reasoning and prioritize educational\nvalue (to the model).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We meticulously curate and filter organic data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value"
          }
        ],
        "reasoning": "Explicit description of careful filtering and curation of training data sources"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "AIRT conducted a two-week red-teaming exercise that tested phi-4 for risky behaviors by emulating both\naverage and adversarial users in single and multi-turn scenarios.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "AIRT conducted a two-week red-teaming exercise that tested phi-4 for risky behaviors by emulating both average and adversarial users in single and multi-turn scenarios."
          }
        ],
        "reasoning": "Direct description of red team testing by an independent group to identify safety vulnerabilities"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To address these issues, we maintain an internal benchmark called PhiBench, which is tailored to\nevaluate the diverse skills and reasoning abilities that we found critical to phi-4\u2019s development.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We maintain an internal benchmark called PhiBench, which is tailored to evaluate the diverse skills and reasoning abilities that we found critical to phi-4's development."
          }
        ],
        "reasoning": "Detailed description of a custom safety benchmark designed to evaluate model capabilities"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 Multilingual Data: We incorporated multilingual datasets to ensure that our model could han-\ndle a wide range of languages, including German, Spanish, French, Portuguese, Italian, Hindi\nand Japanese.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We incorporated multilingual datasets to ensure that our model could handle a wide range of languages... using classifiers trained on multilingual LLM-generated annotations."
          }
        ],
        "reasoning": "Suggests efforts to mitigate potential language and cultural biases through diverse data inclusion"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We used it to guide decisions about dataset\nmixtures and hyperparameter choices for more effective post-training techniques.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We used PhiBench to guide decisions about dataset mixtures and hyperparameter choices... to identify weaknesses in the model and provide feedback for new incoming data sources."
          }
        ],
        "reasoning": "Indicates ongoing monitoring of model capabilities during development"
      }
    ],
    "deletions": []
  },
  "pixtral-12b-blog": {
    "additions": [
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models."
          }
        ],
        "reasoning": "The document describes explicit multimodal safety alignment techniques for processing images and text together"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Architecture\nVariable image size:\nPixtral is designed to optimize for both speed and performance. We trained a new vision encoder that natively supports variable image sizes:\nWe simply pass images through the vision encoder at their native resolution and aspect ratio, converting them into image tokens for each 16x16 patch in the image\nThese tokens are then flattened to create a sequence, with\n[IMG BREAK]\nand\n[IMG END]\ntokens added between rows and at the end of the image.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Pixtral is designed to optimize for both speed and performance. We trained a new vision encoder that natively supports variable image sizes."
          }
        ],
        "reasoning": "The description suggests careful data processing and filtering during training of the vision encoder"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation protocol\nWe re-evaluate a range of open and closed models through the\nsame\nevaluation harness\n.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We re-evaluate a range of open and closed models through the same evaluation harness. For each dataset, the prompt was chosen such that we could reproduce the results of leading multimodal models (GPT-4o and Claude-3.5-Sonnet)."
          }
        ],
        "reasoning": "The document explicitly describes a standardized benchmarking approach for safety and performance evaluation"
      }
    ],
    "deletions": []
  },
  "qwen2-5-coder-tech-report": {
    "additions": [
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Direct Preference Optimization for Code After obtaining the SFT model, we further align\nthe Qwen2.5-Coder with the help of offline direct preference optimization (DPO) (Rafailov\net al., 2023).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "After obtaining the SFT model, we further align the Qwen2.5-Coder with the help of offline direct preference optimization (DPO). Given that human feedback is highly labor-intensive, we use a multilingual code sandbox to provide code execution feedback, while an LLM is utilized for human judgment feedback."
          }
        ],
        "reasoning": "The document provides a detailed description of implementing Direct Preference Optimization (DPO) with specific methodology for feedback collection."
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We have implemented sophisticated procedures to recall and\nclean potential code data and filter out low-quality content using weak model based classi-\nfiers and scorers.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have implemented sophisticated procedures to recall and clean potential code data and filter out low-quality content using weak model based classifiers and scorers."
          }
        ],
        "reasoning": "The authors explicitly describe a data quality filtering process using model-based classifiers during data preparation."
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4\n\n\fTechnical Report\n\n3.1.1 Data Composition\n\nSource Code We collected public repositories from GitHub created before February 2024,\nspanning 92 programming languages.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We collected public repositories from GitHub created before February 2024, spanning 92 programming languages. Similar to StarCoder2 and DS-Coder, we applied a series of rule-based filtering methods."
          }
        ],
        "reasoning": "The document shows evidence of systematic dataset auditing and representation analysis across multiple programming languages."
      },
      {
        "techniqueId": "tech-code-execution-sandboxing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We create a multilingual sandbox to support the code static checking for the main\nprogramming language.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We created a multilingual sandbox to support the code static checking for the main programming language. Further, the multilingual sandbox is a comprehensive platform designed to validate code snippets across multiple programming languages."
          }
        ],
        "reasoning": "The authors describe a multilingual code execution sandbox for validating and testing code snippets."
      }
    ],
    "deletions": []
  },
  "qwen2-5-tech-report": {
    "additions": [
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We decoupled the text data and vision\ndata storage. We simply store text data on CPFS and use mmap for efficient access.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We decoupled the text data and vision data storage. We simply store text data on CPFS and use mmap for efficient access. For vision data, we use Alibaba Cloud's OSS (Object Storage Service) for persistent storage."
          }
        ],
        "reasoning": "The document describes careful data storage and access strategies that suggest rigorous data filtering and management."
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The cutoff date for our data knowledge is June 2023. This diverse data composition is\ninstrumental in developing a robust multimodal understanding capability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The cutoff date for our data knowledge is June 2023. This diverse data composition is instrumental in developing a robust multimodal understanding capability."
          }
        ],
        "reasoning": "The authors explicitly mention tracking data sources and setting a knowledge cutoff, indicating some form of dataset auditing."
      },
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Unified Image and Video Understanding Qwen2-VL employs a mixed training regimen incorporating both\nimage and video data, ensuring proficiency in image understanding and video comprehension.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We employ a mixed training regimen incorporating both image and video data, ensuring proficiency in image understanding and video comprehension."
          }
        ],
        "reasoning": "The document describes a deliberate approach to training across multiple modalities to ensure comprehensive understanding."
      },
      {
        "techniqueId": "tech-hallucination-grounding",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The cutoff date for our data knowledge is June 2023. This diverse data composition is\ninstrumental in developing a robust multimodal understanding capability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The cutoff date for our data knowledge is June 2023. This diverse data composition is instrumental in developing a robust multimodal understanding capability."
          }
        ],
        "reasoning": "By setting a specific knowledge cutoff and emphasizing data diversity, the authors suggest techniques to reduce potential hallucinations."
      }
    ],
    "deletions": []
  },
  "qwen3-max": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.5.1 Reward Design\n\nWe explore two reward formulations to guide the RL training process:\n\nGuard-Only Reward This reward scheme directly leverages Generative Qwen3Guard\u2019s safety judg-\nments.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We explore two reward formulations to guide the RL training process: Guard-Only Reward and Hybrid Reward. We employ Group Sequence Policy Optimization (GSPO) (Zheng et al., 2025), a stable and efficient reinforcement learning algorithm, to train the policy model."
          }
        ],
        "reasoning": "Detailed description of using reinforcement learning with specific reward design and optimization algorithm"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Refusal Detection In addition to content moderation, Qwen3Guard-Gen is capable of detecting whether\na model\u2019s response constitutes a refusal.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Generative Qwen3Guard is capable of detecting whether a model's response constitutes a refusal. We evaluate this capability using XSTest and WildGuardTest as benchmark datasets."
          }
        ],
        "reasoning": "Explicit implementation of training models to detect and potentially generate refusal responses"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "During a conversation: (1) The user\u2019s\nprompt is simultaneously submitted to both the LLM assistant and Stream Qwen3Guard.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "During a conversation: (1) The user's prompt is simultaneously submitted to both the LLM assistant and Stream Qwen3Guard. Stream Qwen3Guard evaluates the prompt and assigns a safety label; based on this assessment, the upper-level framework determines whether to interrupt the conversation."
          }
        ],
        "reasoning": "Detailed description of a system-level input guardrail mechanism with real-time classification"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We present two\nspecialized variants: Generative Qwen3Guard, which reformulates safety classification as a generative\ntask, and Stream Qwen3Guard, which performs token-level safety detection during incremental text\ngeneration, thereby enabling real-time intervention and dynamic moderation.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Stream Qwen3Guard performs token-level safety detection during incremental text generation, thereby enabling real-time intervention and dynamic moderation."
          }
        ],
        "reasoning": "Explicit implementation of a token-level output filtering system with dynamic intervention"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Auto Labeling To annotate the unlabeled data, we design tailored annotation instructions and leverage\nmultiple versions of Qwen models, such as Qwen2.5-72B-Instruct and Qwen3-235B-A22B, to generate\npreliminary labels.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We design tailored annotation instructions and leverage multiple versions of Qwen models, such as Qwen2.5-72B-Instruct and Qwen3-235B-A22B, to generate preliminary labels."
          }
        ],
        "reasoning": "Uses system-level instructions to guide model behavior and labeling process"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.5.1 Reward Design\n\nWe explore two reward formulations to guide the RL training process:\n\nGuard-Only Reward This reward scheme directly leverages Generative Qwen3Guard\u2019s safety judg-\nments.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We explore two reward formulations to guide the RL training process: Guard-Only Reward and Hybrid Reward. The Hybrid Reward jointly optimizes for three objectives: high safety, high helpfulness, and low refusal rate."
          }
        ],
        "reasoning": "Explicit design of reward models specifically optimized for safety objectives"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In the current version of Qwen3Guard, we consider the following safety categories:\n\n\u2022 Violent: Content that provides detailed instructions, methods, or advice on how to commit acts\nof violence, including the manufacture, acquisition, or use of weapons.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "In the current version of Qwen3Guard, we consider the following safety categories: Violent: Content that provides detailed instructions, methods, or advice on how to commit acts of violence, including the manufacture, acquisition, or use of weapons."
          }
        ],
        "reasoning": "Detailed specification of violence detection as a core safety classification category"
      },
      {
        "techniqueId": "tech-self-harm-prevention",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 Suicide & Self-Harm: Content advocating, directly encouraging, or detailing methods for\n\nself-harm, suicide, or dangerous activities that could lead to serious injury or death.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Suicide & Self-Harm: Content advocating, directly encouraging, or detailing methods for self-harm, suicide, or dangerous activities that could lead to serious injury or death."
          }
        ],
        "reasoning": "Explicit inclusion of self-harm prevention as a safety detection category"
      },
      {
        "techniqueId": "tech-illegal-activity-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 Non-violent Illegal Acts: Content providing guidance or advice for non-violent illegal activities\n\nlike hacking, unauthorized drug production, or stealing.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Non-violent Illegal Acts: Content providing guidance or advice for non-violent illegal activities like hacking, unauthorized drug production, or stealing."
          }
        ],
        "reasoning": "Clear specification of detecting illegal activity as a safety classification category"
      },
      {
        "techniqueId": "tech-contextual-safety",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Departing from conventional binary safe/unsafe\nclassification, Qwen3Guard introduces a controversial category, enabling more flexible moderation\ndecisions where safety judgments may vary across regions, platforms, or use cases.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We introduce a controversial category, enabling more flexible moderation decisions where safety judgments may vary across regions, platforms, or use cases."
          }
        ],
        "reasoning": "Explicit implementation of a context-aware safety assessment mechanism"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We comprehensively evaluate Qwen3Guard across a diverse suite of benchmarks, including English,\nChinese, and multilingual datasets.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We comprehensively evaluate Qwen3Guard across a diverse suite of benchmarks, including English, Chinese, and multilingual datasets."
          }
        ],
        "reasoning": "Detailed description of using standardized safety benchmarks for evaluation"
      }
    ],
    "deletions": [
      {
        "techniqueId": "tech-dpo",
        "deleted_by": "llm",
        "reasoning": "No implementation evidence, only mentioned in technique catalog"
      }
    ]
  },
  "xai-security": {
    "additions": [
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Logging and monitoring software are configured to collect data from system components to monitor system events (including security events / IDS events), performance, and resource utilization.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Logging and monitoring software are configured to collect data from system components to monitor system events (including security events / IDS events), performance, and resource utilization."
          }
        ],
        "reasoning": "Direct quote describes comprehensive logging and monitoring infrastructure for system events and performance tracking."
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "A formal incident management framework has been established that defines roles, responsibilities, escalation paths, and internal and external communication requirements in the event of incidents that impact the security or availability of the system.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "A formal incident management framework has been established that defines roles, responsibilities, escalation paths, and internal and external communication requirements in the event of incidents that impact the security or availability of the system."
          }
        ],
        "reasoning": "Explicit description of a structured incident reporting and management process."
      },
      {
        "techniqueId": "tech-access-control-documentation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Role-Based Access Control\nAccess Management roles are available in the xAI Enterprise Platform. Additionally, predefined security groups are used to assign role-based access privileges and segregate access to systems and data in the production environment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Access Management roles are available in the xAI Enterprise Platform. Additionally, predefined security groups are used to assign role-based access privileges and segregate access to systems and data in the production environment."
          }
        ],
        "reasoning": "Clear documentation of access control mechanisms and role-based access management."
      },
      {
        "techniqueId": "tech-enterprise-integration",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI supports Single Sign On for Business Tier accounts. You can use any SAML-based Identity Provider (IdP), for example Okta, X, OneLogin, or use GSuite to serve as your identity provider, delegating access to the application based on rules you create in your central identity management solution.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI supports Single Sign On for Business Tier accounts. You can use any SAML-based Identity Provider (IdP), for example Okta, X, OneLogin, or use GSuite to serve as your identity provider, delegating access to the application based on rules you create in your central identity management solution."
          }
        ],
        "reasoning": "Detailed description of enterprise integration features including SSO and identity management."
      },
      {
        "techniqueId": "tech-data-retention-policies",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI retains security logs, which include application, tooling, and access logs, for 180 days. Data access logs are maintained for 365 days.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI retains security logs, which include application, tooling, and access logs, for 180 days. Data access logs are maintained for 365 days."
          }
        ],
        "reasoning": "Explicit statement of data retention timeframes for different log types."
      },
      {
        "techniqueId": "tech-regulatory-compliance",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI includes a suite of Privacy tools to help your organization comply with regulations like HIPAA, the GDPR, and the CCPA.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI includes a suite of Privacy tools to help your organization comply with regulations like HIPAA, the GDPR, and the CCPA."
          }
        ],
        "reasoning": "Direct statement of compliance with multiple international privacy regulations."
      },
      {
        "techniqueId": "tech-responsible-release",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Core production infrastructure is deployed across multiple active availability zones at the cloud service provider, and the core systems are duplicated in an idle state in a separate availability region for quick deployment during an availability event.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Core production infrastructure is deployed across multiple active availability zones at the cloud service provider, and the core systems are duplicated in an idle state in a separate availability region for quick deployment during an availability event."
          }
        ],
        "reasoning": "Description suggests a staged, careful approach to infrastructure deployment with redundancy and failover planning."
      }
    ],
    "deletions": []
  }
}