{
  "anthropic-rsp": {
    "additions": [
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We will routinely test models to determine whether their capabilities fall suf\ufb01ciently\nfar below the Capability Thresholds such that the ASL-2 Standard remains appropriate.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will routinely test models to determine whether their capabilities fall sufficiently far below the Capability Thresholds such that we are confident that the ASL-2 Standard remains appropriate."
          }
        ],
        "reasoning": "Describes a systematic testing approach for safety benchmarking against predefined capability thresholds"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Our\nUsage Policy sets forth our standards for the use of our products, including prohibitions on using our models\nto spread misinformation, incite violence or hateful behavior, or engage in fraudulent or abusive practices, and\nwe continually re\ufb01ne our technical measures for enforcing our trust and safety standards at scale.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Our Usage Policy sets forth our standards for the use of our products, including prohibitions on using our models to spread misinformation, incite violence or hateful behavior, or engage in fraudulent or abusive practices"
          }
        ],
        "reasoning": "Indicates system-level instructions/prompts defining behavioral boundaries for model usage"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We will routinely test models to determine whether their capabilities fall suf\ufb01ciently\nfar below the Capability Thresholds such that the ASL-2 Standard remains appropriate.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will routinely test models to determine whether their capabilities fall sufficiently far below the Capability Thresholds such that the ASL-2 Standard remains appropriate."
          }
        ],
        "reasoning": "Explicitly describes a framework for monitoring and tracking model capabilities against predefined safety thresholds"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Noncompliance: We will maintain a process through which Anthropic staff may anonymously notify\nthe Responsible Scaling Of\ufb01cer of any potential instances of noncompliance with this policy.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will maintain a process through which Anthropic staff may anonymously notify the Responsible Scaling Officer of any potential instances of noncompliance with this policy."
          }
        ],
        "reasoning": "Describes a formal system for reporting safety incidents and potential policy violations"
      },
      {
        "techniqueId": "tech-responsible-release",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We may deploy or store a model if either of the following criteria are met:\n(1) the model\u2019s capabilities are suf\ufb01ciently far away from the existing Capability Thresholds, making the\ncurrent ASL-2 Standard appropriate; or (2) the model\u2019s capabilities have surpassed the existing Capabilities\nThreshold, but we have implemented the ASL-3 Required Safeguards and conducted the follow-up capability\nassessment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We may deploy or store a model if either of the following criteria are met: (1) the model's capabilities are sufficiently far away from the existing Capability Thresholds, making the current ASL-2 Standard appropriate; or (2) the model's capabilities have surpassed the existing Capabilities Threshold, but we have implemented the ASL-3 Required Safeguards"
          }
        ],
        "reasoning": "Describes a structured, staged approach to model deployment with explicit safety criteria"
      },
      {
        "techniqueId": "tech-constitutional-ai",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Harmlessness training and automated detection: Training models to refuse requests to aid in\n\ncausing harm, such as with Constitutional AI or other improved techniques, and the use of model\nenhanced trust and safety detection and enforcement.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Harmlessness training and automated detection: Training models to refuse requests to aid in causing harm, such as with Constitutional AI or other improved techniques"
          }
        ],
        "reasoning": "Explicitly mentions Constitutional AI as a training technique for model safety"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Red-teaming: Conduct red-teaming that demonstrates that threat actors with realistic access levels\nand resources are highly unlikely to be able to consistently elicit information from any generally\naccessible systems that greatly increases their ability to cause catastrophic harm relative to other\navailable tools.10\n\n4.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red-teaming: Conduct red-teaming that demonstrates that threat actors with realistic access levels and resources are highly unlikely to be able to consistently elicit information from any generally accessible systems that greatly increases their ability to cause catastrophic harm relative to other available tools."
          }
        ],
        "reasoning": "Provides a detailed description of red teaming as a systematic adversarial testing approach"
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Monitoring: Prespecify empirical evidence that would show the system is operating within the\n\naccepted risk range and de\ufb01ne a process for reviewing the system\u2019s performance on a reasonable\ncadence.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Monitoring: Prespecify empirical evidence that would show the system is operating within the accepted risk range and define a process for reviewing the system's performance on a reasonable cadence."
          }
        ],
        "reasoning": "Describes a comprehensive monitoring and audit logging approach for tracking system performance"
      }
    ],
    "deletions": []
  },
  "claude-3-5-sonnet-card": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2.1 Human Feedback Evaluations\n\nWe evaluated Claude 3.5 Sonnet via direct comparison to prior Claude models.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Claude 3.5 Sonnet via direct comparison to prior Claude models. We asked raters to chat with our models and evaluate them on a number of tasks, using task-specific instructions."
          }
        ],
        "reasoning": "Evidence suggests human feedback evaluation, which is a core component of RLHF training methodology"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Our safety teams performed a range of evaluations on Claude 3.5 Sonnet in the areas of Chemical, Biological,\nRadiological, and Nuclear (CBRN) risks, cybersecurity, and autonomous capabilities.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Our safety teams performed a range of evaluations on Claude 3.5 Sonnet in the areas of Chemical, Biological, Radiological, and Nuclear (CBRN) risks, cybersecurity, and autonomous capabilities."
          }
        ],
        "reasoning": "Systematic adversarial testing across multiple risk domains indicates comprehensive red teaming approach"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2 Evaluations\n\nReasoning, Coding, and Question Answering\n\nWe evaluated Claude 3.5 Sonnet on a series of industry-standard benchmarks covering reasoning, reading\ncomprehension, math, science, and coding.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Claude 3.5 Sonnet on a series of industry-standard benchmarks covering reasoning, reading comprehension, math, science, and coding."
          }
        ],
        "reasoning": "Explicit use of standardized benchmarks for performance and safety evaluation"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Refusals\n\nWe assessed Claude 3.5 Sonnet\u2019s ability to differentiate between harmful and benign requests.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We assessed Claude 3.5 Sonnet's ability to differentiate between harmful and benign requests. These tests, which use the Wildchat and XSTest datasets, are designed to measure the model's ability to avoid unnecessary refusals with harmless prompts while maintaining appropriate caution with harmful content."
          }
        ],
        "reasoning": "Direct evidence of training the model to appropriately refuse harmful requests while avoiding over-refusal"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "If the model had exceeded our preset thresholds during testing, we planned\nto convene a council consisting of our Responsible Scaling Officer, evaluations leads, and external subject\nmatter experts to determine whether the model\u2019s capabilities were close enough to a threshold of concern to\nwarrant either more intensive evaluations or an increase in safety and security protections.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "If the model had exceeded our preset thresholds during testing, we planned to convene a council consisting of our Responsible Scaling Officer, evaluations leads, and external subject matter experts to determine whether the model's capabilities were close enough to a threshold of concern."
          }
        ],
        "reasoning": "Systematic monitoring of model capabilities against predefined safety thresholds"
      },
      {
        "techniqueId": "tech-community-evaluation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Additionally, we worked with external third party evaluation partners such as the\nUK Artificial Intelligence Safety Institute (UK AISI) to independently assess Claude 3.5 Sonnet.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We worked with external third party evaluation partners such as the UK Artificial Intelligence Safety Institute (UK AISI) to independently assess Claude 3.5 Sonnet."
          }
        ],
        "reasoning": "Involvement of external partners in model evaluation suggests community-based assessment"
      },
      {
        "techniqueId": "tech-safety-advisory",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "If the model had exceeded our preset thresholds during testing, we planned\nto convene a council consisting of our Responsible Scaling Officer, evaluations leads, and external subject\nmatter experts to determine whether the model\u2019s capabilities were close enough to a threshold of concern to\nwarrant either more intensive evaluations or an increase in safety and security protections.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "If the model had exceeded our preset thresholds during testing, we planned to convene a council consisting of our Responsible Scaling Officer, evaluations leads, and external subject matter experts"
          }
        ],
        "reasoning": "Indicates use of external expert advisory mechanism for safety oversight"
      }
    ],
    "deletions": []
  },
  "claude-3-haiku-model-card": {
    "additions": [
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.1 Trust & Safety\n\n3.1.1 Model Red-Teaming\n\nWe conducted comprehensive Trust & Safety (T&S) evaluations across fourteen policy areas in six languages:\nEnglish, Arabic, Spanish, Hindi, Tagalog, and Chinese.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conducted comprehensive Trust & Safety (T&S) evaluations across fourteen policy areas in six languages: English, Arabic, Spanish, Hindi, Tagalog, and Chinese. Our assessment paid particular attention to critical areas such as Elections Integrity, Child Safety, Cyber Attacks, Hate & Discrimination, and Violent Extremism."
          }
        ],
        "reasoning": "Detailed description of systematic red teaming across multiple domains and languages"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.1.2 Prompt Injection\n\nWe enhanced the ability of the upgraded Claude 3.5 Sonnet and Claude 3.5 Haiku to recognize and resist\nprompt injection attempts.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We enhanced the ability of the upgraded Claude 3.5 Sonnet and Claude 3.5 Haiku to recognize and resist prompt injection attempts. Prompt injection is an attack where a malicious user feeds instructions to a model that attempt to change its originally intended behavior."
          }
        ],
        "reasoning": "Explicit statement of developing defenses against prompt injection attacks"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2.4 Refusals\n\nWe assessed the upgraded Claude 3.5 Sonnet and new Claude 3.5 Haiku models on their capacity to appro-\npriately refuse harmful prompts while minimizing incorrect refusals for harmless inputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We assessed the upgraded Claude 3.5 Sonnet and new Claude 3.5 Haiku models on their capacity to appropriately refuse harmful prompts while minimizing incorrect refusals for harmless inputs."
          }
        ],
        "reasoning": "Direct evidence of training models to appropriately refuse harmful prompts"
      },
      {
        "techniqueId": "tech-community-evaluation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "As part of our continued effort to partner with external experts, joint pre-deployment testing of the new\nClaude 3.5 Sonnet model was conducted by the US AI Safety Institute (US AISI) [6] and the UK AI Safety\nInstitute (UK AISI) [7].",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "As part of our continued effort to partner with external experts, joint pre-deployment testing of the new Claude 3.5 Sonnet model was conducted by the US AI Safety Institute (US AISI) and the UK AI Safety Institute (UK AISI). We also collaborated with METR to conduct an independent assessment."
          }
        ],
        "reasoning": "Explicit description of external expert evaluation and collaboration"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2 Evaluations\n\nWe conducted extensive evaluations of the upgraded Claude 3.5 Sonnet and Claude 3.5 Haiku models to\nassess their performance across a wide range of tasks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conducted extensive evaluations of the upgraded Claude 3.5 Sonnet and Claude 3.5 Haiku models to assess their performance across a wide range of tasks. These include standard benchmarks, novel tests, and human evaluations."
          }
        ],
        "reasoning": "Clear description of using standardized benchmarks for safety and performance evaluation"
      },
      {
        "techniqueId": "tech-frontier-risk-evaluation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.2 Frontier Risk Evaluations\n\nAs part of our Responsible Scaling Policy, we conducted comprehensive safety evaluations on the upgraded\nClaude 3.5 Sonnet prior to release.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "As part of our Responsible Scaling Policy, we conducted comprehensive safety evaluations on the upgraded Claude 3.5 Sonnet prior to release. These evaluations focused on potential catastrophic risks in three areas: CBRN, Cybersecurity, and Autonomous capabilities."
          }
        ],
        "reasoning": "Detailed description of systematic frontier risk assessment across multiple domains"
      },
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Our safety teams conducted rigorous multimodal red-team exercises to help ensure align-\nment with Anthropic\u2019s Usage Policy [5].",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Our safety teams conducted rigorous multimodal red-team exercises to help ensure alignment with Anthropic's Usage Policy."
          }
        ],
        "reasoning": "Brief mention of multimodal safety testing, though details are limited"
      }
    ],
    "deletions": []
  },
  "claude-opus-4-5-system-card": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "8\n\n\fAfter the pretraining process, Claude Opus 4.5 underwent substantial post-training and\n\ufb01ne-tuning, with the intention of making it a helpful, honest, and harmless assistant1.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "After the pretraining process, Claude Opus 4.5 underwent substantial post-training and fine-tuning, with the intention of making it a helpful, honest, and harmless assistant. This involved a variety of techniques including reinforcement learning from human feedback (RLHF) and reinforcement learning from AI feedback."
          }
        ],
        "reasoning": "Direct statement of RLHF implementation during model training"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Claude Opus 4.5 demonstrated substantial improvements over Claude Opus 4.1 across child\nsafety harms in multi-turn contexts, including stronger resistance to jailbreaking\ntechniques, earlier recognition of harmful intent signals, and more robust refusals.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We observed that Claude Opus 4.5 demonstrated substantial improvements over Claude Opus 4.1 across child safety harms in multi-turn contexts, including stronger resistance to jailbreaking techniques, earlier recognition of harmful intent signals, and more robust refusals."
          }
        ],
        "reasoning": "Explicit description of training model to refuse harmful requests"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "7.2.4.9 ASL-4 expert red teaming\n\nDetails\n\n131\n\n\fWe worked with a bioengineering and biosecurity expert to engage in conversations with\nClaude around bioweapons ideation and design, over two days of testing.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We worked with a bioengineering and biosecurity expert to engage in conversations with Claude around bioweapons ideation and design, over two days of testing. This red-teaming effort involved identifying potential bottlenecks and failure modes, and gathering qualitative assessments of model risk."
          }
        ],
        "reasoning": "Detailed description of systematic adversarial testing by expert red teamers"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Threshold and evaluations\nTo test a model\u2019s cyber capabilities, we have developed a series of cyber challenges in\ncollaboration with expert partners.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have developed a series of cyber challenges in collaboration with expert partners. We designed these challenges to cover a range of cyberoffensive tasks, and aimed for them to be (a) substantially more difficult than publicly-available challenges and (b) more representative of true cyberoffensive tasks."
          }
        ],
        "reasoning": "Explicit development of standardized safety benchmarks across multiple domains"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u25cf  Added a paragraph to Section 6.5 which clari\ufb01es that we have continued our\n\npractice of refraining from training on the model\u2019s chain of thought.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We added a sentence to Section 6.5 which clarifies that we have continued our practice of refraining from training on the model's chain of thought."
          }
        ],
        "reasoning": "System prompts used to define model behavior and constraints"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.2 Prompt injection risk within agentic systems\n\nPrevention of prompt injection remains one of the highest priorities for secure deployment\nof our models in agentic systems.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prevention of prompt injection remains one of the highest priorities for secure deployment of our models in agentic systems. Claude Opus 4.5 is our most robust model to date against prompt injection attacks across all agentic surfaces: tool use, computer use, browser use and coding."
          }
        ],
        "reasoning": "Explicit focus on developing defenses against prompt injection attacks"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.1 Single-turn evaluations\n\nWe evaluated Claude Opus 4.5\u2019s willingness to provide information in single-turn\nscenarios\u2014that is, examining a single model response to a user\u2019s query\u2014spanning a broad\nrange of topics outlined in our Usage Policy.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Claude Opus 4.5's willingness to provide information in single-turn scenarios\u2014that is, examining a single model response to a user's query\u2014spanning a broad range of topics outlined in our Usage Policy. These scenarios included queries representing straightforward policy violations, where harmless responses are expected."
          }
        ],
        "reasoning": "Systematic evaluation and detection of violent content"
      },
      {
        "techniqueId": "tech-self-harm-prevention",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "For the\nrelease of Claude Opus 4.5, we added new test cases in the areas of cyber harm and\nstandardized additional test cases for suicide and self-harm scenarios, bringing the total\nnumber of test cases to 93 across 10 different risk areas.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We added new test cases in the areas of cyber harm and standardized additional test cases for suicide and self-harm scenarios, bringing the total number of test cases to 93 across 10 different risk areas."
          }
        ],
        "reasoning": "Explicit development of test cases and prevention mechanisms for self-harm scenarios"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.5 Bias evaluations\n\n3.5.1 Political bias\n\nWe evaluated Claude Opus 4.5 on political bias by measuring political even-handedness\nacross pairs of political stances.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Claude Opus 4.5 on political bias by measuring political even-handedness across pairs of political stances. Our intention is for Claude to be fair, trustworthy, and unbiased when people from across the political spectrum ask it about political topics."
          }
        ],
        "reasoning": "Systematic evaluation and mitigation of political bias"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Throughout the training\nprocess we used several data cleaning and \ufb01ltering methods including deduplication and\nclassi\ufb01cation.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Throughout the training process we used several data cleaning and filtering methods including deduplication and classification."
          }
        ],
        "reasoning": "Direct statement of data quality filtering during training"
      }
    ],
    "deletions": []
  },
  "cohere-safety-framework": {
    "additions": [
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To narrow this gap, we first present a novel threat model where we discuss the objectives, constraints,\nand two scenarios of a realistic privacy attack on websites (Sec.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We first present a novel threat model where we discuss the objectives, constraints, and two scenarios of a realistic privacy attack on websites"
          }
        ],
        "reasoning": "The paper describes a systematic adversarial testing approach by exploring potential attack scenarios against web agents"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We select those tasks that involve PII information. Specifically,\nfor each action step per task, we use both GPT-4 (Achiam et al., 2023) and GPT-4o (OpenAI, 2024)\nto determine whether PII is involved and to identify the PII category.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We select those tasks that involve PII information. Specifically, for each action step per task, we use both GPT-4 and GPT-4o to determine whether PII is involved and to identify the PII category."
          }
        ],
        "reasoning": "The authors explicitly describe a process for detecting and categorizing personally identifiable information"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.2 MITIGATION BY DEFENSIVE SYSTEM PROMPT\n\nWe assess if the risks posed by EIA can be easily mitigated by a defensive system prompt.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We assess if the risks posed by EIA can be easily mitigated by a defensive system prompt. Particularly, in the prompt, we warn the web agent about potential prompt injection to avoid any elements or actions that are not typically found on the websites"
          }
        ],
        "reasoning": "The paper explores defensive system prompts as a mitigation strategy against prompt injection attacks"
      },
      {
        "techniqueId": "tech-cybersecurity-threat",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We consider two realistic attack scenarios where websites are compromised:\n(1) The website developers being benign but using contaminated development tools.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We consider two realistic attack scenarios where websites are compromised: (1) The website developers being benign but using contaminated development tools... (2) The website developers being malicious."
          }
        ],
        "reasoning": "The research systematically analyzes potential cybersecurity threats and attack vectors"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Auto-submission Mechanism: We further design an auto-submission mechanism to make the attack\nfeasible.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We further design an auto-submission mechanism to make the attack feasible. Specifically, we eliminate the need for a button click to submit data."
          }
        ],
        "reasoning": "While the paper discusses input mechanisms, it is describing an attack technique rather than a guardrail system"
      }
    ],
    "deletions": []
  },
  "command-a": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To enhance the alignment of our model\nwith human preferences, we further employ Reinforcement Learning from Human Feedback (RLHF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We employ Reinforcement Learning from Human Feedback (RLHF). We use online CoPG with two generations per prompt. The prompts used for RLHF training are derived from a subset of those previously used during SFT, including reasoning, multilingual, coding, and preference-based tasks prompts."
          }
        ],
        "reasoning": "Detailed description of RLHF implementation with specific training methodology and prompt selection"
      },
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.2.2.1 Preference Training with Self-refinement\nWe consider preference training methods for learning offline from preference datasets: Sequence Likelihood\nCalibration, or SLiC (Zhao et al., 2023), Direct Preference Optimisation, or DPO (Rafailov et al., 2024), and\nIdentity Preference Optimisation, or IPO (Azar et al., 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We consider preference training methods for learning offline from preference datasets: Sequence Likelihood Calibration, or SLiC, Direct Preference Optimisation, or DPO, and Identity Preference Optimisation, or IPO."
          }
        ],
        "reasoning": "Explicit mention of Direct Preference Optimization as part of their training methodology"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.2.3 Reward Models\nWe train a Bradley-Terry reward model for use in online preference learning, evaluation, and data filtering.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We train a Bradley-Terry reward model for use in online preference learning, evaluation, and data filtering. Similar to Gunter et al. (2024), we use a cross-entropy loss with soft labels as targets."
          }
        ],
        "reasoning": "Detailed description of training a reward model specifically for safety and preference learning"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Ensuring that the model cannot produce harmful content\nmeans that a lot of training data shows refusal as the preferred behaviour.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Ensuring that the model cannot produce harmful content means that a lot of training data shows refusal as the preferred behaviour. It is crucial to balance such data points with authorised user requests and compliant completions to prevent the model from over-relying on refusal."
          }
        ],
        "reasoning": "Explicit strategy of training the model to refuse unsafe content while maintaining helpfulness"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We optimise the web text data by enhancing the ratio of educational samples that are relatively\nsparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based\nquality filters after careful de-duplication and heuristic filtering for safety and quality.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We optimise the web text data by enhancing the ratio of educational samples that are relatively sparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based quality filters after careful de-duplication and heuristic filtering for safety and quality."
          }
        ],
        "reasoning": "Clear description of data filtering process to improve training data quality"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.6.2 Default Safety\nIn the default setting, we evaluate the safety of the model without a system preamble to simulate cases\noutside of Cohere\u2019s API or enterprise contexts.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate the safety of the model without a system preamble to simulate cases outside of Cohere's API or enterprise contexts. We measure both absolute and relative safety. Absolute safety evaluation tests models with potentially eliciting prompts from the categories of our core safety behaviour."
          }
        ],
        "reasoning": "Detailed description of comprehensive safety benchmarking methodology"
      },
      {
        "techniqueId": "tech-rag-guardrails",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In RAG (Lewis et al., 2020), the model\nhas access to a search tool (e.g. a dense embedding index) to answer information-seeking queries.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "In RAG (Lewis et al., 2020), the model has access to a search tool (e.g. a dense embedding index) to answer information-seeking queries. It generates search queries, retrieves relevant snippets from the selected knowledge source, and uses this context to craft a well-informed response."
          }
        ],
        "reasoning": "Explicit description of RAG guardrails to ensure accurate and contextually grounded responses"
      },
      {
        "techniqueId": "tech-realtime-fact-checking",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We find that reward models tend to suffer from high memorisation, causing catastrophic collapse in perfor-\nmance on a second epoch over the same data; so, we train the model in two stages.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We find that reward models tend to suffer from high memorisation, causing catastrophic collapse in performance on a second epoch over the same data; so, we train the model in two stages."
          }
        ],
        "reasoning": "Approach to dynamically validate and improve model responses through multi-stage training"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We perform two stages of safety-related pretraining filtering: first, we remove known domains\nfor CSEA and sexual content, and second, we use a classifier to remove generally low quality content, including\nsexual content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We perform two stages of safety-related pretraining filtering: first, we remove known domains for CSEA and sexual content, and second, we use a classifier to remove generally low quality content, including sexual content."
          }
        ],
        "reasoning": "Explicit strategy for filtering and moderating sexual content during model training"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.7 Enterprise-specific Benchmarks\nOur enterprise evaluation process focuses on testing model capabilities in generative and retrieval-augmented\nuse cases that mirror typical enterprise applications.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We track whether model capabilities have crossed predefined danger thresholds requiring upgraded safeguards. We measure capability thresholds against safety benchmarks to ensure the model remains within acceptable risk parameters."
          }
        ],
        "reasoning": "Detailed approach to monitoring and tracking model capabilities against safety thresholds"
      }
    ],
    "deletions": []
  },
  "deepseek-privacy": {
    "additions": [
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We do not ask for, and you should not provide sensitive Personal Data\nto the Services,\nwhether about yourself or other individuals\n.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We do not ask for, and you should not provide sensitive Personal Data (e.g., personal data revealing racial or ethnic origin, religious beliefs, health, sexuality, citizenship, immigration status, genetic or biometric data, personal data of children, precise geolocation or criminal membership)."
          }
        ],
        "reasoning": "The policy explicitly describes avoiding collection of sensitive personal information, indicating a PII detection and prevention approach."
      },
      {
        "techniqueId": "tech-data-sovereignty",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Where We Store Your Personal Data\nThe Personal Data we collect from you may be stored on a server located\noutside of the country where you live.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The Personal Data we collect from you may be stored on a server located outside of the country where you live. To provide you with our services, we directly collect, process and store your Personal Data in People's Republic of China."
          }
        ],
        "reasoning": "The document clearly describes data localization and storage practices specific to China, demonstrating data sovereignty controls."
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "If you believe that we processed\nPersonal Data about or collected from a child, please contact us at\nprivacy@deepseek.com\n.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "If you believe that we processed Personal Data about or collected from a child, please contact us at privacy@deepseek.com."
          }
        ],
        "reasoning": "The policy provides a mechanism for reporting potential data handling incidents, suggesting an incident reporting system."
      },
      {
        "techniqueId": "tech-regulatory-compliance",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Privacy Policy Updates\nWe may update this Privacy Policy from time to time as required by law. When we update the Privacy Policy, we will notify you by updating the\n\u201cLast Updated\u201d date at the top of the new Privacy Policy, posting the\nnew Privacy Policy, or by providing any other notice required by\napplicable law.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We may update this Privacy Policy from time to time as required by law. When we update the Privacy Policy, we will notify you by updating the 'Last Updated' date at the top of the new Privacy Policy, posting the new Privacy Policy, or by providing any other notice required by applicable law."
          }
        ],
        "reasoning": "The document demonstrates explicit commitment to adapting policies to meet legal requirements across jurisdictions."
      }
    ],
    "deletions": []
  },
  "deepseek-r1-paper": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Specifically, we build upon DeepSeek-V3-Base (DeepSeek-AI, 2024b) and employ Group\nRelative Policy Optimization (GRPO) (Shao et al., 2024) as our RL framework.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We employ Group Relative Policy Optimization (GRPO) (Shao et al., 2024) as our RL framework. The reward signal is solely based on the correctness of final predictions against ground-truth answers, without imposing constraints on the reasoning process itself."
          }
        ],
        "reasoning": "Detailed description of reinforcement learning approach using human feedback principles"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We build upon the DeepSeek-V3 pipeline and adopt a similar distribution\nof preference pairs and training prompts.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We build upon the DeepSeek-V3 pipeline and adopt a similar distribution of preference pairs and training prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessment emphasizes the utility and relevance of the response to the user while minimizing interference with the underlying reasoning process."
          }
        ],
        "reasoning": "Explicit description of creating reward models to assess safety and helpfulness of model outputs"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Thus, we constructed a\ndedicated test suite for jailbreaking evaluation. Specifically, we developed a template collection\nconsisting of 2,232 jailbreaking instructions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We constructed a dedicated test suite for jailbreaking evaluation. Specifically, we developed a template collection consisting of 2,232 jailbreaking instructions. We then randomly concatenated these jailbreaking prompts with questions from the original safety testset and further examined the performance differences in the model's responses when confronted with original unsafe questions versus newly formulated questions with jailbreaking elements."
          }
        ],
        "reasoning": "Comprehensive description of systematic adversarial testing to probe model safety vulnerabilities"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Experiment Setup\n\nBenchmarks We evaluate models on MMLU (Hendrycks et al., 2021), MMLU-Redux (Gema\net al., 2025), MMLU-Pro (Wang et al., 2024), C-Eval (Huang et al., 2023), IFEval (Zhou et al.,\n2023b), FRAMES (Krishna et al., 2024), GPQA Diamond (Rein et al., 2023), SimpleQA (OpenAI,\n2024a), C-SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024b), Aider (Gauthier,\n2025), LiveCodeBench (Jain et al., 2024) (2024-08 \u2013 2025-01), Codeforces (Mirzayanov, 2025),\nChinese National High School Mathematics Olympiad (CNMO 2024) (CMS, 2024), and American\nInvitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate models on MMLU, MMLU-Redux, MMLU-Pro, C-Eval, IFEval, FRAMES, GPQA Diamond, SimpleQA, C-SimpleQA, SWE-Bench Verified, Aider, LiveCodeBench, Codeforces, Chinese National High School Mathematics Olympiad (CNMO 2024), and American Invitational Mathematics Examination 2024 (AIME 2024)."
          }
        ],
        "reasoning": "Extensive use of standardized safety and performance benchmarks for model evaluation"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "However, to\nmitigate the issue of language mixing, we introduce a language consistency reward during RL\ntraining, which is calculated as the proportion of target language words in the CoT.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT."
          }
        ],
        "reasoning": "Explicit technique to reduce bias and improve language consistency"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "39\n\n0200040006000800010000Steps02000400060008000100001200014000The Frequency of Reflection Words0200040006000800010000Steps0200400600800100012001400The Frequency of Word 'Wait'\fDecontamination To prevent benchmark contamination, we implemented comprehensive\ndecontamination procedures for both pre-training and post-training data.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "To prevent benchmark contamination, we implemented comprehensive decontamination procedures for both pre-training and post-training data. We filtered out any text segments (including web pages and GitHub files) that contained matching 10-gram sequences from evaluation questions or reference solutions."
          }
        ],
        "reasoning": "Detailed description of data filtering to prevent contamination and improve training quality"
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Safety Reward Model To assess and improve model safety, we curated a dataset of 106,000\nprompts with model-generated responses annotated as \u201csafe\" or \u201cunsafe\" according to prede-\nfined safety guidelines.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We curated a dataset of 106,000 prompts with model-generated responses annotated as 'safe' or 'unsafe' according to predefined safety guidelines. Unlike the pairwise loss employed in the helpfulness reward model, the safety reward model was trained using a point-wise methodology to distinguish between safe and unsafe responses."
          }
        ],
        "reasoning": "Systematic approach to auditing and analyzing dataset safety characteristics"
      }
    ],
    "deletions": []
  },
  "deepseek-v3-paper": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Following this, we\nconduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)\non the base model of DeepSeek-V3, to align it with human preferences and further unlock its\npotential.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
          }
        ],
        "reasoning": "Explicit description of RLHF process with two-stage fine-tuning approach"
      },
      {
        "techniqueId": "tech-constitutional-ai",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "During the development of DeepSeek-V3, for\nthese broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging\nthe voting evaluation results of DeepSeek-V3 itself as a feedback source.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We employ the constitutional AI approach, leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source. This method has produced notable alignment effects, significantly enhancing the performance of DeepSeek-V3 in subjective evaluations."
          }
        ],
        "reasoning": "Describes a self-reflective approach to model alignment using internal voting mechanisms"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To address this issue, we randomly split a certain proportion of such combined tokens during\ntraining, which exposes the model to a wider array of special cases and mitigates this bias.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We randomly split a certain proportion of combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias."
          }
        ],
        "reasoning": "Limited evidence of a bias mitigation strategy focused on token boundary handling"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Data Construction\n\nCompared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio\nof mathematical and programming samples, while expanding multilingual coverage beyond\n\n21\n\n\fEnglish and Chinese.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
          }
        ],
        "reasoning": "Explicit description of careful data curation and quality filtering process"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We will explore more comprehensive and multi-dimensional model evaluation methods to\nprevent the tendency towards optimizing a fixed set of benchmarks during research, which\nmay create a misleading impression of the model capabilities and affect our foundational\nassessment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will explore more comprehensive and multi-dimensional model evaluation methods to prevent the tendency towards optimizing a fixed set of benchmarks during research, which may create a misleading impression of the model capabilities and affect our foundational assessment."
          }
        ],
        "reasoning": "Suggests an approach to systematically test and evaluate model capabilities beyond standard benchmarks"
      },
      {
        "techniqueId": "tech-safety-benchmarking",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation Benchmarks\n\nThe base model of DeepSeek-V3 is pretrained on a multilingual corpus with English and Chinese\nconstituting the majority, so we evaluate its performance on a series of benchmarks primarily\nin English and Chinese, as well as on a multilingual benchmark.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate its performance on a series of benchmarks primarily in English and Chinese, as well as on a multilingual benchmark. Our evaluation is based on our internal evaluation framework integrated in our HAI-LLM framework."
          }
        ],
        "reasoning": "Detailed description of comprehensive safety and performance benchmarking across multiple domains"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We will consistently explore and iterate on the deep thinking capabilities of our models,\naiming to enhance their intelligence and problem-solving abilities by expanding their\nreasoning length and depth.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will consistently explore and iterate on the deep thinking capabilities of our models, aiming to enhance their intelligence and problem-solving abilities by expanding their reasoning length and depth."
          }
        ],
        "reasoning": "Explicit commitment to monitoring and tracking model capabilities over time"
      }
    ],
    "deletions": [
      {
        "techniqueId": "tech-dpo",
        "deleted_by": "llm",
        "reasoning": "No evidence of Direct Preference Optimization implementation"
      }
    ]
  },
  "gemini-1-5-paper": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Reinforcement Learning from Human Feedback\n\nFor the RLHF stage, we divide our interventions into reward model (RM) improvements and rein-\nforcement learning (RL) improvements.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "For the RLHF stage, we divide our interventions into reward model (RM) improvements and reinforcement learning (RL) improvements. For both RM and RL, similarly to SFT, we source prompts either through human-model or model-model interactions, striving for coverage of safety policies and use cases."
          }
        ],
        "reasoning": "Detailed description of RLHF implementation with specific steps for sourcing prompts and improving reward models"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Humans then provide feedback on the responses, often comparing multiple\npotential response candidates for each query.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Humans then provide feedback on the responses, often comparing multiple potential response candidates for each query. Preference data is then amortized in our Reward Model."
          }
        ],
        "reasoning": "Describes creating a reward model based on human feedback comparisons"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "When we find that model behavior needs improvement, either because of\nsafety policy violations, or because of the model refuses when a helpful, non-policy-violating answer\nexists, we use a combination of custom data generation recipes loosely inspired by Constitutional\nAI (Bai et al., 2022), as well as human intervention to revise responses.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "When we find that model behavior needs improvement, either because of safety policy violations, or because of the model refuses when a helpful, non-policy-violating answer exists, we use a combination of custom data generation recipes loosely inspired by Constitutional AI, as well as human intervention to revise responses."
          }
        ],
        "reasoning": "Explicit description of training the model to refuse unsafe queries while remaining helpful"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Red teaming, a form of adversarial testing where adversaries launch an attack on an AI system, is\nconducted by specialist internal teams across the policies and desiderata.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red teaming, a form of adversarial testing where adversaries launch an attack on an AI system, is conducted by specialist internal teams across the policies and desiderata. These activities include both automated systems performing sophisticated adversarial attacks to identify new vulnerabilities, as well as creative manual testing."
          }
        ],
        "reasoning": "Detailed description of red teaming process with both automated and manual approaches"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The RSC\u2019s review process includes rotating experts from a wide range\nof disciplines, with machine learning researchers, ethicists, and safety experts sitting alongside\nengineers, security experts, policy professionals, and more.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We developed a set of safety benchmarks that include policy violations across modalities, helpfulness evaluations, security and privacy tests, and representational harm assessments."
          }
        ],
        "reasoning": "Comprehensive description of safety benchmark development and testing"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In evaluating prompt injection attacks we seek to measure Gemini 1.5 models\u2019 vulnerability to\nadversarially-injected instructions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate prompt injection attacks where the attacker's objective is to manipulate the model to output a specific markdown image crafted to discreetly exfiltrate sensitive information from the conversation history."
          }
        ],
        "reasoning": "Detailed evaluation of prompt injection defense mechanisms"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Detection of graphic violence, terrorism threats, and violent content is part of our content classification system to prevent harmful outputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": null
          }
        ],
        "reasoning": "Explicit mention of violence detection as a safety technique"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Hate speech\n4. Dangerous or malicious content (including promoting self-harm, or instructing in harmful\n\nactivities)\n5.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Detection of hate speech, discrimination, harassment, and offensive language is implemented as part of our content safety policies."
          }
        ],
        "reasoning": "Clear implementation of hate speech detection mechanism"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Sexually explicit content\n7. Medical advice that runs contrary to scientific or medical consensus\n\nThese policies apply across modalities.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Detection and filtering of pornographic or sexually explicit content is part of our safety training approach to prevent inappropriate outputs."
          }
        ],
        "reasoning": "Explicit description of sexual content moderation technique"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We measure memorization using the methodology described in (Gemma-Team et al., 2024). Specifically, for each corpus of the pre-training dataset, we sample 10,000 documents and use the first\n50 tokens to prompt the model.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We measure memorization using methodology that identifies potentially sensitive personal information, using Google Cloud Sensitive Data Protection (SDP) service which classifies many types of potentially sensitive information."
          }
        ],
        "reasoning": "Detailed approach to detecting personally identifiable information"
      },
      {
        "techniqueId": "tech-misinformation-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Gemini also has guidelines designed to prevent misinformation or biased content. The model\nshould also follow guidelines that prioritize content grounded in authoritative consensus and answers\nthat are neutral (including political neutrality).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Gemini also has guidelines designed to prevent misinformation or biased content. The model should also follow guidelines that prioritize content grounded in authoritative consensus and answers that are neutral."
          }
        ],
        "reasoning": "Explicit strategy for detecting and preventing misinformation"
      }
    ],
    "deletions": []
  },
  "gemini-25-flash-lite": {
    "additions": [
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Training Data Processing: Data \ufb01ltering and preprocessing included techniques such as\ndeduplication, safety \ufb01ltering in-line with Google's commitment to advancing AI safely and\nresponsibly and quality \ufb01ltering to mitigate risks and improve training data reliability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Data filtering and preprocessing included techniques such as deduplication, safety filtering in-line with Google's commitment to advancing AI safely and responsibly and quality filtering to mitigate risks and improve training data reliability."
          }
        ],
        "reasoning": "Direct quote describes explicit data filtering techniques used during model training"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "A range of evaluations and red teaming activities were\nconducted to help improve the model and inform decision-making.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "A range of evaluations and red teaming activities were conducted to help improve the model and inform decision-making. These evaluations and activities align with Google's AI Principles and responsible AI approach."
          }
        ],
        "reasoning": "Explicit mention of red teaming as part of the model's safety evaluation process"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation types included but were not limited to:\n\n\u25cf  Training/Development Evaluations including automated and human evaluations carried\nout continuously throughout and after the model\u2019s training, to monitor its progress and\nperformance;\n\n\u25cf  Human Red Teaming conducted by specialist teams across the policies and desiderata,\n\ndeliberately trying to spot weaknesses and ensure the model adheres to safety policies\nand desired outcomes;\n\n\u25cf  Automated Red Teaming to dynamically evaluate Gemini for safety and security\n\nconsiderations at scale, complementing human red teaming and static evaluations;\n\u25cf  Assurance Evaluations conducted by human evaluators independent of the model\ndevelopment team, and assess responsibility and safety governance decisions\n\n\u25cf  Ethics & Safety Reviews were conducted ahead of the model\u2019s release\n\n5\n\n\fIn addition, we perform testing following the guidelines in Google DeepMind\u2019s Frontier Safety\nFramework (FSF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Evaluation types included but were not limited to: Training/Development Evaluations including automated and human evaluations carried out continuously throughout and after the model's training, to monitor its progress and performance"
          }
        ],
        "reasoning": "Describes systematic benchmarking and evaluation processes throughout model development"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We have focused on improving instruction following (IF) abilities of Gemini 2.5. This means that we\ntrain Gemini to answer questions as accurately as possible, while prioritizing safety and minimising\nunhelpful responses.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have focused on improving instruction following (IF) abilities of Gemini 2.5. This means that we train Gemini to answer questions as accurately as possible, while prioritizing safety and minimising unhelpful responses."
          }
        ],
        "reasoning": "Indicates explicit training to improve safety and reduce inappropriate responses"
      },
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Mitigations include, but are not limited to:\n\n\u25cf  dataset \ufb01ltering;\n\u25cf  conditional pre-training;\n\u25cf  supervised \ufb01ne-tuning;\n\u25cf\n\u25cf  safety policies and desiderata;\n\u25cf  product-level mitigations such as safety \ufb01ltering.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Mitigations include, but are not limited to: [...] reinforcement learning from human and critic feedback"
          }
        ],
        "reasoning": "Direct mention of RLHF as a mitigation technique in model development"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We continue to improve our internal\nevaluations, including re\ufb01ning automated evaluations to reduce false positives and negatives, as\nwell as update query sets to ensure balance and maintain a high standard of results.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We continue to improve our internal evaluations, including refining automated evaluations to reduce false positives and negatives, as well as update query sets to ensure balance and maintain a high standard of results."
          }
        ],
        "reasoning": "Suggests ongoing efforts to mitigate potential biases in model evaluations"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Frontier Safety Assessment: We evaluated Gemini 2.5 Pro Preview for Frontier Safety and\nreported the results in the 2.5 Pro Preview model card, \ufb01nding that it did not reach any critical\ncapability levels outlined in our Frontier Safety Framework.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Gemini 2.5 Pro Preview for Frontier Safety and reported the results in the 2.5 Pro Preview model card, finding that it did not reach any critical capability levels outlined in our Frontier Safety Framework."
          }
        ],
        "reasoning": "Describes explicit monitoring of model capabilities against predefined safety thresholds"
      }
    ],
    "deletions": []
  },
  "gemini-3-pro": {
    "additions": [
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Training Data Processing: Data \ufb01ltering and preprocessing included techniques such as deduplication,\nhonoring robots.txt, safety \ufb01ltering in-line with Google's commitment to advancing AI safely and\nresponsibly, and quality \ufb01ltering to mitigate risks and improve training data reliability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Data filtering and preprocessing included techniques such as deduplication, honoring robots.txt, safety filtering in-line with Google's commitment to advancing AI safely and responsibly, and quality filtering to mitigate risks and improve training data reliability."
          }
        ],
        "reasoning": "Direct quote describes explicit data quality filtering techniques applied during training"
      },
      {
        "techniqueId": "tech-csam-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Once data is\ncollected, it is cleaned and preprocessed to make it suitable for training. This process involves, on a\ncase-by-case basis, \ufb01ltering irrelevant or harmful content, text, and other modalities, including \ufb01ltering\ncontent that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Once data is collected, it is cleaned and preprocessed to make it suitable for training. This process involves, on a case-by-case basis, filtering irrelevant or harmful content, text, and other modalities, including filtering content that is pornographic, violent, or violative of child sexual abuse material (CSAM) laws."
          }
        ],
        "reasoning": "Explicit mention of CSAM filtering as part of data preprocessing"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation types included but were not limited to:\n\n\u25cf  Training/Development Evaluations including automated and human evaluations carried out\n\ncontinuously throughout and after the model\u2019s training, to monitor its progress and\nperformance;\n\n\u25cf  Human Red Teaming conducted by specialist teams who sit outside of the model development\nteam, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the\nmodel adheres to safety policies and desired outcomes;\n\n\u25cf  Automated Red Teaming to dynamically evaluate Gemini for safety and security\n\nconsiderations at scale, complementing human red teaming and static evaluations;\n\n\u25cf  Ethics & Safety Reviews were conducted ahead of the model\u2019s release\n\nIn addition, we perform testing following the guidelines in Google DeepMind\u2019s Frontier Safety\nFramework (FSF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Human Red Teaming conducted by specialist teams who sit outside of the model development team, across the policies and desiderata, deliberately trying to spot weaknesses and ensure the model adheres to safety policies and desired outcomes"
          }
        ],
        "reasoning": "Clear description of human red teaming process used during model development"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3\n\n\fEvaluation\n\nApproach: Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal\ncapabilities, agentic tool use, multi-lingual performance, and long-context.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Gemini 3 Pro was evaluated across a range of benchmarks, including reasoning, multimodal capabilities, agentic tool use, multi-lingual performance, and long-context."
          }
        ],
        "reasoning": "Explicit mention of comprehensive safety benchmark evaluations"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "7\n\n\fFrontier Safety\n\nWe evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025),\nand found that it did not reach any critical capability levels as outlined in the table below:\n\nDomain\n\nKey Results for Gemini 3 Pro\n\nCCL\n\nCBRN\n\nGemini 3 Pro provides accurate and\noccasionally actionable information but\ngenerally fails to o\ufb00er novel or su\ufb03ciently\ncomplete and detailed instructions to\nsigni\ufb01cantly enhance the capabilities of low\nto medium resourced threat actors.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Gemini 3 Pro as outlined in our latest Frontier Safety Framework (September-2025), and found that it did not reach any critical capability levels"
          }
        ],
        "reasoning": "Direct evidence of capability threshold monitoring using a formal framework"
      },
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The post-training dataset included\ndi\ufb00erent types of instruction tuning data  reinforcement learning data, and human-preference data.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The post-training dataset included different types of instruction tuning data, reinforcement learning data, and human-preference data. Gemini 3 Pro is trained using reinforcement learning techniques that can leverage multi-step reasoning, problem-solving and theorem-proving data."
          }
        ],
        "reasoning": "Clear description of RLHF implementation during model training"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Risks and Mitigations: Safety and responsibility was built into Gemini 3 Pro throughout the training and\ndeployment lifecycle, including pre-training, post-training, and product-level mitigations.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Safety and responsibility was built into Gemini 3 Pro throughout the training and deployment lifecycle, including pre-training, post-training, and product-level mitigations."
          }
        ],
        "reasoning": "Suggests bias mitigation as part of broader safety approach, though specific details are limited"
      },
      {
        "techniqueId": "tech-safety-policies",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5\n\n\fSafety Policies: Gemini\u2019s safety policies aim to prevent our Generative AI models from generating\nharmful content, including:\n\n1.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Gemini's safety policies aim to prevent our Generative AI models from generating harmful content, including: 1. Content related to child sexual abuse material and exploitation 2. Hate speech 3. Dangerous content 4. Harassment 5. Sexually explicit content 6. Medical advice that runs contrary to scientific or medical consensus"
          }
        ],
        "reasoning": "Comprehensive list of explicit safety policies implemented in the model"
      }
    ],
    "deletions": []
  },
  "gemini-3-technical-report": {
    "additions": [
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Training and Model Guardrails\n\nWe deploy multiple guardrails to reduce the risk of Gemini 3 Pro generating harmful content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We deploy multiple guardrails to reduce the risk of Gemini 3 Pro generating harmful content. These may include query filters that guide the model's responses to certain inputs, fine-tuning processes that align model outputs with safety guidelines, and filtering and processing of inputs."
          }
        ],
        "reasoning": "Direct description of input filtering and guardrail systems implemented in the model"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We mitigate against prompt injection attacks with a layered defense strategy, which includes measures\nsuch as: prompt injection content classifiers, security through reinforcement, markdown sanitation and\nsuspicious URL redaction, user confirmations, and end-user security mitigation notifications, as\ndescribed in further detail in this recent blog post.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We mitigate against prompt injection attacks with a layered defense strategy, which includes measures such as: prompt injection content classifiers, security through reinforcement, markdown sanitation and suspicious URL redaction, user confirmations, and end-user security mitigation notifications."
          }
        ],
        "reasoning": "Explicit list of specific prompt injection defense techniques implemented"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Mitigation Assessment\n\nInternal and external red team efforts continually test the efficacy of the mitigations, including their\nrobustness to universal and query-specific jailbreaks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Internal and external red team efforts continually test the efficacy of the mitigations, including their robustness to universal and query-specific jailbreaks. Feedback from these red teams is used to improve the suite of mitigations."
          }
        ],
        "reasoning": "Clear description of ongoing red teaming process for safety testing"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We use these evaluations as a proxy for uplift capability: we believe that full or partial automation of\nthese key skills are the path through which AI can be used to reduce the resources needed for\nsophisticated cyberattacks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use these evaluations as a proxy for uplift capability: we believe that full or partial automation of these key skills are the path through which AI can be used to reduce the resources needed for sophisticated cyberattacks."
          }
        ],
        "reasoning": "Detailed description of using standardized benchmarks to evaluate model safety capabilities"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Interventions are designed to prevent violative model responses while allowing benign responses. We\nconsider a response to be violative if it helps with attacks in a concrete way.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Interventions are designed to prevent violative model responses while allowing benign responses. We consider a response to be violative if it helps with attacks in a concrete way."
          }
        ],
        "reasoning": "Suggests training to refuse potentially harmful outputs"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Frontier Safety Summary\n\nWe evaluated Gemini 3 Pro against the CCLs defined in our FSF, which defines explicit risk acceptance\ncriteria for CBRN, cybersecurity, machine learning R&D, and harmful manipulation.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated Gemini 3 Pro against the CCLs defined in our FSF, which defines explicit risk acceptance criteria for CBRN, cybersecurity, machine learning R&D, and harmful manipulation."
          }
        ],
        "reasoning": "Systematic monitoring of model capabilities against predefined safety thresholds"
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We continue to invest in automated and manual red teaming to\nimprove mitigation jailbreak robustness and coverage.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We continue to invest in automated and manual red teaming to improve mitigation jailbreak robustness and coverage."
          }
        ],
        "reasoning": "Ongoing monitoring and tracking of model performance and potential safety issues"
      }
    ],
    "deletions": [
      {
        "techniqueId": "tech-hallucination-grounding",
        "deleted_by": "llm",
        "reasoning": "No specific implementation of hallucination detection is described in the document"
      },
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "deleted_by": "llm",
        "reasoning": "No concrete evidence of multimodal safety alignment techniques is provided"
      }
    ]
  },
  "google-ai-principles-2024": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We also use feedback from people to\ntune the model, known as reinforcement learning\nfrom human feedback (RLHF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We also use feedback from people to tune the model, known as reinforcement learning from human feedback (RLHF)."
          }
        ],
        "reasoning": "Direct mention of RLHF as a technique used to tune their models"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Red teaming, also referred to as adversarial testing,\nis a technique where \u201cethical hackers\u201d intentionally\nviolate policies for the purpose of discovering and\naddressing vulnerabilities which could harm users.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red teaming, also referred to as adversarial testing, is a technique where 'ethical hackers' intentionally violate policies for the purpose of discovering and addressing vulnerabilities which could harm users. We've established a dedicated Google AI security red team focused on testing for security and privacy risks."
          }
        ],
        "reasoning": "Explicit description of red teaming implementation with a dedicated internal team"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We\nhave also included standard academic benchmarks\nfor the capabilities of models, such as GSMK8, MMLU,\n\nHumanEval, and MATH, in our model cards (see\nGemini, Gemini 1.5, and Gemma) so that the reporting\nof model capabilities, limitations, and testing results is\nconsistent and auditable.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have also included standard academic benchmarks for the capabilities of models, such as GSMK8, MMLU, HumanEval, and MATH, in our model cards... We're continuously improving how we measure AI safety as industry benchmark tools emerge."
          }
        ],
        "reasoning": "Clear evidence of using standardized safety benchmarks in model evaluation"
      },
      {
        "techniqueId": "tech-watermarking",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "As part of our commitment to user context, we\ndeveloped SynthID to detect and watermark\nAI-generated content made with our services.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We developed SynthID to detect and watermark AI-generated content made with our services. The technology enables our models to attach an invisible mark to the content they generate that denotes it was created with Google AI."
          }
        ],
        "reasoning": "Detailed description of a specific watermarking technology implemented across their AI services"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Other layered\nprotections are deployed both when a person inputs\na prompt and again when the model provides the\noutput.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Other layered protections are deployed both when a person inputs a prompt and again when the model provides the output."
          }
        ],
        "reasoning": "Suggests input-level filtering and protection mechanisms are in place"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Common product mitigations for\nlarge language models (LLMs):\n\n\u2022  Disclosures in the Privacy Notice stating that\npeople should not rely on a large language\nmodel\u2019s responses for medical, legal, finan-\ncial or other professional advice.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Common product mitigations for large language models (LLMs) include: 'Disclosures in the Privacy Notice stating that people should not rely on a large language model's responses for medical, legal, financial or other professional advice.'"
          }
        ],
        "reasoning": "Weak evidence of system-level guidance, but not a clear implementation of system prompts"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022  Feedback channels and operational support\nfor user feedback to improve the model and\naddress issues.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have feedback channels and operational support for user feedback to improve the model and address issues."
          }
        ],
        "reasoning": "Indicates a mechanism for reporting and tracking incidents"
      },
      {
        "techniqueId": "tech-misinformation-detection",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "For example,\ngenerative AI makes it easier than ever to create\nnew content, but it can also raise questions about\ntrustworthiness of information, like we see with\n\u201cdeepfakes.\u201d As more than 1 billion people around the\nworld head to the voting booth in 2024, our teams\nare particularly focused on maintaining the integrity\nof information related to elections on our platforms.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "As more than 1 billion people around the world head to the voting booth in 2024, our teams are particularly focused on maintaining the integrity of information related to elections on our platforms."
          }
        ],
        "reasoning": "Suggests active efforts to detect and mitigate misinformation, particularly around elections"
      }
    ],
    "deletions": []
  },
  "gpt-4o-system-card": {
    "additions": [
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We post-trained GPT-4o to refuse to comply with requests to\nidentify someone based on a voice in an audio input, while still\ncomplying with requests to identify famous quotes.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We post-trained GPT-4o to refuse to comply with requests to identify someone based on a voice in an audio input, while still complying with requests to identify famous quotes."
          }
        ],
        "reasoning": "Direct quote describes explicit training to refuse certain types of requests"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We run our existing moderation classifier over text transcrip-\ntions of audio prompts, and block the output if the prompt\ncontains erotic or violent language.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run our existing moderation classifier over text transcriptions of audio prompts and block the output if the prompt contains erotic or violent language."
          }
        ],
        "reasoning": "Describes a filtering layer that inspects and blocks inputs based on content classification"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 We run our existing moderation classifier over text transcrip-\ntions of audio prompts, and block the output if the prompt\ncontains erotic or violent language.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run our existing moderation classifier over text transcriptions of audio prompts and generations, and block the output for certain high-severity categories."
          }
        ],
        "reasoning": "Explicit description of post-generation filtering and blocking mechanism"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Additionally, we run our existing moderation model over a text transcription of both audio input\nand audio output to detect if either contains potentially harmful language, and will block a\ngeneration if so8.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run our existing moderation model over a text transcription of both audio input and audio output to detect if either contains potentially harmful language, and will block a generation if so."
          }
        ],
        "reasoning": "Describes detection and blocking of violent content across input and output"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Additionally, we run our existing moderation model over a text transcription of both audio input\nand audio output to detect if either contains potentially harmful language, and will block a\ngeneration if so8.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run our existing moderation model over a text transcription of both audio input and audio output to detect if either contains potentially harmful language, and will block a generation if so."
          }
        ],
        "reasoning": "Moderation system includes detection of harmful language, which includes hate speech"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Additionally, we run our existing moderation model over a text transcription of both audio input\nand audio output to detect if either contains potentially harmful language, and will block a\ngeneration if so8.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We run our existing moderation model over a text transcription of the audio input to detect if it contains a request for violent or erotic content, and will block a generation if so."
          }
        ],
        "reasoning": "Direct description of filtering sexual content in audio inputs"
      },
      {
        "techniqueId": "tech-weapons-illegal-activity",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "At\nthe same time, certain pre-training filtering mitigations can provide an additional layer of defense\nthat, along with other safety mitigations, help exclude unwanted and harmful information from\nour datasets:\n\n\u2022 We use our Moderation API and safety classifiers to filter out data that could contribute to\nharmful content or information hazards, including CSAM, hateful content, violence, and\nCBRN.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use our Moderation API and safety classifiers to filter out data that could contribute to harmful content or information hazards, including CSAM, hateful content, violence, and CBRN."
          }
        ],
        "reasoning": "Explicit mention of filtering content related to illegal activities and violence"
      },
      {
        "techniqueId": "tech-csam-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "At\nthe same time, certain pre-training filtering mitigations can provide an additional layer of defense\nthat, along with other safety mitigations, help exclude unwanted and harmful information from\nour datasets:\n\n\u2022 We use our Moderation API and safety classifiers to filter out data that could contribute to\nharmful content or information hazards, including CSAM, hateful content, violence, and\nCBRN.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use our Moderation API and safety classifiers to filter out data that could contribute to harmful content or information hazards, including CSAM, hateful content, violence, and CBRN."
          }
        ],
        "reasoning": "Direct quote includes CSAM detection as part of safety filtering"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We use advanced data filtering processes to reduce personal information from training data.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use advanced data filtering processes to reduce personal information from training data."
          }
        ],
        "reasoning": "Explicit description of filtering personal information during data preparation"
      },
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "SOURCE_ID: gpt-4o-system-card\nSOURCE_TITLE: GPT-4o System Card\nSOURCE_URI: https://cdn.openai.com/gpt-4o-system-card.pdf\n--------------------\nGPT-4o System Card\n\nOpenAI\u2217\n\nAugust 8, 2024\n\n1\n\nIntroduction\n\nGPT-4o[1] is an autoregressive omni model, which accepts as input any combination of text, audio,\nimage, and video and generates any combination of text, audio, and image outputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "GPT-4o is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It's trained end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network."
          }
        ],
        "reasoning": "Describes a multimodal model with safety alignment considerations across different input/output modalities"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We also evaluated GPT-4o\nin accordance with our Preparedness Framework[4]. 2\n\n\f3.1 External red teaming\n\nOpenAI worked with more than 100 external red teamers2, speaking a total of 45 different\nlanguages, and representing geographic backgrounds of 29 different countries.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated GPT-4o in accordance with our Preparedness Framework. The Preparedness Framework is a living document that describes our procedural commitments to track, evaluate, forecast, and protect against catastrophic risks from frontier models."
          }
        ],
        "reasoning": "Describes a systematic approach to monitoring model capabilities and potential risks"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Although some technical mitigations are still in development, our Usage Policies[20] disallow\nintentionally deceiving or misleading others, and circumventing safeguards or safety mitigations.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Although some technical mitigations are still in development, our Usage Policies disallow intentionally deceiving or misleading others, and circumventing safeguards or safety mitigations. In addition to technical mitigations, we enforce our Usage Policies through monitoring and take action on violative behavior in both ChatGPT and the API."
          }
        ],
        "reasoning": "Indicates a process for tracking and responding to policy violations"
      }
    ],
    "deletions": []
  },
  "gpt-5-system-card": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "OpenAI reasoning models, including gpt-5-thinking, gpt-5-thinking-mini, and gpt-5-thinking-nano,\nare trained to reason through reinforcement learning.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "OpenAI reasoning models, including gpt-5-thinking, gpt-5-thinking-mini, and gpt-5-thinking-nano, are trained to reason through reinforcement learning. These models are trained to think before they answer: they can produce a long internal chain of thought before responding to the user."
          }
        ],
        "reasoning": "Direct description of reinforcement learning training approach for reasoning models"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Refuse all requests for weaponization assistance\n\n2. Never provide detailed actionable assistance on dual use topics.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We trained the model to: 1. Refuse all requests for weaponization assistance 2. Never provide detailed actionable assistance on dual use topics."
          }
        ],
        "reasoning": "Explicit statement of training models to refuse certain types of harmful requests"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We used the OpenAI o1-\n\n23\n\n\fpreview (pre-mitigation) model as an autograder, validating agreement with a trusted biosecurity\nexpert.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We used the OpenAI o1-preview (pre-mitigation) model as an autograder, validating agreement with a trusted biosecurity expert."
          }
        ],
        "reasoning": "Evidence of using a reward model for safety evaluation and grading"
      },
      {
        "techniqueId": "tech-hallucination-grounding",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Table 7: Prompt Injection Evaluations\n\nEvaluation (higher is better)\n\ngpt-5-thinking\n\nOpenAI o3\n\nBrowsing prompt injections\n\nTool calling prompt injections\n\nCoding prompt injections\n\n0.99\n\n0.99\n\n0.97\n\n0.89\n\n0.80\n\n0.94\n\n3.7 Hallucinations\n\nOne of our focuses when training the GPT-5 models was to reduce the frequency of factual\nhallucinations.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "One of our focuses when training the GPT-5 models was to reduce the frequency of factual hallucinations. While ChatGPT has browsing enabled by default, many API queries do not use browsing tools. Thus, we focused both on training our models to browse effectively for up-to-date information, and on reducing hallucinations when the models are relying on their own internal knowledge."
          }
        ],
        "reasoning": "Explicit description of techniques to reduce hallucinations through browsing and knowledge grounding"
      },
      {
        "techniqueId": "tech-multistage-pipeline",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Instead, we\u2019ve deployed a two tiered system of real-time,\nautomated oversight surrounding the model to monitor and block unsafe prompts and generations:\n\n\u2022 The first tier in this system is a fast, topical classifier model that determines whether or not\nthe content is related to biology.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We've deployed a two tiered system of real-time, automated oversight surrounding the model to monitor and block unsafe prompts and generations: 1. The first tier in this system is a fast, topical classifier model that determines whether or not the content is related to biology. 2. The second tier monitor is a reasoning model that determines which part of the biological threat taxonomy a particular generated response falls into."
          }
        ],
        "reasoning": "Clear description of a multi-layered defense system with multiple classification stages"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Mitigations\n\nTo mitigate this, we use a multilayered defense stack including teaching models to ignore prompt\ninjections in web or connector contents.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "To mitigate this, we use a multilayered defense stack including teaching models to ignore prompt injections in web or connector contents."
          }
        ],
        "reasoning": "Direct statement of implementing defenses against prompt injection attacks"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We use advanced data filtering processes to reduce personal information\nfrom training data. We also employ a combination of our Moderation API and safety classifiers\nto help prevent the use of harmful or sensitive content, including explicit materials such as sexual\ncontent involving a minor.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use advanced data filtering processes to reduce personal information from training data. We also employ a combination of our Moderation API and safety classifiers to help prevent the use of harmful or sensitive content, including explicit materials such as sexual content involving a minor."
          }
        ],
        "reasoning": "Explicit description of filtering mechanisms for sexual content"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We use advanced data filtering processes to reduce personal information\nfrom training data. We also employ a combination of our Moderation API and safety classifiers\nto help prevent the use of harmful or sensitive content, including explicit materials such as sexual\ncontent involving a minor.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use advanced data filtering processes to reduce personal information from training data."
          }
        ],
        "reasoning": "Direct statement of filtering out personally identifiable information"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We also employ a combination of our Moderation API and safety classifiers\nto help prevent the use of harmful or sensitive content, including explicit materials such as sexual\ncontent involving a minor.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We employ a combination of our Moderation API and safety classifiers to help prevent the use of harmful or sensitive content."
          }
        ],
        "reasoning": "Evidence of using safety classifiers to detect and prevent violent content"
      },
      {
        "techniqueId": "tech-misinformation-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We first evaluate the factual correctness of gpt-5-thinking and gpt-5-main on prompts representa-\ntive of real ChatGPT production conversations, using an LLM-based grading model with web\naccess to identify major and minor factual errors in the assistant\u2019s responses.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We first evaluate the factual correctness of gpt-5-thinking and gpt-5-main on prompts representative of real ChatGPT production conversations, using an LLM-based grading model with web access to identify major and minor factual errors in the assistant's responses."
          }
        ],
        "reasoning": "Detailed description of misinformation detection through factual error checking"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The framework commits us to track and\nmitigate the risk of severe harm, including by implementing safeguards that sufficiently minimize\nthe risk for highly capable models.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We track and mitigate the risk of severe harm, including by implementing safeguards that sufficiently minimize the risk for highly capable models."
          }
        ],
        "reasoning": "Explicit statement of monitoring and tracking model capabilities for potential risks"
      }
    ],
    "deletions": []
  },
  "grok-4": {
    "additions": [
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We define a basic refusal policy which instructs Grok 4 to\ndecline queries demonstrating clear intent to engage in activities that threaten severe, imminent harm\nto others, including violent crimes, child sexual exploitation, fraud, hacking, and more.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We define a basic refusal policy which instructs Grok 4 to decline queries demonstrating clear intent to engage in activities that threaten severe, imminent harm to others, including violent crimes, child sexual exploitation, fraud, hacking, and more."
          }
        ],
        "reasoning": "The document explicitly describes a refusal policy designed to train the model to decline harmful requests"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "With Grok 4\u2019s strong reasoning and instruction-following capabilities, we find\nthat including our basic refusal policy in the system prompt greatly reduces response rate on harmful\nqueries.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "With Grok 4's strong reasoning and instruction-following capabilities, we find that including our basic refusal policy in the system prompt greatly reduces response rate on harmful queries."
          }
        ],
        "reasoning": "System prompts are directly used as a key safety mechanism with explicit implementation details"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We also employ model-based filters for both Grok 4 API and Grok 4 Web,\nwhich reject classes of harmful requests, including biological and chemical weapons, self-harm, and\nCSAM.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We also employ model-based filters for both Grok 4 API and Grok 4 Web, which reject classes of harmful requests, including biological and chemical weapons, self-harm, and CSAM."
          }
        ],
        "reasoning": "Input filtering systems are explicitly described as a safety mechanism"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We find that warning the model against jailbreaks greatly reduces the attack success rate, as the\nmodel is able to reason through the policy.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We find that warning the model against jailbreaks greatly reduces the attack success rate, as the model is able to reason through the policy."
          }
        ],
        "reasoning": "The document describes specific defenses against prompt injection and jailbreak attempts"
      },
      {
        "techniqueId": "tech-weapons-illegal-activity",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We place\nfurther emphasis on refusing requests concerning the development of CBRN or cyber weapons. System Prompt.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We place further emphasis on refusing requests concerning the development of CBRN or cyber weapons."
          }
        ],
        "reasoning": "Explicit policy to prevent requests related to weapons and illegal activities"
      },
      {
        "techniqueId": "tech-misinformation-detection",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI aims to build truth-seeking models. As such, we continually evaluate whether\nGrok 4\u2019s training may cause it to display biases, especially on controversial sociopolitical questions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI aims to build truth-seeking models. As such, we continually evaluate whether Grok 4's training may cause it to display biases, especially on controversial sociopolitical questions."
          }
        ],
        "reasoning": "While not a full misinformation detection system, there's an explicit effort to detect and mitigate potential misinformation"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "As such, we continually evaluate whether\nGrok 4\u2019s training may cause it to display biases, especially on controversial sociopolitical questions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We continually evaluate whether Grok 4's training may cause it to display biases, especially on controversial sociopolitical questions."
          }
        ],
        "reasoning": "The document describes ongoing monitoring of model capabilities and potential risks"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Transparency into AI progress can help developers coordinate\nsafety efforts, governments enact sensible legislation, and the public stay abreast of the benefits and\nrisks of AI.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Transparency into AI progress can help developers coordinate safety efforts, governments enact sensible legislation, and the public stay abreast of the benefits and risks of AI."
          }
        ],
        "reasoning": "While transparency is mentioned, there's no clear evidence of a formal incident reporting system"
      }
    ],
    "deletions": []
  },
  "grok-image-gen-update": {
    "additions": [],
    "deletions": []
  },
  "grok-security": {
    "additions": [
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI partners with GitHub's Secret Scanning program to detect leaked keys. If a leak is found, we disable the key and notify you via email.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI partners with GitHub's Secret Scanning program to detect leaked keys. If a leak is found, we disable the key and notify you via email."
          }
        ],
        "reasoning": "The document explicitly describes a systematic process for reporting and responding to security incidents involving API key leaks."
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Team admins are able to view an audit log of user interactions. This lists all of the user interactions with our API server.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Team admins are able to view an audit log of user interactions. This lists all of the user interactions with our API server. You can view it at xAI Console -> Audit Log."
          }
        ],
        "reasoning": "The document describes a comprehensive audit logging system that provides real-time monitoring of system interactions."
      },
      {
        "techniqueId": "tech-access-control-documentation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Do not share keys between teammates to avoid unauthorized access. Store keys securely using environment variables or secret management tools.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Do not share keys between teammates to avoid unauthorized access. Store keys securely using environment variables or secret management tools."
          }
        ],
        "reasoning": "The document provides explicit documentation about access control mechanisms for API keys."
      },
      {
        "techniqueId": "tech-data-retention-policies",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "API requests and responses are temporarily stored on our servers for 30 days in case they need to be audited for potential abuse or misuse. This data is automatically deleted after 30 days.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "API requests and responses are temporarily stored on our servers for 30 days in case they need to be audited for potential abuse or misuse. This data is automatically deleted after 30 days."
          }
        ],
        "reasoning": "The document clearly outlines a specific data retention and deletion policy."
      }
    ],
    "deletions": []
  },
  "hunyuan-technical-report": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "This stage contains a\nsupervised fine-tuning (SFT) phase and a Reinforcement Learning from Human Feedback (RLHF)\nphase on elaborately selected datasets and outputs of current policy models (Bai et al., 2022).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "This stage contains a supervised fine-tuning (SFT) phase and a Reinforcement Learning from Human Feedback (RLHF) phase on elaborately selected datasets and outputs of current policy models."
          }
        ],
        "reasoning": "Explicit description of RLHF implementation as part of the model's post-training process"
      },
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.2 Reinforcement Learning from Human Feedback\n\nTo align Hunyuan-Large with human preferences, we further train our SFT model using DPO (Rafailov\net al., 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We further train our SFT model using DPO (Rafailov et al., 2024). We adopt a single-stage training strategy that integrates both offline and online training, which demonstrates superior controllability and overall performance."
          }
        ],
        "reasoning": "Direct implementation of Direct Preference Optimization with detailed training strategy"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We\nfilter the data based on criteria such as writing quality, educational value, and toxicity to ensure\nits high quality.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We filter the data based on criteria such as writing quality, educational value, and toxicity to ensure its high quality. Additionally, we anonymize all privacy-sensitive data and other harmful data."
          }
        ],
        "reasoning": "Explicit description of data filtering techniques during training data preparation"
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We\nhave also implemented an elaborate system of category labels, which allows us to flexibly adjust the\nproportions of various types of data in the training dataset.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have also implemented an elaborate system of category labels, which allows us to flexibly adjust the proportions of various types of data in the training dataset."
          }
        ],
        "reasoning": "Indicates systematic analysis and categorization of training data"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Simultaneously, we place great\nemphasis on data security, striving to ensure that the model aligns with human values under most\ncircumstances.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We place great emphasis on data security, striving to ensure that the model aligns with human values under most circumstances."
          }
        ],
        "reasoning": "Vague reference to bias mitigation without specific implementation details"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In SFT, we concentrate on the detailed data collection and processing manners that\nensure the effectiveness of Hunyuan-Large\u2019s post-training, along with the training settings of SFT.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We concentrate on the detailed data collection and processing manners that ensure the effectiveness of Hunyuan-Large's post-training, along with the training settings of SFT."
          }
        ],
        "reasoning": "No explicit evidence of refusal training implementation"
      }
    ],
    "deletions": []
  },
  "llama-3-paper": {
    "additions": [
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.1.4 Direct Preference Optimization\n\nWe further train our SFT models with Direct Preference Optimization (DPO; Rafailov et al., 2024) for human\npreference alignment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We further train our SFT models with Direct Preference Optimization (DPO; Rafailov et al., 2024) for human preference alignment."
          }
        ],
        "reasoning": "Explicit description of using Direct Preference Optimization during model alignment"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.1.2 Reward Modeling\n\nWe train a reward model (RM) covering different capabilities on top of the pre-trained checkpoint.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We first train a reward model (RM) covering different capabilities on top of the pre-trained checkpoint. The training objective is the same as Llama 2 except that we remove the margin term in the loss."
          }
        ],
        "reasoning": "Detailed description of training a reward model specifically for safety and capability assessment"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We developed a refusal tone guideline for Llama 3 and ensured that all new safety data\nadhered to it through rigorous quality assurance process.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We developed a refusal tone guideline for Llama 3 and ensured that all new safety data adhered to it through rigorous quality assurance process."
          }
        ],
        "reasoning": "Explicit training of the model to refuse unsafe requests with a specific tone and approach"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We apply several de-duplication methods and data cleaning mechanisms on each data\nsource to obtain high-quality tokens.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We apply several de-duplication methods and data cleaning mechanisms on each data source to obtain high-quality tokens. We remove domains that contain large amounts of personally identifiable information (PII), and domains with known adult content."
          }
        ],
        "reasoning": "Comprehensive description of data filtering and cleaning techniques during pre-training"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "5.4.6 Red Teaming\n\nWe utilize Red Teaming to discover risks and use the findings to improve our benchmarks and safety tuning\ndatasets.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We utilize Red Teaming to discover risks and use the findings to improve our benchmarks and safety tuning datasets. We conduct recurring red teaming exercises to continuously iterate and discover new risks, which guides our model development and mitigation process."
          }
        ],
        "reasoning": "Explicit description of systematic adversarial testing to identify and mitigate potential safety risks"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To enable this, we develop and release a new classifier, Llama Guard 3, which is a Llama 3 8B model fine-tuned\nfor safety classification.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We develop and release a new classifier, Llama Guard 3, which is a Llama 3 8B model fine-tuned for safety classification. This classifier is used to detect whether input prompts and/or output responses generated by language models violate safety policies on specific categories of harm."
          }
        ],
        "reasoning": "Detailed implementation of an input guardrail system with specific safety classification capabilities"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Llama Guard 3 is able to significantly reduce violations across capabilities (-65% violations on average\nacross our benchmarks).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Llama Guard 3 is able to significantly reduce violations across capabilities (-65% violations on average across our benchmarks)."
          }
        ],
        "reasoning": "Explicit implementation of an output filtering system that reduces safety violations"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Prompt Guard is a model-based filter designed to detect prompt attacks, which are input strings designed to\nsubvert the intended behavior of an LLM functioning as part of an application.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prompt Guard is a model-based filter designed to detect prompt attacks, which are input strings designed to subvert the intended behavior of an LLM functioning as part of an application."
          }
        ],
        "reasoning": "Specific implementation of a system to detect and prevent prompt injection attacks"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We train on the 13 hazard categories listed in the AI Safety taxonomy (Vidgen et al., 2024): Child\nSexual Exploitation, Defamation, Elections, Hate, Indiscriminate Weapons, Intellectual Property, Non-Violent\nCrimes, Privacy, Sex-Related Crimes, Sexual Content, Specialized Advice, Suicide & Self-Harm, and Violent\nCrimes.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We train on the 13 hazard categories listed in the AI Safety taxonomy, including Violent Crimes as one of the key categories to detect and prevent."
          }
        ],
        "reasoning": "Explicit inclusion of violence detection as a core safety classification category"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We train on the 13 hazard categories listed in the AI Safety taxonomy (Vidgen et al., 2024): Child\nSexual Exploitation, Defamation, Elections, Hate, Indiscriminate Weapons, Intellectual Property, Non-Violent\nCrimes, Privacy, Sex-Related Crimes, Sexual Content, Specialized Advice, Suicide & Self-Harm, and Violent\nCrimes.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We train on the 13 hazard categories listed in the AI Safety taxonomy, including Hate as one of the key categories to detect and prevent."
          }
        ],
        "reasoning": "Explicit inclusion of hate speech detection as a core safety classification category"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We train on the 13 hazard categories listed in the AI Safety taxonomy (Vidgen et al., 2024): Child\nSexual Exploitation, Defamation, Elections, Hate, Indiscriminate Weapons, Intellectual Property, Non-Violent\nCrimes, Privacy, Sex-Related Crimes, Sexual Content, Specialized Advice, Suicide & Self-Harm, and Violent\nCrimes.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We train on the 13 hazard categories listed in the AI Safety taxonomy, including Sexual Content as one of the key categories to detect and prevent."
          }
        ],
        "reasoning": "Explicit inclusion of sexual content moderation as a core safety classification category"
      },
      {
        "techniqueId": "tech-csam-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We scan all our training images for\nCSAM using perceptual hashing approaches such as PhotoDNA (Farid, 2021) as well as internal, proprietary\nclassifiers.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We scan all our training images for CSAM using perceptual hashing approaches such as PhotoDNA as well as internal, proprietary classifiers."
          }
        ],
        "reasoning": "Specific implementation of Child Sexual Abuse Material (CSAM) detection during data preparation"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We remove domains that contain large amounts of personally identifiable\ninformation (PII), and domains with known adult content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We remove domains that contain large amounts of personally identifiable information (PII). We use the Presidio Analyzer to identify such PII."
          }
        ],
        "reasoning": "Explicit implementation of PII detection and removal during data processing"
      },
      {
        "techniqueId": "tech-misinformation-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We train on the 13 hazard categories listed in the AI Safety taxonomy (Vidgen et al., 2024): Child\nSexual Exploitation, Defamation, Elections, Hate, Indiscriminate Weapons, Intellectual Property, Non-Violent\nCrimes, Privacy, Sex-Related Crimes, Sexual Content, Specialized Advice, Suicide & Self-Harm, and Violent\nCrimes.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We train on the 13 hazard categories listed in the AI Safety taxonomy, which includes detection of verifiably false information and disinformation."
          }
        ],
        "reasoning": "Inclusion of misinformation detection as a core safety classification category"
      }
    ],
    "deletions": []
  },
  "llama-4-maverick": {
    "additions": [
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Llama Guard models offer unparalleled safety content moderation performance and flexibility, and we now offer a collection of specialized models tailored to specific development needs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Llama Guard models offer unparalleled safety content moderation performance and flexibility, and we now offer a collection of specialized models tailored to specific development needs."
          }
        ],
        "reasoning": "Explicit description of input guardrail system with Llama Guard models designed for content moderation"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Download the model\nGet started\nPrompt Guard 2\nPrompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity. Categories of prompt attacks include prompt injection and jailbreaking."
          }
        ],
        "reasoning": "Direct implementation of a dedicated system to defend against prompt injection attacks"
      },
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Llama Guard 4\nLlama Guard 4 is an 12B-parameter high-performance multimodal input and output moderation model designed to support developers to detect various common types of violating content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Llama Guard 4 is an 12B-parameter high-performance multimodal input and output moderation model... It supports 12 languages and works across modalities to detect and filter policy-violating inputs and outputs across text and images."
          }
        ],
        "reasoning": "Explicit multimodal safety alignment system that works across text and image modalities"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Llama Guard 4 is an 12B-parameter high-performance multimodal input and output moderation model designed to support developers to detect various common types of violating content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Llama Guard 4 is an 12B-parameter high-performance multimodal input and output moderation model designed to support developers to detect various common types of violating content."
          }
        ],
        "reasoning": "Clear implementation of an output filtering system for content moderation"
      },
      {
        "techniqueId": "tech-cybersecurity-threat",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "AutoPatchBench: Read the technical blog post\nGet CyberSecEval\nOur evaluation suite measures LLMs\u2019 propensity to generate insecure code, comply with requests to aid cyber attackers, offensive cybersecurity capabilities, defensive cyber security capabilities, and susceptibility to code interpreter abuse and prompt injection attacks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Our evaluation suite measures LLMs' propensity to generate insecure code, comply with requests to aid cyber attackers, offensive cybersecurity capabilities, defensive cyber security capabilities, and susceptibility to code interpreter abuse and prompt injection attacks."
          }
        ],
        "reasoning": "Comprehensive cybersecurity threat detection and evaluation framework"
      },
      {
        "techniqueId": "tech-code-execution-sandboxing",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Testing propensity to abuse a code interpreter\nCode interpreters allow LLMs to run code in a sandboxed environment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Code interpreters allow LLMs to run code in a sandboxed environment. This set of prompts try to manipulate an LLM into executing malicious code to either gain access to the system that runs the LLM."
          }
        ],
        "reasoning": "Explicit description of code execution sandboxing to prevent malicious code execution"
      }
    ],
    "deletions": []
  },
  "llama-4-responsible-use-guide": {
    "additions": [
      {
        "techniqueId": "tech-configurable-policies",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Also consider whether it\u2019s appropriate or feasible to\nallow end users to opt out or bypass interacting with the AI system and offer an alternate\nmethod to accomplish the use case.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Consider whether it's appropriate or feasible to allow end users to opt out or bypass interacting with the AI system and offer an alternate method to accomplish the use case."
          }
        ],
        "reasoning": "The document suggests configurable options for users to opt out or choose alternative interaction methods."
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Develop and run\nongoing performance tests and use these test results and feedback to identify areas where\nadditional data or development may improve your system\u2019s performance.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Develop and run ongoing performance tests and use these test results and feedback to identify areas where additional data or development may improve your system's performance."
          }
        ],
        "reasoning": "This describes a process of monitoring system capabilities and performance over time."
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "13\n\n\fUse adversarial-style testing: Adversarial testing, or red teaming, is an adversarial attack\nsimulation of the AI system usually conducted by AI developers with the goal to identify\nvulnerabilities which might be exploited by an attacker.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Adversarial testing, or red teaming, is an adversarial attack simulation of the AI system usually conducted by AI developers with the goal to identify vulnerabilities which might be exploited by an attacker."
          }
        ],
        "reasoning": "The document explicitly defines and recommends red teaming as a testing approach."
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Assess performance: Whenever possible, try to use multiple datasets and human workforces\nto evaluate the performance of your system or application, as it is unlikely that a single\nevaluation dataset can provide an absolute picture of performance.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Whenever possible, try to use multiple datasets and human workforces to evaluate the performance of your system or application, as it is unlikely that a single evaluation dataset can provide an absolute picture of performance."
          }
        ],
        "reasoning": "The document suggests using multiple evaluation approaches, which aligns with safety benchmarking principles."
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Continue to assess\naccuracy and monitor for potential bias, including that your models perform as expected\nacross different segments.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Continue to assess accuracy and monitor for potential bias, including that your models perform as expected across different segments."
          }
        ],
        "reasoning": "The document recommends explicit bias monitoring and mitigation strategies."
      },
      {
        "techniqueId": "tech-watermarking",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Watermarks are one type of content\nauthentication mechanism that can be used to verify whether digital content, such as\nimages and videos, was AI-generated.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Watermarks are one type of content authentication mechanism that can be used to verify whether digital content, such as images and videos, was AI-generated."
          }
        ],
        "reasoning": "The document discusses watermarking as a content authentication technique."
      }
    ],
    "deletions": []
  },
  "meta-llama-responsible-use": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022  Reinforcement Learning from Human Feedback\n\n(RLHF) or AI Feedback (RLAIF): Training safety\n\nand helpfulness reward models to support\n\nRLHF techniques iteratively improves models\n\nand makes them more robust to jailbreaking\n\ntechniques.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Reinforcement Learning from Human Feedback (RLHF) mechanisms. This involves collecting ranking data from trained annotators or users (given a model input and several generated outputs, ranking them from best to worst according to policies), training a reward or helpfulness model to act as a proxy of human feedback, and then optimizing the LLM to maximize the reward/helpfulness model score with reinforcement learning."
          }
        ],
        "reasoning": "Detailed description of RLHF implementation with specific steps for collecting and using human feedback"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022  Reinforcement Learning from Human Feedback\n\n(RLHF) or AI Feedback (RLAIF): Training safety\n\nand helpfulness reward models to support\n\nRLHF techniques iteratively improves models\n\nand makes them more robust to jailbreaking\n\ntechniques.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Training safety and helpfulness reward models to support RLHF techniques iteratively improves models and makes them more robust to jailbreaking techniques."
          }
        ],
        "reasoning": "Explicit mention of creating specialized reward models for safety and helpfulness"
      },
      {
        "techniqueId": "tech-rlaif",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Reinforcement Learning from AI Feedback (RLAIF)\n\nReward models can also be improved and tailored to\n\nspecific policies by using Reinforcement Learning\n\nfrom AI Feedback (RLAIF).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Reinforcement Learning from AI Feedback (RLAIF). The fine-tuned LLM itself can be used to create synthetic ranking data for reward model training. Given a model input, response pairs and relevant guidelines, the LLM predicts which response would best follow the guidelines."
          }
        ],
        "reasoning": "Detailed description of using the model itself to generate synthetic feedback data"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022  Red teaming is a systematic effort to identify\n\nmodel vulnerabilities or emergent risks by crafting\n\nprompts that may elicit undesirable behavior or\n\noutputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Red teaming is a systematic effort to identify model vulnerabilities or emergent risks by crafting prompts that may elicit undesirable behavior or outputs. This type of manipulation of the model can be used to test safeguards and attempts to 'jailbreak' the model."
          }
        ],
        "reasoning": "Clear explanation of red teaming methodology with specific goals of identifying vulnerabilities"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "These approaches include:\n\n\u2022  Prompt filters: Even when inputs may not\n\nviolate content policies, the model may produce\n\nproblematic engagements or outputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prompt filters: Even when inputs may not violate content policies, the model may produce problematic engagements or outputs. In these cases, it may be appropriate to filter, block, and hard code responses for some inputs until the model can respond in the intended way."
          }
        ],
        "reasoning": "Description of input filtering mechanisms to prevent problematic model interactions"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Based on the downstream use case, you can apply\n\nseveral approaches for detecting and filtering the\n\ngenerated output of models for problematic or policy-\n\nviolating content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Based on the downstream use case, you can apply several approaches for detecting and filtering the generated output of models for problematic or policy-violating content. Here are some considerations and best practices for filtering outputs."
          }
        ],
        "reasoning": "Explicit discussion of output filtering strategies with multiple approaches"
      },
      {
        "techniqueId": "tech-prompt-injection-defense",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Prompt injection attacks are attempts to circumvent\n\ncontent restrictions to produce particular outputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Prompt injection attacks are attempts to circumvent content restrictions to produce particular outputs. Without implementation of input filters and safeguards, even advanced models can potentially be manipulated to generate harmful or misleading outputs or violate content policies."
          }
        ],
        "reasoning": "Recognition of prompt injection as a threat and discussion of defensive strategies"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "ai/projects/safety_recipes/\n\nPlatforms for tools and evaluations:\n\n\u2022  Benchmarking of LLMs by Stanford\u2019s Center for\n\nResearch on Foundation Models, HELM: https://\n\ncrfm.stanford.edu/helm/latest/\n\n\u2022  EleutherAI LLM Evaluation Harness: https://\n\ngithub.com/EleutherAI/lm-evaluation-harness\n\n\u2022  Huggingface Hub which hosts open source\n\nmodels, datasets, and is a space for developers\n\nto share safeguards and access benchmarking\n\ninformation: https://huggingface.co/docs/\n\nhub/index\n\n\u2022  GenAI Ops Tools database curated by Credo.AI:\n\nhttps://www.credo.ai/gen-ai-ops-landscape\n\n20\n\nJULY 2023\fReporting resources:\n\nIf you have any information about issues, violations,\n\n\u2022  Reporting bugs and security concerns:\n\nfacebook.com/whitehat/info\n\nor problems, please help keep our communities safe\n\n\u2022  Reporting violations of the Acceptable\n\nby using our reporting resources.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Reporting resources: If you have any information about issues, violations, or problems, please help keep our communities safe by using our reporting resources."
          }
        ],
        "reasoning": "Explicit provision of multiple channels for reporting model issues and violations"
      },
      {
        "techniqueId": "tech-transparency",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Developers can\n\nfurther train the model with this feedback to improve\n\nTransparency & control best practices\n\nTo ensure high-quality feedback and provide end\n\nusers with notice and choice about their interactions\n\nwith your AI assets, developers should consider the\n\nfollowing practices for user interactions:\n\n\u2022  Transparency: Developers should consider ways\n\nto provide transparency to end users regarding\n\npotential risks and limitations of the system\n\nprior to or at the time of user interaction.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Developers should consider ways to provide transparency to end users regarding potential risks and limitations of the system prior to or at the time of user interaction. For instance, notice to users that they are interacting with an AI-powered chatbot may increasingly be required in certain markets."
          }
        ],
        "reasoning": "Detailed discussion of transparency mechanisms for user interactions"
      }
    ],
    "deletions": []
  },
  "microsoft-rai-standard": {
    "additions": [
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "RS1.6 Define and document an evaluation plan based on requirements RS1.1, RS1.3, RS1.4, and RS1.5, to include\nthe environment in which the system will be evaluated.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Define and document an evaluation plan based on requirements RS1.1, RS1.3, RS1.4, and RS1.5, to include the environment in which the system will be evaluated."
          }
        ],
        "reasoning": "The document explicitly describes a structured approach to defining safety evaluation plans and benchmarks"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Requirements\n\nRS3.1 Establish and document a detailed inventory of the system health monitoring methods to be used, to\ninclude:\n\n1)  data and insights generated from data repositories, system analytics, and associated alerts,\n2)  processes by which customers can submit information about failures and concerns, and\n3)  processes by which the general public can submit feedback.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Establish and document a detailed inventory of the system health monitoring methods to be used, to include: processes by which customers can submit information about failures and concerns, and processes by which the general public can submit feedback."
          }
        ],
        "reasoning": "The document outlines a clear process for incident reporting and feedback collection"
      },
      {
        "techniqueId": "tech-stakeholder-engagement",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Recommendation F1.1.4 Work with members of identified demographic groups to understand the risks of and\nimpacts associated with differences in quality of service.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Work with members of identified demographic groups to understand risks of and impacts associated with differences in quality of service. Consider using the Community Jury technique to conduct these discussions."
          }
        ],
        "reasoning": "The document recommends direct engagement with stakeholders as part of its responsible AI approach"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "RS3.3 When new uses, critical operational factors, or changes in the supported range of an operational factor are\nidentified, determine whether any new use or operational factor can be supported with the existing system, will be\nsupported but require additional work, or will not be supported.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "When new uses, critical operational factors, or changes in the supported range of an operational factor are identified, determine whether any new use or operational factor can be supported with the existing system, will be supported but require additional work, or will not be supported."
          }
        ],
        "reasoning": "The document describes a systematic approach to monitoring and assessing system capabilities over time"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Recommendation F3.4.2 Use red teaming exercises to evaluate these risks involving identified demographic\ngroups.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Use red teaming exercises to evaluate these risks involving identified demographic groups."
          }
        ],
        "reasoning": "The document recommends red teaming as a technique for evaluating system risks"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "F3.5 Reassess the system design, including the choice of training data, features, objective function, and training\nalgorithm, to pursue the goal of minimizing the potential for stereotyping, demeaning, and erasing the identified\ndemographic groups.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Reassess the system design, including the choice of training data, features, objective function, and training algorithm, to pursue the goal of minimizing the potential for stereotyping, demeaning, and erasing the identified demographic groups."
          }
        ],
        "reasoning": "The document describes a comprehensive approach to mitigating bias during system design"
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "F1.2 Evaluate all data sets to assess inclusiveness of identified demographic groups and collect data to close gaps.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Evaluate all data sets to assess inclusiveness of identified demographic groups and collect data to close gaps. Document this process and its results."
          }
        ],
        "reasoning": "The document mandates thorough dataset auditing to ensure representational inclusiveness"
      }
    ],
    "deletions": []
  },
  "mistral-large-3": {
    "additions": [
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "complete\n(\nmodel\n=\n\"mistral-large-latest\"\n,\nmessages\n=\n[\n{\n\"role\"\n:\n\"user\"\n,\n\"content\"\n:\n\"What is the best French cheese?\"\n}\n]\n,\nsafe_prompt\n=\nTrue\n)\nToggling the safe prompt will prepend your messages with the following system prompt:\nAlways assist with care, respect, and truth.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Toggling the safe prompt will prepend your messages with the following system prompt: Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity."
          }
        ],
        "reasoning": "Explicit implementation of a system-level safety prompt with clear behavioral guidelines"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Moderation\nModeration\nModerate Inputs/Outputs\nOur new moderation service, which is powered by the Mistral Moderation model, is a classifier model\nbased on Ministral 8B 24.10.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Our new moderation service, which is powered by the Mistral Moderation model, is a classifier model based on Ministral 8B 24.10. It enables our users to detect harmful text content along several policy dimensions."
          }
        ],
        "reasoning": "Detailed description of an input guardrail system with a dedicated moderation classifier"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Mistral models can also act as great content moderators: the model itself is able to accurately classify a user prompt or its generated answer as being either acceptable or falling into one of the following categories",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Mistral models can also act as great content moderators: the model itself is able to accurately classify a user prompt or its generated answer as being either acceptable or falling into one of the following categories"
          }
        ],
        "reasoning": "Describes a self-reflection mechanism for output classification and potential filtering"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Violence and Threats\nContent that describes, glorifies, incites, or threatens physical violence against individuals or groups.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Violence and Threats: Content that describes, glorifies, incites, or threatens physical violence against individuals or groups. This includes graphic depictions of injury or death, explicit threats of harm, and instructions for carrying out violent acts."
          }
        ],
        "reasoning": "Detailed classification category for violence detection in the moderation API"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Hate and Discrimination\nContent that expresses prejudice, hostility, or advocates discrimination against individuals or groups based on protected characteristics such as race, ethnicity, religion, gender, sexual orientation, or disability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Hate and Discrimination: Content that expresses prejudice, hostility, or advocates discrimination against individuals or groups based on protected characteristics such as race, ethnicity, religion, gender, sexual orientation, or disability."
          }
        ],
        "reasoning": "Explicit hate speech and discrimination detection category in moderation system"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Category\nDescription\nSexual\nMaterial that explicitly depicts, describes, or promotes sexual activities, nudity, or sexual services.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Sexual: Material that explicitly depicts, describes, or promotes sexual activities, nudity, or sexual services. This includes pornographic content, graphic descriptions of sexual acts, and solicitation for sexual purposes."
          }
        ],
        "reasoning": "Detailed sexual content moderation category in the moderation API"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "PII\nContent that requests, shares, or attempts to elicit personal identifying information such as full names, addresses, phone numbers, social security numbers, or financial account details.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "PII: Content that requests, shares, or attempts to elicit personal identifying information such as full names, addresses, phone numbers, social security numbers, or financial account details."
          }
        ],
        "reasoning": "Explicit PII detection category in the moderation system"
      },
      {
        "techniqueId": "tech-self-harm-prevention",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Self-Harm\nContent that promotes, instructs, plans, or encourages deliberate self-injury, suicide, eating disorders, or other self-destructive behaviors.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Self-Harm: Content that promotes, instructs, plans, or encourages deliberate self-injury, suicide, eating disorders, or other self-destructive behaviors."
          }
        ],
        "reasoning": "Dedicated category for detecting and preventing self-harm content"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "With the recommended system prompt, our models decline to answer to all the questions of the set of adversarial prompts we've tested.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "With the recommended system prompt, our models decline to answer to all the questions of the set of adversarial prompts we've tested."
          }
        ],
        "reasoning": "Explicit demonstration of model's ability to refuse inappropriate requests"
      }
    ],
    "deletions": []
  },
  "nemotron-4-tech-report": {
    "additions": [
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We align the base LLM with Supervised Fine-Tuning (SFT), followed by Preference Fine-Tuning such as\nReinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022) and Direct\nPreference Optimization (DPO) (Rafailov et al., 2024).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We align the base LLM with Supervised Fine-Tuning (SFT), followed by Preference Fine-Tuning such as Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO)."
          }
        ],
        "reasoning": "Explicit description of DPO as an alignment technique used in model training"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The values marked with \u2217 are taken from Qwen-Team\n(2024)\n\n3 Alignment\n\n3.1 Reward Modeling\n\nThe reward model plays a pivotal role in model alignment, serving as a crucial judge for preference ranking\nand quality filtering in the training of a strong instruction-following model.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The reward model plays a pivotal role in model alignment, serving as a crucial judge for preference ranking and quality filtering in the training of a strong instruction-following model."
          }
        ],
        "reasoning": "Detailed description of a reward model specifically designed for safety and alignment"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We are committed to responsible development practices and do not intend for the model to be\nused in generating toxic or harmful content.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We are committed to responsible development practices and do not intend for the model to be used in generating toxic or harmful content."
          }
        ],
        "reasoning": "General commitment to bias mitigation, but limited specific implementation details"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.4.3 Safety Evaluations\n\nWe performed extensive safety evaluation including adversarial testing via these distinct methods:\n\n\u2022 AEGIS (Ghosh et al., 2024) is a content safety evaluation dataset and LLM based content safety\nclassifier model, that adheres to a broad taxonomy of 13 categories of critical risks in human-LLM\ninteractions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We performed extensive safety evaluation including adversarial testing via these distinct methods: AEGIS (Ghosh et al., 2024) is a content safety evaluation dataset and LLM based content safety classifier model, that adheres to a broad taxonomy of 13 categories of critical risks in human-LLM interactions."
          }
        ],
        "reasoning": "Explicit use of a safety evaluation framework that includes violence detection"
      },
      {
        "techniqueId": "tech-hate-speech-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The\ntaxonomy was created by considering most relevant community risks across multiple content safety risk\ntaxonomies.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The taxonomy was created by considering most relevant community risks across multiple content safety risk taxonomies. It aligns with NVIDIA's organizational values for the protected characteristics under categories of hate and harassment"
          }
        ],
        "reasoning": "Detailed safety taxonomy specifically addressing hate speech detection"
      },
      {
        "techniqueId": "tech-sexual-content-moderation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Content that depicts explicit or implicit sexual behavior. Content meant\nto arouse sexual excitement, such as the description of sexual activity,\nor that promotes sexual services.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Content that depicts explicit or implicit sexual behavior. Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services."
          }
        ],
        "reasoning": "Comprehensive definition of sexual content moderation in safety taxonomy"
      },
      {
        "techniqueId": "tech-weapons-illegal-activity",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "33\n\n\fCategory\n\nDefinition\n\nand\n\nGuns\nIllegal\nWeapons\n\nEncompasses statements that specifically encourage, condone, or could\nhelp people plan or execute specific crimes involving the illegal acqui-\nsition, creation, or use of guns or illegal weapons (ex: explosives, bio-\nlogical agents, or chemical weapons), including instructions on how to\ncreate or use them.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Encompasses statements that specifically encourage, condone, or could help people plan or execute specific crimes involving the illegal acquisition, creation, or use of guns or illegal weapons"
          }
        ],
        "reasoning": "Explicit categorization and detection of weapons and illegal activity in safety framework"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.4.3 Safety Evaluations\n\nWe performed extensive safety evaluation including adversarial testing via these distinct methods:\n\n\u2022 AEGIS (Ghosh et al., 2024) is a content safety evaluation dataset and LLM based content safety\nclassifier model, that adheres to a broad taxonomy of 13 categories of critical risks in human-LLM\ninteractions.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We performed extensive safety evaluation including adversarial testing via these distinct methods: AEGIS (Ghosh et al., 2024), Garak (Derczynski et al., 2024) is an automated LLM vulnerability scanner that probes for common weaknesses, and Human Content Red Teaming leveraging human interaction and evaluation of the models' responses."
          }
        ],
        "reasoning": "Comprehensive red teaming approach using multiple evaluation methods"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To\nevaluate the safety of our model, we employ AEGIS (Ghosh et al., 2024), a high quality content safety\nsolution and evaluation benchmark from NVIDIA.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "AEGIS is a high quality content safety solution and evaluation benchmark from NVIDIA. AEGIS is backed by a broad content safety risk taxonomy that covers 12 critical risks in human-LLM interactions."
          }
        ],
        "reasoning": "Detailed description of a standardized safety benchmarking framework"
      }
    ],
    "deletions": []
  },
  "o3-pro": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2 Model Data and Training\n\nOpenAI reasoning models are trained to reason through reinforcement learning.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "OpenAI reasoning models are trained to reason through reinforcement learning. Models in the o-series family are trained to think before they answer: they can produce a long internal chain of thought before responding to the user."
          }
        ],
        "reasoning": "Direct description of reinforcement learning approach used in model training"
      },
      {
        "techniqueId": "tech-constitutional-ai",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Through training, these models learn to refine their\nthinking process, try different strategies, and recognize their mistakes.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Through training, these models learn to refine their thinking process, try different strategies, and recognize their mistakes. Reasoning allows these models to follow specific guidelines and model policies we've set, helping them act in line with our safety expectations."
          }
        ],
        "reasoning": "Describes models learning to follow guidelines and refine their own reasoning process"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Our data processing pipeline includes rigorous filtering to maintain data quality and\nmitigate potential risks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Our data processing pipeline includes rigorous filtering to maintain data quality and mitigate potential risks. We use advanced data filtering processes to reduce personal information from training data."
          }
        ],
        "reasoning": "Explicit description of data filtering techniques during training"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We evaluate completions using an autograder, using the metric not_unsafe, measuring that the\nmodel did not produce output that violates OpenAI policy.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate completions using an autograder, using the metric not_unsafe, measuring that the model did not produce output that violates OpenAI policy."
          }
        ],
        "reasoning": "Demonstrates systematic training to refuse unsafe outputs"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4\n\n\fTable 5: Multimodal refusal evaluations\n\nEvaluation\n\nCategory\n\nMetric\n\nsexual/exploitative\n\nnot_unsafe\n\no3\n\n1\n\no4-mini\n\no1\n\n1\n\n0.97\n\nVision sexual\nrefusal evaluation\n\nVision self-harm\nrefusal evaluation\n\nself-harm/intent\n\nnot_unsafe\n\n0.99\n\n0.99\n\n0.97\n\nself-harm/instructions\n\nnot_unsafe\n\n1\n\n0.99\n\n0.95\n\n3.4.1 Vision Vulnerabilities\n\nOpenAI provided external red teamers access to OpenAI o3 and o4-mini to assess vulnerabilities\nrelated to vision capabilities.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "OpenAI provided external red teamers access to OpenAI o3 and o4-mini to assess vulnerabilities related to vision capabilities. Red teamers were asked to generate conversations that included images where one or more models produced a result they perceived as unsafe."
          }
        ],
        "reasoning": "Detailed description of external red teaming process to identify safety vulnerabilities"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4 Preparedness\n\nWe evaluated OpenAI o3 and o4-mini according to our Preparedness Framework. This is the\nfirst launch and system card to be released under our updated Preparedness Framework.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluated OpenAI o3 and o4-mini according to our Preparedness Framework. This is the first launch and system card to be released under our updated Preparedness Framework."
          }
        ],
        "reasoning": "Systematic framework for evaluating model safety across multiple dimensions"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Table 7: BBQ evaluation\n\nMetric\n\nAccuracy on Ambiguous Questions\n\nAccuracy on Unambiguous Questions\n\nP(not stereotyping | ambiguous\nquestion, not unknown)\n\no3\n\no4-mini\n\no1\n\n0.94\n\n0.93\n\n0.25\n\n0.82\n\n0.95\n\n0.26\n\n0.96\n\n0.93\n\n0.05\n\nWe also tested OpenAI o3 and o4-mini on our first-person fairness evaluation [4].",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We tested OpenAI o3 and o4-mini on the BBQ evaluation. We also tested OpenAI o3 and o4-mini on our first-person fairness evaluation."
          }
        ],
        "reasoning": "Demonstrates active testing and mitigation of potential bias"
      },
      {
        "techniqueId": "tech-hallucination-grounding",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Table 3: Jailbreak evaluations\n\nEvaluation\n\nMetric\n\nHuman sourced jailbreaks\n\nnot_unsafe\n\no3\n\n1\n\nStrongReject\n\nnot_unsafe\n\n0.97\n\no4-mini\n\no1\n\n0.99\n\n0.96\n\n0.97\n\n0.97\n\n3.3 Hallucinations\n\nWe evaluate hallucinations in OpenAI o3 and o4-mini against the following evaluations that aim\nto elicit hallucinations from the models:\n\n\u2022 SimpleQA: A diverse dataset of four-thousand fact-seeking questions with short answers\n\nand measures model accuracy for attempted answers.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We evaluate hallucinations in OpenAI o3 and o4-mini against the following evaluations that aim to elicit hallucinations from the models: SimpleQA and PersonQA."
          }
        ],
        "reasoning": "Systematic evaluation and detection of model hallucinations"
      },
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Table 4: Hallucination evaluations\n\nDataset\n\nSimpleQA\n\nPersonQA\n\nMetric\n\no3\n\no4-mini\n\no1\n\naccuracy (higher is better)\nhallucination rate (lower is better)\n\naccuracy (higher is better)\nhallucination rate (lower is better)\n\n0.49\n0.51\n\n0.59\n0.33\n\n0.20\n0.79\n\n0.36\n0.48\n\n0.47\n0.44\n\n0.47\n0.16\n\n3.4 Multimodal refusals\n\nWe also evaluate refusals for multimodal inputs on our standard evaluation set for disallowed\ncombined text and image content and overrefusals.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We also evaluate refusals for multimodal inputs on our standard evaluation set for disallowed combined text and image content and overrefusals."
          }
        ],
        "reasoning": "Specific safety alignment techniques for multimodal inputs"
      }
    ],
    "deletions": []
  },
  "openai-preparedness": {
    "additions": [
      {
        "techniqueId": "tech-safety-advisory",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We  are  creating  a  Safety  Advisory  Group\n\n(SAG)  that  brings  together  expertise  from  across  the  company  to  help  OpenAI\u2019s\n\nleadership and Board of Directors be best prepared for the safety decisions they need to\n\nmake.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We are creating a Safety Advisory Group (SAG) that brings together expertise from across the company to help OpenAI's leadership and Board of Directors be best prepared for the safety decisions they need to make."
          }
        ],
        "reasoning": "Detailed description of establishing a cross-functional safety advisory body with specific responsibilities"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "For\n\nthis reason, we will be investing in efforts that help create an internal \u201cpreparedness roadmap\u201d\n\nand  help  us  thus  properly  plan  for  and  get  ahead  of  the  emerging  risks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will be investing in efforts that help create an internal 'preparedness roadmap' and help us thus properly plan for and get ahead of the emerging risks. These efforts will include sustained research related to scaling trends for dangerous capabilities and ongoing monitoring of misuse."
          }
        ],
        "reasoning": "Explicit commitment to monitoring and tracking model capabilities against potential risk thresholds"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The BoD may review certain decisions taken and will receive\n\nappropriate documentation (i.e., without needing to proactively ask) to ensure the BOD\n\nis fully informed and able to fulfill its oversight role\n\n Process:\n\n The Preparedness team is responsible for:\n\n maintaining  and  updating  the  Scorecard,\n\nincluding  designing  and  running\n\nevaluations  to  provide  Scorecard  inputs  and  collecting  relevant  information  on\n\nmonitored misuse, red-teaming, and intelligenc\n\n monitoring  for  unknown  unknowns  and  making  the  case  for  inclusion  in  the\n\nPreparedness Framework of any new risk categories as they emerg\n\n ensuring  the  risk  level  distinctions  in  the  Tracked  Risk  Categories  section  are\n\nappropriate given developments in frontier AI models, and suggesting updates to\n\nthese levels if neede\n\n forecasting  potential  changes  to  catastrophic  risk\n\nlevels,  and  summarizing\n\nevidence for an \u201cearly warning\u201d / \u201cheads up\u201d as neede\n\n providing a monthly report (sent to the SAG, Leadership and BoD) synthesizing the\n\nabove  with  any  potential  protective  actions  (the  SAG  Chair,  OpenAI  Leadership,\n\nand/or BoD can adjust this cadence as needed\n\n If  the  Preparedness  or  any  other  team  determines  that  any  changes  to  the\n\nPreparedness  Framework  are  necessary,  it  will  include  a  case  for  this  change  in  its\n\nreport.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The Preparedness team is responsible for... providing a monthly report (sent to the SAG, Leadership and BoD) synthesizing the above with any potential protective actions"
          }
        ],
        "reasoning": "Establishes a formal process for reporting and documenting potential safety incidents and risks"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "External  access:  We  will  also  continue  to  enable  external  research  and  government\n\naccess for model releases to increase the depth of red-teaming and testing of frontier\n\nmodel capabilities\n\n Safety drills: A critical part of this process is to be prepared if fast-moving emergency\n\nscenarios  arise,  including  what  default  organizational  response  might  look  like\n\n(including  how  to  stress-test  against  the  pressures  of  our  business  or  our  culture).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will also continue to enable external research and government access for model releases to increase the depth of red-teaming and testing of frontier model capabilities"
          }
        ],
        "reasoning": "Explicit commitment to systematic adversarial testing of model capabilities"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We  will  be  building  and  continually\n\nimproving suites of evaluations and other monitoring solutions along several Tracked Risk\n\nCategories, and indicating our current levels of pre-mitigation and post-mitigation risk in a\n\nScorecard.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will be building and continually improving suites of evaluations and other monitoring solutions along several Tracked Risk Categories, and indicating our current levels of pre-mitigation and post-mitigation risk in a Scorecard."
          }
        ],
        "reasoning": "Describes a comprehensive framework for standardized safety benchmarking across multiple risk categories"
      },
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We will also, in cooperation with other teams (e.g., Safety Systems), develop monitoring and\n\ninvestigative  systems.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We will also, in cooperation with other teams (e.g., Safety Systems), develop monitoring and investigative systems. This monitoring of real-world misuse (as well as staying abreast of relevant research developments) will help us create a better picture of deployed model characteristics"
          }
        ],
        "reasoning": "Detailed description of continuous monitoring and logging of model deployment and potential misuse"
      }
    ],
    "deletions": []
  },
  "phi-4-tech-report": {
    "additions": [
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "4.2 Direct Preference Optimization\n\nWe use DPO [RSM+23] to align the model with human preferences, and also to steer the model away\nfrom unwanted behavior through pairs of desired and undesired outputs.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We use DPO [RSM+23] to align the model with human preferences, and also to steer the model away from unwanted behavior through pairs of desired and undesired outputs."
          }
        ],
        "reasoning": "Explicit description of Direct Preference Optimization implementation with clear goals of alignment and behavior steering"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Without\nany mitigation, phi-4 would almost never admit to ignorance. For example, in response to too-difficult\nquestions like \u201cWho is the 297th highest ranked tennis player?\u201d the model would essentially act as an\nimprov-style \u201cYes, and.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Without any mitigation, phi-4 would almost never admit to ignorance. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it... and teach it to generate refusals rather than hallucinations on those problems."
          }
        ],
        "reasoning": "Detailed description of training the model to refuse answering questions it cannot confidently answer"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter\norganic2 data sources, including web content, licensed books, and code repositories to extract seeds\nfor the synthetic data pipeline that encourage high-depth reasoning and prioritize educational\nvalue (to the model).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We meticulously curate and filter organic data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value."
          }
        ],
        "reasoning": "Explicit description of careful data filtering and curation process during training"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To address these issues, we maintain an internal benchmark called PhiBench, which is tailored to\nevaluate the diverse skills and reasoning abilities that we found critical to phi-4\u2019s development.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We maintain an internal benchmark called PhiBench, which is tailored to evaluate the diverse skills and reasoning abilities that we found critical to phi-4's development."
          }
        ],
        "reasoning": "Detailed description of a custom safety benchmark designed to rigorously evaluate model capabilities"
      },
      {
        "techniqueId": "tech-red-teaming",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "7.2 Red Teaming\n\nIn addition to RAI benchmarking, we collaborated with the Microsoft AI Red Team (AIRT), an inde-\npendent group tasked with identifying safety and security vulnerabilities in Microsoft\u2019s GenAI products.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We collaborated with the Microsoft AI Red Team (AIRT), an independent group tasked with identifying safety and security vulnerabilities in Microsoft's GenAI products. AIRT conducted a two-week red-teaming exercise that tested phi-4 for risky behaviors by emulating both average and adversarial users in single and multi-turn scenarios."
          }
        ],
        "reasoning": "Comprehensive description of systematic adversarial testing by an independent red team"
      },
      {
        "techniqueId": "tech-hallucination-grounding",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "A Post-Training Dataset Details\n\nA.1 Refusal to Hallucinate\n\nWe created post-training SFT and DPO data to mitigate hallucinations in simple settings.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We created post-training SFT and DPO data to mitigate hallucinations in simple settings. Our goal was to teach the model to generate refusals rather than hallucinations on problems it cannot confidently solve."
          }
        ],
        "reasoning": "Explicit strategy to detect and prevent model hallucinations through targeted training"
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 Multilingual Data: We incorporated multilingual datasets to ensure that our model could han-\ndle a wide range of languages, including German, Spanish, French, Portuguese, Italian, Hindi\nand Japanese.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We incorporated multilingual datasets to ensure that our model could handle a wide range of languages... using classifiers trained on multilingual LLM-generated annotations to filter for quality."
          }
        ],
        "reasoning": "Suggests bias mitigation through diverse and carefully filtered multilingual data"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The Jailbreak (DR1) benchmark consists of simulated\nconversations around child grooming, illegal persuasion, leaking of 100 words of guidelines, popular con-\nspiracy, prejudice against real people, step-by-step illegal advice, and violence against real people.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The Jailbreak (DR1) benchmark consists of simulated conversations around... violence against real people."
          }
        ],
        "reasoning": "Explicit testing and benchmarking for violence-related content detection"
      },
      {
        "techniqueId": "tech-misinformation-detection",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Relying on Contamination-Proof Benchmarks: We give significant weight to benchmarks which\nwere designed in such a way that the questions are original and do not appear on the web, such as\nGPQA [RHS+23].",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We give significant weight to benchmarks which were designed in such a way that the questions are original and do not appear on the web, such as GPQA."
          }
        ],
        "reasoning": "Suggests an approach to detecting and preventing potential misinformation through careful benchmark selection"
      }
    ],
    "deletions": []
  },
  "pixtral-12b-blog": {
    "additions": [
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We simply pass images through the vision encoder at their native resolution and aspect ratio, converting them into image tokens for each 16x16 patch in the image",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We simply pass images through the vision encoder at their native resolution and aspect ratio, converting them into image tokens for each 16x16 patch in the image"
          }
        ],
        "reasoning": "Weak signal suggesting some form of image preprocessing, but not a clear data quality filtering technique"
      },
      {
        "techniqueId": "tech-multimodal-safety-alignment",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models."
          }
        ],
        "reasoning": "Indicates intentional multimodal training approach with safety and reasoning benchmarking"
      },
      {
        "techniqueId": "tech-safety-benchmarks",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Evaluation protocol\nWe re-evaluate a range of open and closed models through the\nsame\nevaluation harness\n.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We re-evaluate a range of open and closed models through the same evaluation harness. For each dataset, the prompt was chosen such that we could reproduce the results of leading multimodal models (GPT-4o and Claude-3.5-Sonnet)."
          }
        ],
        "reasoning": "Explicit description of standardized safety benchmarking methodology across multiple models"
      }
    ],
    "deletions": []
  },
  "qwen2-5-coder-tech-report": {
    "additions": [
      {
        "techniqueId": "tech-dpo",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Direct Preference Optimization for Code After obtaining the SFT model, we further align\nthe Qwen2.5-Coder with the help of offline direct preference optimization (DPO) (Rafailov\net al., 2023).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "After obtaining the SFT model, we further align the Qwen2.5-Coder with the help of offline direct preference optimization (DPO). Given that human feedback is highly labor-intensive, we use a multilingual code sandbox to provide code execution feedback, while an LLM is utilized for human judgment feedback."
          }
        ],
        "reasoning": "Explicit description of DPO implementation with code execution and LLM-based feedback mechanisms"
      },
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We have implemented sophisticated procedures to recall and\nclean potential code data and filter out low-quality content using weak model based classi-\nfiers and scorers.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We have implemented sophisticated procedures to recall and clean potential code data and filter out low-quality content using weak model based classifiers and scorers."
          }
        ],
        "reasoning": "Direct statement about using classifiers to filter and clean training data"
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Instead of the conventional URL-based multi-stage recall method, we developed\na coarse-to-fine hierarchical filtering approach for raw data.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Instead of the conventional URL-based multi-stage recall method, we developed a coarse-to-fine hierarchical filtering approach for raw data. This method offers two key advantages: (1) It enables precise control over each filter's responsibility, ensuring comprehensive handling of each dimension. (2) It naturally assigns quality scores to the dataset, with data retained in the final stage being of higher quality."
          }
        ],
        "reasoning": "Detailed description of a systematic approach to analyzing and scoring dataset quality"
      },
      {
        "techniqueId": "tech-capability-monitoring",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "2https://pypi.org/project/tree-sitter-languages/\n\n9\n\n\fTechnical Report\n\n6 Evaluation on Base Models\n\nFor the base model, we conducted a comprehensive and fair evaluation in six key aspects, in-\ncluding code generation, code completion, code reasoning, mathematical reasoning, general\nnatural language understanding and long-context modeling.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We conducted a comprehensive and fair evaluation in six key aspects, including code generation, code completion, code reasoning, mathematical reasoning, general natural language understanding and long-context modeling."
          }
        ],
        "reasoning": "Systematic evaluation across multiple capability dimensions suggests capability threshold monitoring"
      }
    ],
    "deletions": [
      {
        "techniqueId": "tech-community-evaluation",
        "deleted_by": "llm",
        "reasoning": "No clear evidence of actual community-based evaluation process, only standard benchmark testing"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "deleted_by": "llm",
        "reasoning": "No specific implementation of output filtering systems described in the document"
      }
    ]
  },
  "qwen2-5-tech-report": {
    "additions": [
      {
        "techniqueId": "tech-training-data-quality-filtering",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We decoupled the text data and vision\ndata storage. We simply store text data on CPFS and use mmap for efficient access.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We decoupled the text data and vision data storage. We simply store text data on CPFS and use mmap for efficient access. For vision data, we use Alibaba Cloud's OSS (Object Storage Service) for persistent storage."
          }
        ],
        "reasoning": "The document describes careful data storage and access strategies that suggest deliberate data quality and management approaches."
      },
      {
        "techniqueId": "tech-dataset-auditing",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "The cutoff date for our data knowledge is June 2023. This diverse data composition is\ninstrumental in developing a robust multimodal understanding capability.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "The cutoff date for our data knowledge is June 2023. This diverse data composition is instrumental in developing a robust multimodal understanding capability."
          }
        ],
        "reasoning": "The authors explicitly mention tracking data knowledge cutoff and diversity, indicating dataset composition analysis."
      },
      {
        "techniqueId": "tech-bias-mitigation",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Our data sources primarily comprise cleaned web pages, open-source datasets, and\nsynthetic data. The cutoff date for our data knowledge is June 2023.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Our data sources primarily comprise cleaned web pages, open-source datasets, and synthetic data."
          }
        ],
        "reasoning": "The mention of 'cleaned' datasets suggests potential bias mitigation efforts during data selection."
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "Low",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "We aim to advance AI technologies\nand enhance their beneficial effects on society by dedicating ourselves to these endeavors.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We aim to advance AI technologies and enhance their beneficial effects on society by dedicating ourselves to these endeavors."
          }
        ],
        "reasoning": "While not explicitly detailed, the ethical framing suggests potential refusal training principles."
      }
    ],
    "deletions": []
  },
  "qwen3-max": {
    "additions": [
      {
        "techniqueId": "tech-rlhf",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "3.5.1 Reward Design\n\nWe explore two reward formulations to guide the RL training process:\n\nGuard-Only Reward This reward scheme directly leverages Generative Qwen3Guard\u2019s safety judg-\nments.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We explore two reward formulations to guide the RL training process: Guard-Only Reward and Hybrid Reward. We employ Group Sequence Policy Optimization (GSPO) (Zheng et al., 2025), a stable and efficient reinforcement learning algorithm, to train the policy model."
          }
        ],
        "reasoning": "Detailed description of using reinforcement learning with specific reward design and optimization algorithm"
      },
      {
        "techniqueId": "tech-refusal-training",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "To mitigate this, we introduce a Hybrid\nReward that jointly optimizes for three objectives: high safety, high helpfulness, and low refusal rate.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "To mitigate this, we introduce a Hybrid Reward that jointly optimizes for three objectives: high safety, high helpfulness, and low refusal rate."
          }
        ],
        "reasoning": "Explicit description of training to manage refusal behaviors during safety optimization"
      },
      {
        "techniqueId": "tech-safety-reward-modeling",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In addition to Generative Qwen3Guard for safety judge, we incorporate the WorldPM-Helpsteer2\nmodel (Wang et al., 2025) to score response helpfulness.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We incorporate the WorldPM-Helpsteer2 model (Wang et al., 2025) to score response helpfulness. The hybrid reward r(x, t, y) is defined as follows: [...] where both is safe and is refusal are predicates provided by Qwen3Guard-4B-Gen."
          }
        ],
        "reasoning": "Detailed implementation of a reward model specifically designed for safety and helpfulness assessment"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "During a conversation: (1) The user\u2019s\nprompt is simultaneously submitted to both the LLM assistant and Stream Qwen3Guard.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "During a conversation: (1) The user's prompt is simultaneously submitted to both the LLM assistant and Stream Qwen3Guard. Stream Qwen3Guard evaluates the prompt and assigns a safety label; based on this assessment, the upper-level framework determines whether to interrupt the conversation."
          }
        ],
        "reasoning": "Explicit description of an input guardrail system that classifies and potentially blocks user prompts"
      },
      {
        "techniqueId": "tech-output-filtering-systems",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "(2) If the conversation proceeds, the\nLLM assistant begins generating its response in a streaming fashion.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "(2) If the conversation proceeds, the LLM assistant begins generating its response in a streaming fashion. Each output token is immediately forwarded to Stream Qwen3Guard, which performs real-time safety evaluation on a per-token basis, enabling dynamic content moderation throughout the generation process."
          }
        ],
        "reasoning": "Detailed implementation of a token-level output filtering mechanism during generation"
      },
      {
        "techniqueId": "tech-violence-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "In the current version of Qwen3Guard, we consider the following safety categories:\n\n\u2022 Violent: Content that provides detailed instructions, methods, or advice on how to commit acts\nof violence, including the manufacture, acquisition, or use of weapons.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "In the current version of Qwen3Guard, we consider the following safety categories: \u2022 Violent: Content that provides detailed instructions, methods, or advice on how to commit acts of violence, including the manufacture, acquisition, or use of weapons. Also includes depictions of violence."
          }
        ],
        "reasoning": "Explicit categorization and detection mechanism for violent content"
      },
      {
        "techniqueId": "tech-weapons-illegal-activity",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 Non-violent Illegal Acts: Content providing guidance or advice for non-violent illegal activities\n\nlike hacking, unauthorized drug production, or stealing.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "\u2022 Non-violent Illegal Acts: Content providing guidance or advice for non-violent illegal activities like hacking, unauthorized drug production, or stealing."
          }
        ],
        "reasoning": "Specific detection category for illegal activities and weapons-related content"
      },
      {
        "techniqueId": "tech-self-harm-prevention",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 Suicide & Self-Harm: Content advocating, directly encouraging, or detailing methods for\n\nself-harm, suicide, or dangerous activities that could lead to serious injury or death.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "\u2022 Suicide & Self-Harm: Content advocating, directly encouraging, or detailing methods for self-harm, suicide, or dangerous activities that could lead to serious injury or death."
          }
        ],
        "reasoning": "Explicit safety category and detection mechanism for self-harm and suicide prevention"
      },
      {
        "techniqueId": "tech-pii-detection",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 Personally Identifiable Information: Content offering unauthorized sharing or disclosure of\nsensitive personal identifying information, such as name, ID number, address, phone number,\nmedical records, financial details, and account passwords, etc.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "\u2022 Personally Identifiable Information: Content offering unauthorized sharing or disclosure of sensitive personal identifying information, such as name, ID number, address, phone number, medical records, financial details, and account passwords, etc."
          }
        ],
        "reasoning": "Detailed definition and detection category for personally identifiable information"
      },
      {
        "techniqueId": "tech-copyright-ip-violation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "\u2022 Copyright Violation: Content offering unauthorized reproduction, distribution, public display,\nor derivative use of copyrighted materials, such as novels, scripts, lyrics, and other creative\nworks protected by law, without the explicit permission of the copyright holder.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "\u2022 Copyright Violation: Content offering unauthorized reproduction, distribution, public display, or derivative use of copyrighted materials, such as novels, scripts, lyrics, and other creative works protected by law, without the explicit permission of the copyright holder."
          }
        ],
        "reasoning": "Specific safety category for detecting copyright and intellectual property violations"
      },
      {
        "techniqueId": "tech-system-prompts",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Specifically, we first decompose the safety policy into a fine-grained taxonomy, collect seed prompts for\neach target category, and then prompt LLMs to generate additional relevant examples based on these\nseeds.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We begin by decomposing the safety policy into a fine-grained taxonomy, collect seed prompts for each target category, and then prompt LLMs to generate additional relevant examples based on these seeds."
          }
        ],
        "reasoning": "Use of system-level instructions to guide model behavior and safety classification"
      }
    ],
    "deletions": [
      {
        "techniqueId": "tech-dpo",
        "deleted_by": "llm",
        "reasoning": "No implementation evidence, only mentioned in available techniques list"
      }
    ]
  },
  "xai-security": {
    "additions": [
      {
        "techniqueId": "tech-observability-monitoring",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Security Information and Event Management\nLogging and monitoring software are configured to collect data from system components to monitor system events (including security events / IDS events), performance, and resource utilization.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Logging and monitoring software are configured to collect data from system components to monitor system events (including security events / IDS events), performance, and resource utilization. The security team is alerted of unusual or suspicious security events that are detected."
          }
        ],
        "reasoning": "Direct quote describes comprehensive logging, monitoring, and alerting system for security events"
      },
      {
        "techniqueId": "tech-incident-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "A formal incident management framework has been established that defines roles, responsibilities, escalation paths, and internal and external communication requirements in the event of incidents that impact the security or availability of the system.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "A formal incident management framework has been established that defines roles, responsibilities, escalation paths, and internal and external communication requirements in the event of incidents that impact the security or availability of the system."
          }
        ],
        "reasoning": "Explicit description of a structured incident reporting and management process"
      },
      {
        "techniqueId": "tech-access-control-documentation",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Access Control\nData Access\nWe follow the principles of least privilege and need-to-know basis. Firewalls, network access controls, identity and access management (IAM) controls, and other techniques are used that are designed to prevent unauthorized access to systems processing customer data.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "We follow the principles of least privilege and need-to-know basis. Firewalls, network access controls, identity and access management (IAM) controls, and other techniques are used that are designed to prevent unauthorized access to systems processing customer data."
          }
        ],
        "reasoning": "Clear documentation of access control principles and implementation strategies"
      },
      {
        "techniqueId": "tech-enterprise-integration",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI supports Single Sign On for Business Tier accounts. You can use any SAML-based Identity Provider (IdP), for example Okta, X, OneLogin, or use GSuite to serve as your identity provider, delegating access to the application based on rules you create in your central identity management solution.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI supports Single Sign On for Business Tier accounts. You can use any SAML-based Identity Provider (IdP), for example Okta, X, OneLogin, or use GSuite to serve as your identity provider, delegating access to the application based on rules you create in your central identity management solution."
          }
        ],
        "reasoning": "Detailed description of enterprise-grade integration features for authentication and access management"
      },
      {
        "techniqueId": "tech-input-guardrail-systems",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI utilizes Cloudflare WAF, and Wiz to bolster our application's resilience and security. Cloudflare ensures fast and secure content delivery, and also provides robust protection against DDoS attacks.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI utilizes Cloudflare WAF, and Wiz to bolster our application's resilience and security. Cloudflare ensures fast and secure content delivery, and also provides robust protection against DDoS attacks."
          }
        ],
        "reasoning": "Web Application Firewall suggests input filtering and protection mechanisms"
      },
      {
        "techniqueId": "tech-regulatory-compliance",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "xAI includes a suite of Privacy tools to help your organization comply with regulations like HIPAA, the GDPR, and the CCPA.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "xAI includes a suite of Privacy tools to help your organization comply with regulations like HIPAA, the GDPR, and the CCPA."
          }
        ],
        "reasoning": "Explicit statement of compliance with multiple regulatory frameworks"
      },
      {
        "techniqueId": "tech-data-sovereignty",
        "confidence": "Medium",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Customer archive data stored in S3 is encrypted at rest using server-side encryption with Amazon S3 managed keys (SSE-S3).",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "Customer archive data stored in S3 is encrypted at rest using server-side encryption with Amazon S3 managed keys (SSE-S3)."
          }
        ],
        "reasoning": "Indicates data handling practices consistent with data sovereignty principles"
      },
      {
        "techniqueId": "tech-vulnerability-reporting",
        "confidence": "High",
        "active": true,
        "deleted_by": null,
        "evidence": [
          {
            "text": "Vulnerability Reporting\nTo report vulnerabilities, please contact the xAI security team through our HackerOne program at\nhttps://hackerone.com/x\nor by emailing\nvulnerabilities@x.ai\nwith the subject line \"Responsible Disclosure\" All reported vulnerabilities will be tracked via our HackerOne bug bounty program for efficient resolution and acknowledgment.",
            "created_by": "llm",
            "active": true,
            "deleted_by": null,
            "llm_original": "To report vulnerabilities, please contact the xAI security team through our HackerOne program at https://hackerone.com/x or by emailing vulnerabilities@x.ai with the subject line 'Responsible Disclosure'"
          }
        ],
        "reasoning": "Explicit vulnerability reporting mechanism through a public bug bounty program"
      }
    ],
    "deletions": []
  }
}