[
  {
    "id": "tech-rlhf",
    "name": "Reinforcement Learning from Human Feedback",
    "categoryId": "cat-alignment",
    "description": "Training models to align with human preferences through feedback",
    "nlu_profile": {
      "primary_concept": "Optimization using a reward model trained on human preference data, typically via PPO.",
      "semantic_anchors": [
        "RLHF",
        "PPO",
        "human feedback",
        "reward model training",
        "preference learning",
        "Proximal Policy Optimization",
        "InstructGPT",
        "preference pair",
        "comparison data",
        "reward signal",
        "preference training",
        "human preference alignment",
        "reward model optimization",
        "preference-based training",
        "human preference data",
        "RL fine-tuning",
        "preference-based RL",
        "human feedback training",
        "preference model"
      ],
      "entailment_hypothesis": "The model utilizes reinforcement learning from human feedback (RLHF)."
    }
  },
  {
    "id": "tech-constitutional-ai",
    "name": "Constitutional AI / Self-Critique",
    "categoryId": "cat-alignment",
    "description": "Training models to critique and revise their own outputs based on constitutional principles",
    "nlu_profile": {
      "primary_concept": "Method where the model critiques and revises outputs based on high-level principles.",
      "semantic_anchors": [
        "Constitutional AI",
        "RLAIF",
        "self-critique",
        "principle-driven training",
        "CAI",
        "constitutional principles",
        "self-revision",
        "AI feedback",
        "critique-revise loop",
        "harmlessness principles",
        "RL from AI feedback",
        "critiques its own output",
        "principle-based training",
        "constitutional training",
        "self-critique training",
        "principle-guided learning",
        "constitutional approach",
        "AI constitution",
        "principle-based alignment",
        "rule-based self-improvement"
      ],
      "entailment_hypothesis": "The model uses Constitutional AI or a self-critique mechanism."
    }
  },
  {
    "id": "tech-adversarial-training",
    "name": "Adversarial Training",
    "categoryId": "cat-alignment",
    "description": "Training with adversarial examples to improve robustness",
    "nlu_profile": {
      "primary_concept": "Augmenting training data with adversarial examples to improve robustness.",
      "semantic_anchors": [
        "adversarial training",
        "robustness training",
        "attack-defense training",
        "adversarial examples",
        "perturbation training",
        "adversarial fine-tuning",
        "robustness fine-tuning",
        "attack augmentation"
      ],
      "entailment_hypothesis": "The training process included adversarial examples to improve robustness."
    }
  },
  {
    "id": "tech-bias-detection-training",
    "name": "Dataset Auditing & Representation Analysis",
    "categoryId": "cat-pre-training-safety",
    "description": "Detection and mitigation of demographic and cultural biases in training data",
    "nlu_profile": {
      "primary_concept": "Statistical analysis and filtering of training datasets to mitigate representational imbalances.",
      "semantic_anchors": [
        "dataset auditing",
        "bias analysis",
        "representation measurement",
        "demographic filtering"
      ],
      "entailment_hypothesis": "The training data underwent auditing to mitigate bias."
    }
  },
  {
    "id": "tech-bias-mitigation",
    "name": "Bias Mitigation (Post-Training)",
    "categoryId": "cat-alignment",
    "description": "Algorithmic interventions during fine-tuning to reduce output bias",
    "nlu_profile": {
      "primary_concept": "Techniques applied during fine-tuning or inference to reduce stereotypical biases.",
      "semantic_anchors": [
        "fairness tuning",
        "de-biasing",
        "bias reduction",
        "counterfactual data augmentation",
        "demographic parity",
        "equalized odds",
        "fairness constraint",
        "representation balance",
        "bias audit",
        "stereotype mitigation",
        "demographic fairness"
      ],
      "entailment_hypothesis": "The model underwent specific fine-tuning to mitigate bias."
    }
  },
  {
    "id": "tech-copyright-filtering",
    "name": "Copyright Content Filtering",
    "categoryId": "cat-pre-training-safety",
    "description": "Detection and removal of copyrighted content from training datasets",
    "nlu_profile": {
      "primary_concept": "Removal of copyrighted or licensed content from training datasets.",
      "semantic_anchors": [
        "copyright filtering",
        "deduplication",
        "licensed content removal",
        "IP protection"
      ],
      "entailment_hypothesis": "The training data was filtered to remove copyrighted material."
    }
  },
  {
    "id": "tech-system-prompts",
    "name": "System Prompts / Metaprompts",
    "categoryId": "cat-runtime-safety",
    "description": "Instructions provided at the system level to enforce behavior and safety",
    "nlu_profile": {
      "primary_concept": "Pre-pended instructional context or 'system message' that defines behavioral boundaries.",
      "semantic_anchors": [
        "system prompt",
        "metaprompt",
        "system message",
        "hidden context",
        "developer instructions",
        "system instruction",
        "pre-prompt",
        "instruction tuning context",
        "behavioral guidelines",
        "operator instructions",
        "preamble",
        "system-level instruction",
        "foundational prompt",
        "base instruction",
        "system-level prompt",
        "constitutional prompt",
        "system context",
        "pre-configured prompt",
        "default instructions",
        "system directive"
      ],
      "entailment_hypothesis": "The system uses a hidden system prompt to enforce safety constraints."
    }
  },
  {
    "id": "tech-input-guardrails",
    "name": "Input Guardrails",
    "categoryId": "cat-runtime-safety",
    "description": "Classification and blocking of input prompts for safety risks",
    "nlu_profile": {
      "primary_concept": "Filtering layer that inspects user input for toxicity or attacks before processing.",
      "semantic_anchors": [
        "input filtering",
        "prompt classification",
        "Llama Guard",
        "NeMo Guardrails",
        "input moderation",
        "input classifier",
        "prompt filter",
        "request classification",
        "pre-generation filter",
        "input safety classifier",
        "ShieldGemma",
        "Aegis Guard"
      ],
      "entailment_hypothesis": "The system employs an input classification layer to block unsafe prompts."
    }
  },
  {
    "id": "tech-output-filtering",
    "name": "Output Content Filtering",
    "categoryId": "cat-runtime-safety",
    "description": "Post-generation filtering of model outputs for safety violations",
    "nlu_profile": {
      "primary_concept": "Post-generation filtering layer that blocks model responses if they violate policies.",
      "semantic_anchors": [
        "output moderation",
        "response filtering",
        "toxicity detection",
        "completion blocking",
        "post-generation filter",
        "output classifier",
        "response moderation",
        "generation blocking",
        "safety filter",
        "content moderation API"
      ],
      "entailment_hypothesis": "The system filters or blocks generated model outputs that violate safety policies."
    }
  },
  {
    "id": "tech-rag-guardrails",
    "name": "RAG Guardrails",
    "categoryId": "cat-runtime-safety",
    "description": "Checks for grounding and citation accuracy in RAG systems",
    "nlu_profile": {
      "primary_concept": "Verification mechanisms in retrieval systems ensuring outputs are supported by context.",
      "semantic_anchors": [
        "hallucination detection",
        "grounding check",
        "citation verification",
        "context adherence"
      ],
      "entailment_hypothesis": "The system uses grounding checks to ensure RAG accuracy."
    }
  },
  {
    "id": "tech-hallucination-grounding",
    "name": "Hallucination Detection & Grounding",
    "categoryId": "cat-runtime-safety",
    "description": "General mechanisms to verify factuality and reduce fabrication",
    "nlu_profile": {
      "primary_concept": "Techniques to detect or prevent fabricated information in model outputs.",
      "semantic_anchors": [
        "factuality check",
        "hallucination mitigation",
        "citation enforcement",
        "grounding metric",
        "factual grounding",
        "source attribution",
        "claim verification",
        "confabulation",
        "unfaithful generation",
        "factuality evaluation",
        "attribution"
      ],
      "entailment_hypothesis": "The model uses mechanisms to detect or reduce hallucinations."
    }
  },
  {
    "id": "tech-circuit-breakers",
    "name": "Circuit Breakers / Kill Switches",
    "categoryId": "cat-runtime-safety",
    "description": "Mechanisms to immediately disconnect system if dangerous actions occur",
    "nlu_profile": {
      "primary_concept": "Emergency intervention mechanisms that automatically halt model execution.",
      "semantic_anchors": [
        "circuit breaker",
        "kill switch",
        "emergency stop",
        "automatic shutdown"
      ],
      "entailment_hypothesis": "The system has a kill switch or circuit breaker."
    }
  },
  {
    "id": "tech-watermarking",
    "name": "Provenance & Watermarking",
    "categoryId": "cat-runtime-safety",
    "description": "Embedding detectable patterns in AI-generated content",
    "nlu_profile": {
      "primary_concept": "Techniques to embed imperceptible signals or sign metadata to prove AI origin.",
      "semantic_anchors": [
        "watermarking",
        "SynthID",
        "C2PA",
        "provenance",
        "digital signature",
        "AI-generated content detection",
        "synthetic content marking",
        "content authenticity",
        "AI watermark",
        "generation fingerprint",
        "metadata injection"
      ],
      "entailment_hypothesis": "The system applies watermarking or provenance metadata."
    }
  },
  {
    "id": "tech-code-execution-sandboxing",
    "name": "Code Execution Sandboxing",
    "categoryId": "cat-runtime-safety",
    "description": "Isolating code generated or executed by the LLM",
    "nlu_profile": {
      "primary_concept": "Secure, isolated environments for executing model-generated code.",
      "semantic_anchors": [
        "sandbox",
        "secure execution environment",
        "code interpreter isolation",
        "containerization"
      ],
      "entailment_hypothesis": "Code execution is confined to a secure sandbox environment."
    }
  },
  {
    "id": "tech-red-teaming",
    "name": "Red Team Exercises",
    "categoryId": "cat-evaluation",
    "description": "Systematic adversarial testing by security experts",
    "nlu_profile": {
      "primary_concept": "Adversarial testing where experts attempt to bypass safety filters.",
      "semantic_anchors": [
        "red teaming",
        "adversarial testing",
        "penetration testing",
        "attack simulation",
        "red team",
        "manual red teaming",
        "human red teaming",
        "safety testing",
        "expert adversarial testing",
        "jailbreak testing",
        "security testing",
        "adversarial evaluation",
        "security assessment",
        "attack testing",
        "vulnerability testing",
        "stress testing",
        "adversarial probing",
        "safety testing by experts",
        "manual adversarial testing",
        "external red team"
      ],
      "entailment_hypothesis": "The model underwent adversarial red teaming."
    }
  },
  {
    "id": "tech-automated-red-teaming",
    "name": "Automated Red Teaming",
    "categoryId": "cat-evaluation",
    "description": "Using attacker models to generate adversarial prompts at scale",
    "nlu_profile": {
      "primary_concept": "Scalable testing using an 'attacker' AI model to generate jailbreaks.",
      "semantic_anchors": [
        "LLM-as-a-Judge",
        "automated adversarial testing",
        "model-based red teaming",
        "automated attacks",
        "automatic red teaming",
        "AI red teaming",
        "synthetic adversarial",
        "adversarial prompt generation",
        "attack generation",
        "automated jailbreak"
      ],
      "entailment_hypothesis": "The safety evaluation involved automated adversarial attacks."
    }
  },
  {
    "id": "tech-safety-benchmarks",
    "name": "Safety Benchmarking",
    "categoryId": "cat-evaluation",
    "description": "Standardized benchmarks for measuring safety performance",
    "nlu_profile": {
      "primary_concept": "Evaluation using standardized datasets of harmful prompts.",
      "semantic_anchors": [
        "safety benchmarks",
        "TruthfulQA",
        "RealToxicityPrompts",
        "Bold",
        "safety suite",
        "HarmBench",
        "AdvBench",
        "Do-Not-Answer",
        "safety evaluation",
        "harmful prompt dataset",
        "toxicity benchmark",
        "safety metrics",
        "BBQ",
        "WinoBias",
        "safety testing",
        "benchmark evaluation",
        "evaluation suite",
        "safety assessment benchmarks",
        "risk evaluation",
        "benchmark testing",
        "safety measurement"
      ],
      "entailment_hypothesis": "The model was evaluated against standardized safety benchmarks."
    }
  },
  {
    "id": "tech-transparency-artifacts",
    "name": "Transparency Artifacts",
    "categoryId": "cat-transparency",
    "description": "Standardized disclosure of model capabilities and limitations",
    "nlu_profile": {
      "primary_concept": "Formal documentation artifacts detailing creation and safety evaluations.",
      "semantic_anchors": [
        "Model Card",
        "System Card",
        "Transparency Note",
        "Safety Report",
        "Datasheet",
        "technical report",
        "safety documentation",
        "model documentation",
        "capability card",
        "AI service card",
        "responsible AI documentation",
        "technical disclosure",
        "system documentation",
        "transparency report",
        "technical documentation",
        "disclosure document",
        "accountability documentation",
        "model transparency",
        "safety card"
      ],
      "entailment_hypothesis": "The release includes a Model Card or System Card."
    }
  },
  {
    "id": "tech-observability-monitoring",
    "name": "Observability & Monitoring",
    "categoryId": "cat-runtime-safety",
    "description": "Real-time analysis of inputs/outputs and risk assessment",
    "nlu_profile": {
      "primary_concept": "Continuous runtime tracking of system inputs and outputs.",
      "semantic_anchors": [
        "audit logging",
        "runtime monitoring",
        "drift detection",
        "violation logging"
      ],
      "entailment_hypothesis": "The deployment includes real-time monitoring or logging."
    }
  },
  {
    "id": "tech-stakeholder-community-engagement",
    "name": "Stakeholder Engagement",
    "categoryId": "cat-governance",
    "description": "Processes for collecting feedback from users and communities",
    "nlu_profile": {
      "primary_concept": "Structured processes to incorporate feedback from external stakeholders.",
      "semantic_anchors": [
        "participatory design",
        "community consultation",
        "stakeholder feedback",
        "civil society"
      ],
      "entailment_hypothesis": "The process involved engagement with external stakeholders."
    }
  },
  {
    "id": "tech-machine-unlearning",
    "name": "Machine Unlearning",
    "categoryId": "cat-privacy-security",
    "description": "Algorithms to 'forget' specific training data without retraining",
    "nlu_profile": {
      "primary_concept": "Interventions to remove influence of specific data points from model weights.",
      "semantic_anchors": [
        "unlearning",
        "model editing",
        "knowledge removal",
        "targeted forgetting"
      ],
      "entailment_hypothesis": "The model underwent machine unlearning or targeted forgetting."
    }
  },
  {
    "id": "tech-pii-detection-inference",
    "name": "PII Detection & Redaction",
    "categoryId": "cat-privacy-security",
    "description": "Detection and redaction of personal information",
    "nlu_profile": {
      "primary_concept": "Identification and obfuscation of Personally Identifiable Information.",
      "semantic_anchors": [
        "PII redaction",
        "anonymization",
        "data masking",
        "scrubbing",
        "personally identifiable information",
        "data anonymization",
        "privacy filtering",
        "named entity redaction",
        "de-identification",
        "sensitive data detection"
      ],
      "entailment_hypothesis": "The system detects and redacts personally identifiable information."
    }
  },
  {
    "id": "tech-sovereignty-options",
    "name": "Data Sovereignty Controls",
    "categoryId": "cat-governance",
    "description": "Controls ensuring data processing complies with regional requirements",
    "nlu_profile": {
      "primary_concept": "Mechanisms ensuring data processing occurs within specific jurisdictions.",
      "semantic_anchors": [
        "data residency",
        "sovereignty controls",
        "geo-fencing",
        "local processing"
      ],
      "entailment_hypothesis": "The system enforces data sovereignty or residency controls."
    }
  },
  {
    "id": "tech-data-retention-policies",
    "name": "Data Retention Policies",
    "categoryId": "cat-privacy-security",
    "description": "Policies for data retention and lifecycle management",
    "nlu_profile": {
      "primary_concept": "Defined lifecycle management policies for storing and deleting data.",
      "semantic_anchors": [
        "retention policy",
        "data deletion",
        "minimization",
        "logs storage limit"
      ],
      "entailment_hypothesis": "The system adheres to a data retention or deletion policy."
    }
  },
  {
    "id": "tech-government-oversight",
    "name": "Regulatory Compliance",
    "categoryId": "cat-governance",
    "description": "Compliance with government regulatory requirements",
    "nlu_profile": {
      "primary_concept": "Adherence to legal frameworks and reporting requirements.",
      "semantic_anchors": [
        "regulatory compliance",
        "EU AI Act",
        "NIST AI RMF",
        "compliance audit"
      ],
      "entailment_hypothesis": "The system complies with government regulations."
    }
  },
  {
    "id": "tech-safety-advisory",
    "name": "Independent Safety Advisory",
    "categoryId": "cat-governance",
    "description": "External advisory board for safety oversight",
    "nlu_profile": {
      "primary_concept": "External advisory board providing non-binding safety guidance.",
      "semantic_anchors": [
        "safety board",
        "advisory committee",
        "ethics board",
        "independent oversight"
      ],
      "entailment_hypothesis": "An independent safety advisory board provides oversight."
    }
  },
  {
    "id": "tech-incident-reporting",
    "name": "Incident Reporting Systems",
    "categoryId": "cat-governance",
    "description": "Systems for reporting safety incidents",
    "nlu_profile": {
      "primary_concept": "Processes for tracking and disclosing safety failures.",
      "semantic_anchors": [
        "incident reporting",
        "bug bounty",
        "vulnerability disclosure",
        "failure tracking",
        "safety incident",
        "violation reporting",
        "abuse reporting",
        "feedback mechanism",
        "report abuse",
        "trust and safety",
        "T&S"
      ],
      "entailment_hypothesis": "A mechanism exists for reporting and tracking safety incidents."
    }
  },
  {
    "id": "tech-responsible-release",
    "name": "Responsible Release Protocols",
    "categoryId": "cat-governance",
    "description": "Structured processes for staged release",
    "nlu_profile": {
      "primary_concept": "Staged deployment strategies to monitor safety before full release.",
      "semantic_anchors": [
        "staged release",
        "phased deployment",
        "controlled rollout",
        "gradual release",
        "limited release",
        "alpha release",
        "beta release",
        "early access",
        "preview release",
        "staged deployment",
        "incremental rollout"
      ],
      "entailment_hypothesis": "The model was released using a responsible release protocol."
    }
  },
  {
    "id": "tech-community-governance",
    "name": "Community Governance Models",
    "categoryId": "cat-governance",
    "description": "Governance involving community participation",
    "nlu_profile": {
      "primary_concept": "Decision-making structures where the community votes on policy.",
      "semantic_anchors": [
        "participatory governance",
        "DAO",
        "community voting",
        "user council"
      ],
      "entailment_hypothesis": "The system utilizes a community governance model."
    }
  },
  {
    "id": "tech-contextual-safety",
    "name": "Contextual Safety Assessment",
    "categoryId": "cat-runtime-safety",
    "description": "Context-aware safety evaluation",
    "nlu_profile": {
      "primary_concept": "Safety mechanisms that adapt behavior based on user context.",
      "semantic_anchors": [
        "adaptive safety",
        "context-aware filtering",
        "personalized safety",
        "risk assessment",
        "contextual harm",
        "use-case specific safety",
        "deployment context",
        "application-specific",
        "user context",
        "situational safety"
      ],
      "entailment_hypothesis": "Safety mechanisms adapt based on user context."
    }
  },
  {
    "id": "tech-realtime-fact-checking",
    "name": "Real-time Fact Checking",
    "categoryId": "cat-runtime-safety",
    "description": "Dynamic verification of information accuracy",
    "nlu_profile": {
      "primary_concept": "Dynamic verification of claims against external knowledge bases.",
      "semantic_anchors": [
        "fact checking",
        "claim verification",
        "truthfulness check",
        "google grounding"
      ],
      "entailment_hypothesis": "The system performs real-time fact checking."
    }
  },
  {
    "id": "tech-configurable-policies",
    "name": "Configurable Safety Policies",
    "categoryId": "cat-runtime-safety",
    "description": "User or admin configurable safety thresholds",
    "nlu_profile": {
      "primary_concept": "Allowing users to adjust safety thresholds for their use case.",
      "semantic_anchors": [
        "custom safety settings",
        "adjustable filters",
        "configurable moderation",
        "safety mode",
        "strictness level",
        "moderation level",
        "harm category toggle",
        "safety threshold",
        "content filter settings"
      ],
      "entailment_hypothesis": "Users can configure or customize the safety policies."
    }
  },
  {
    "id": "tech-multistage-pipeline",
    "name": "Multi-stage Safety Pipeline",
    "categoryId": "cat-runtime-safety",
    "description": "Multiple layers of safety checks",
    "nlu_profile": {
      "primary_concept": "Defense-in-depth architecture using multiple safety checks.",
      "semantic_anchors": [
        "defense in depth",
        "cascaded checks",
        "layered safety",
        "pipeline architecture"
      ],
      "entailment_hypothesis": "The system employs a multi-stage safety pipeline."
    }
  },
  {
    "id": "tech-prompt-injection-protection",
    "name": "Jailbreak & Injection Defense",
    "categoryId": "cat-runtime-safety",
    "description": "Detection and prevention of prompt injection",
    "nlu_profile": {
      "primary_concept": "Detection of adversarial attempts to hijack the model.",
      "semantic_anchors": [
        "prompt injection",
        "jailbreak defense",
        "indirect injection",
        "DAN mode",
        "jailbreak",
        "prompt attack",
        "adversarial prompt",
        "instruction injection",
        "system prompt extraction",
        "prompt leaking",
        "goal hijacking",
        "context manipulation"
      ],
      "entailment_hypothesis": "The system includes defenses against prompt injection."
    }
  },
  {
    "id": "tech-csam-detection",
    "name": "CSAM Detection & Removal",
    "categoryId": "cat-pre-training-safety",
    "description": "Detection and removal of child sexual abuse material",
    "nlu_profile": {
      "primary_concept": "Automated hashing and matching against CSAM databases.",
      "semantic_anchors": [
        "CSAM",
        "hash matching",
        "PhotoDNA",
        "NCMEC",
        "child safety",
        "child sexual abuse material",
        "minor safety",
        "child exploitation",
        "CSAM hash",
        "perceptual hashing"
      ],
      "entailment_hypothesis": "The system detects and blocks CSAM."
    }
  },
  {
    "id": "tech-training-data-filtering",
    "name": "Training Data Filtering",
    "categoryId": "cat-pre-training-safety",
    "description": "Removal of harmful content from training datasets",
    "nlu_profile": {
      "primary_concept": "Removal of low-quality, toxic, or harmful text from training corpora.",
      "semantic_anchors": [
        "data cleaning",
        "toxicity filtering",
        "harmful content removal",
        "deduping",
        "pre-training filtering",
        "data curation",
        "corpus filtering",
        "quality filtering",
        "content filtering",
        "data sanitization",
        "data quality",
        "dataset filtering",
        "training data curation",
        "harmful content filtering",
        "dataset deduplication",
        "data quality filtering",
        "toxic data removal"
      ],
      "entailment_hypothesis": "The training data was filtered to remove harmful content."
    }
  },
  {
    "id": "tech-red-team-data",
    "name": "Red Team Data Integration",
    "categoryId": "cat-alignment",
    "description": "Incorporating red team findings into training data",
    "nlu_profile": {
      "primary_concept": "Feeding successful attack traces back into training to patch vulnerabilities.",
      "semantic_anchors": [
        "red team replay",
        "attack traces",
        "vulnerability patching"
      ],
      "entailment_hypothesis": "Red team data was integrated back into training."
    }
  },
  {
    "id": "tech-safety-reward-modeling",
    "name": "Safety Reward Modeling",
    "categoryId": "cat-alignment",
    "description": "Reward models specifically optimized for safety",
    "nlu_profile": {
      "primary_concept": "Training a specific Reward Model to penalize unsafe outputs.",
      "semantic_anchors": [
        "Safety RM",
        "harmlessness reward model",
        "safety signal",
        "safety reward",
        "harmlessness signal",
        "safety preference",
        "safety-trained reward model",
        "constitutional reward",
        "rule-based reward"
      ],
      "entailment_hypothesis": "A specific safety reward model was used."
    }
  },
  {
    "id": "tech-capability-monitoring",
    "name": "Capability Threshold Monitoring",
    "categoryId": "cat-governance",
    "description": "Monitoring capabilities against safety thresholds",
    "nlu_profile": {
      "primary_concept": "Tracking dangerous capabilities (CBRN, cyber) against limits.",
      "semantic_anchors": [
        "dangerous capabilities",
        "bio-risk",
        "cyber-offense",
        "capability overhang",
        "CBRN",
        "ASL-3",
        "ASL-4",
        "RSP",
        "Responsible Scaling Policy",
        "capability threshold",
        "frontier risk",
        "dangerous capability evaluation",
        "uplift",
        "capability elicitation",
        "capability tracking",
        "capability assessment",
        "model capability evaluation",
        "capability measurement",
        "capability testing",
        "threshold monitoring",
        "capability benchmarking",
        "progression monitoring",
        "capability auditing"
      ],
      "entailment_hypothesis": "Dangerous capabilities are monitored against safety thresholds."
    }
  },
  {
    "id": "tech-community-evaluation",
    "name": "Community-Based Evaluation",
    "categoryId": "cat-evaluation",
    "description": "Evaluation processes leveraging community expertise",
    "nlu_profile": {
      "primary_concept": "Distributed testing where the public tests the model.",
      "semantic_anchors": [
        "crowdsourced testing",
        "community red teaming",
        "public evaluation",
        "external evaluation",
        "third-party evaluation",
        "independent testing",
        "public beta",
        "early access testing",
        "external red team",
        "domain expert evaluation"
      ],
      "entailment_hypothesis": "The model underwent community-based evaluation."
    }
  },
  {
    "id": "tech-multimodal-safety-alignment",
    "name": "Multimodal Safety Alignment",
    "categoryId": "cat-alignment",
    "description": "Safety alignment for multimodal models",
    "nlu_profile": {
      "primary_concept": "Alignment techniques for images/audio or cross-modal risks.",
      "semantic_anchors": [
        "visual safety",
        "multimodal alignment",
        "image safety",
        "cross-modal",
        "vision safety",
        "image-to-text safety",
        "audio safety",
        "video safety",
        "multimodal harm",
        "visual content moderation",
        "NSFW detection"
      ],
      "entailment_hypothesis": "The system employs multimodal safety alignment."
    }
  },
  {
    "id": "tech-enterprise-integration",
    "name": "Enterprise Integration Safety",
    "categoryId": "cat-governance",
    "description": "Safety protocols for enterprise deployment",
    "nlu_profile": {
      "primary_concept": "Safety protocols for corporate workflows (SSO, VPC).",
      "semantic_anchors": [
        "enterprise safety",
        "corporate deployment",
        "VPC",
        "business compliance"
      ],
      "entailment_hypothesis": "The system includes safety features for enterprise integration."
    }
  },
  {
    "id": "tech-access-control-documentation",
    "name": "Access Control Documentation",
    "categoryId": "cat-governance",
    "description": "Documentation of access control and authentication",
    "nlu_profile": {
      "primary_concept": "Documentation regarding API keys, roles, and login permissions.",
      "semantic_anchors": [
        "access policy",
        "authentication rules",
        "API keys",
        "role-based access",
        "RBAC",
        "IAM",
        "SSO",
        "OAuth",
        "API authentication",
        "permission model",
        "access tier",
        "usage tier",
        "access management",
        "identity verification",
        "user authentication",
        "credential management",
        "authorization policy",
        "security controls",
        "access governance",
        "identity and access management",
        "authentication protocol",
        "privileged access"
      ],
      "entailment_hypothesis": "Documentation defines access controls and authentication."
    }
  },
  {
    "id": "tech-refusal-abstention",
    "name": "Refusal / Abstention",
    "categoryId": "cat-alignment",
    "description": "The model's trained ability to politely refuse harmful instructions",
    "nlu_profile": {
      "primary_concept": "Model behavior where it explicitly declines to answer unsafe prompts.",
      "semantic_anchors": [
        "refusal mechanism",
        "declining harmful requests",
        "abstention",
        "safe refusal",
        "I cannot answer",
        "refuse to",
        "declined to",
        "will not",
        "cannot assist",
        "unable to help",
        "policy violation",
        "content policy",
        "not able to generate",
        "respectfully decline",
        "refusal training",
        "rejection mechanism",
        "declining requests",
        "not able to assist",
        "cannot provide",
        "refuse harmful requests",
        "trained to refuse",
        "decline to answer",
        "abstention behavior",
        "refusal response"
      ],
      "entailment_hypothesis": "The model explicitly refuses or abstains from answering harmful prompts."
    }
  }
]