[
  {
    "id": "tech-rlhf",
    "name": "Reinforcement Learning from Human Feedback (RLHF)",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["harmful_content", "bias_and_fairness"],
    "lifecycleStages": ["training"],
    "description": "Training models to align with human preferences through feedback",
    "nlu_profile": {
      "primary_concept": "Optimization using a reward model trained on human preference data, typically via PPO, applied during model fine-tuning.",
      "semantic_anchors": [
        "RLHF",
        "PPO",
        "human feedback",
        "reward model training",
        "preference learning",
        "Proximal Policy Optimization",
        "InstructGPT",
        "preference pair",
        "comparison data",
        "reward signal",
        "preference training",
        "human preference alignment",
        "reward model optimization"
      ],
      "entailment_hypothesis": "The model was trained using reinforcement learning from human feedback (RLHF) during fine-tuning."
    }
  },
  {
    "id": "tech-dpo",
    "name": "Direct Preference Optimization (DPO)",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["harmful_content", "bias_and_fairness"],
    "lifecycleStages": ["training"],
    "description": "Direct optimization on preference data without a separate reward model",
    "nlu_profile": {
      "primary_concept": "Alignment method that directly optimizes policy on preference pairs without requiring a reward model training step.",
      "semantic_anchors": [
        "DPO",
        "Direct Preference Optimization",
        "preference optimization",
        "direct alignment",
        "preference-based fine-tuning",
        "reward-free alignment",
        "direct policy learning"
      ],
      "entailment_hypothesis": "The model uses Direct Preference Optimization (DPO) for alignment."
    }
  },
  {
    "id": "tech-constitutional-ai",
    "name": "Constitutional AI / Self-Critique",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["training"],
    "description": "Training models to critique and revise their own outputs based on constitutional principles",
    "nlu_profile": {
      "primary_concept": "Training method where the model critiques and revises outputs based on explicit principles, rules, or guidelines.",
      "semantic_anchors": [
        "Constitutional AI",
        "RLAIF",
        "self-critique",
        "principle-driven training",
        "CAI",
        "constitutional principles",
        "self-revision",
        "AI feedback",
        "critique-revise loop",
        "harmlessness principles",
        "RL from AI feedback",
        "rule-based reward model",
        "principle-based training",
        "self-play alignment",
        "safety guidelines training",
        "value alignment training",
        "safety-tuning",
        "safety fine-tuning"
      ],
      "entailment_hypothesis": "The model was trained using principle-based self-critique, Constitutional AI, or safety-specific fine-tuning."
    }
  },
  {
    "id": "tech-adversarial-training",
    "name": "Adversarial Training",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["security_and_misuse"],
    "lifecycleStages": ["training"],
    "description": "Training with adversarial examples to improve robustness",
    "nlu_profile": {
      "primary_concept": "Incorporating adversarial examples, red-team findings, or attack data into model training to improve safety robustness.",
      "semantic_anchors": [
        "adversarial training",
        "robustness training",
        "attack-defense training",
        "adversarial examples",
        "perturbation training",
        "adversarial fine-tuning",
        "adversarial data augmentation",
        "red-team data integration",
        "safety training with attacks",
        "adversarial robustness",
        "jailbreak resistance training",
        "hardening against attacks"
      ],
      "entailment_hypothesis": "The model was trained with adversarial examples or attack data to improve robustness."
    }
  },
  {
    "id": "tech-safety-reward-modeling",
    "name": "Safety Reward Modeling",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["training"],
    "description": "Reward models specifically optimized for safety",
    "nlu_profile": {
      "primary_concept": "Training a specific Reward Model to penalize unsafe outputs during RLHF.",
      "semantic_anchors": [
        "Safety RM",
        "harmlessness reward model",
        "safety signal",
        "safety reward",
        "harmlessness signal",
        "safety preference",
        "safety-trained reward model"
      ],
      "entailment_hypothesis": "A specific safety reward model was used during alignment."
    }
  },
  {
    "id": "tech-refusal-training",
    "name": "Refusal / Abstention Training",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["training"],
    "description": "Training the model to politely refuse harmful instructions",
    "nlu_profile": {
      "primary_concept": "Training model behavior to explicitly decline unsafe prompts during fine-tuning.",
      "semantic_anchors": [
        "refusal mechanism",
        "declining harmful requests",
        "abstention",
        "safe refusal",
        "I cannot answer",
        "refuse to",
        "will not",
        "cannot assist",
        "refusal training",
        "trained to refuse"
      ],
      "entailment_hypothesis": "The model was trained to refuse or abstain from answering harmful prompts."
    }
  },
  {
    "id": "tech-multimodal-safety-alignment",
    "name": "Multimodal Safety Alignment",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["training"],
    "description": "Safety alignment for multimodal models",
    "nlu_profile": {
      "primary_concept": "Alignment techniques specifically designed for images, audio, or cross-modal risks.",
      "semantic_anchors": [
        "visual safety",
        "multimodal alignment",
        "image safety",
        "cross-modal safety",
        "vision safety",
        "audio safety",
        "video safety",
        "NSFW detection"
      ],
      "entailment_hypothesis": "The system employs multimodal safety alignment."
    }
  },
  {
    "id": "tech-bias-mitigation",
    "name": "Bias Mitigation (Post-Training)",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["bias_and_fairness"],
    "lifecycleStages": ["training"],
    "description": "Algorithmic interventions during fine-tuning to reduce output bias",
    "nlu_profile": {
      "primary_concept": "Techniques applied during fine-tuning to reduce stereotypical biases.",
      "semantic_anchors": [
        "fairness tuning",
        "de-biasing",
        "bias reduction",
        "counterfactual data augmentation",
        "demographic parity",
        "stereotype mitigation",
        "demographic fairness"
      ],
      "entailment_hypothesis": "The model underwent specific fine-tuning to mitigate bias."
    }
  },
  {
    "id": "tech-machine-unlearning",
    "name": "Machine Unlearning",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["privacy_and_pii"],
    "lifecycleStages": ["training"],
    "status": "aspirational",
    "status_note": "No tracked provider documents production use. Google DeepMind research (2024) questions effectiveness of current approaches.",
    "description": "Algorithms to 'forget' specific training data without retraining",
    "nlu_profile": {
      "primary_concept": "Interventions to remove influence of specific data points from model weights.",
      "semantic_anchors": [
        "unlearning",
        "model editing",
        "knowledge removal",
        "targeted forgetting"
      ],
      "entailment_hypothesis": "The model underwent machine unlearning or targeted forgetting."
    }
  },
  {
    "id": "tech-training-data-quality-filtering",
    "name": "Training Data Quality Filtering",
    "categoryId": "cat-model-development",
    "riskAreaIds": [],
    "lifecycleStages": ["pre-training"],
    "description": "Deduplication and quality filtering of training corpora",
    "nlu_profile": {
      "primary_concept": "Removal of low-quality, duplicated, or noisy text from training corpora.",
      "semantic_anchors": [
        "data cleaning",
        "deduplication",
        "deduping",
        "quality filtering",
        "corpus filtering",
        "data curation",
        "dataset filtering"
      ],
      "entailment_hypothesis": "The training data underwent quality filtering and deduplication."
    }
  },
  {
    "id": "tech-dataset-auditing",
    "name": "Dataset Auditing & Representation Analysis",
    "categoryId": "cat-model-development",
    "riskAreaIds": ["bias_and_fairness"],
    "lifecycleStages": ["pre-training"],
    "description": "Detection and analysis of demographic and cultural biases in training data",
    "nlu_profile": {
      "primary_concept": "Analysis and documentation of training datasets to identify representational imbalances, demographic biases, and data quality issues.",
      "semantic_anchors": [
        "dataset auditing",
        "bias analysis",
        "representation measurement",
        "demographic analysis",
        "data diversity",
        "data documentation",
        "datasheet",
        "data card",
        "training data analysis",
        "corpus analysis",
        "data provenance",
        "representation analysis",
        "data composition",
        "dataset curation",
        "data sampling"
      ],
      "entailment_hypothesis": "The training data was analyzed or audited for bias, representation, or quality issues."
    }
  },
  {
    "id": "tech-red-teaming",
    "name": "Red Teaming",
    "categoryId": "cat-evaluation",
    "riskAreaIds": ["security_and_misuse"],
    "lifecycleStages": ["evaluation"],
    "description": "Systematic adversarial testing including manual expert red teaming, automated AI-driven attacks, and integration of findings into training",
    "nlu_profile": {
      "primary_concept": "Adversarial testing where human experts or automated systems attempt to bypass safety filters, with findings fed back into improvement.",
      "semantic_anchors": [
        "red teaming",
        "adversarial testing",
        "penetration testing",
        "attack simulation",
        "red team",
        "manual red teaming",
        "human red teaming",
        "automated red teaming",
        "LLM-as-a-Judge",
        "model-based red teaming",
        "automated attacks",
        "AI red teaming",
        "red team replay",
        "attack traces",
        "vulnerability patching",
        "red team data",
        "adversarial data integration"
      ],
      "entailment_hypothesis": "The model underwent adversarial red teaming, either manual or automated."
    }
  },
  {
    "id": "tech-safety-benchmarks",
    "name": "Safety Benchmarking",
    "categoryId": "cat-evaluation",
    "riskAreaIds": [],
    "lifecycleStages": ["evaluation"],
    "description": "Standardized benchmarks for measuring safety performance",
    "nlu_profile": {
      "primary_concept": "Evaluation using standardized safety benchmark suites to measure model safety performance.",
      "semantic_anchors": [
        "safety benchmarks",
        "TruthfulQA",
        "RealToxicityPrompts",
        "HarmBench",
        "AdvBench",
        "Do-Not-Answer",
        "safety evaluation",
        "benchmark evaluation",
        "safety score",
        "evaluation suite",
        "MLCommons",
        "safety test suite"
      ],
      "entailment_hypothesis": "The model was evaluated against standardized safety benchmarks."
    }
  },
  {
    "id": "tech-community-evaluation",
    "name": "Community-Based Evaluation",
    "categoryId": "cat-evaluation",
    "riskAreaIds": [],
    "lifecycleStages": ["evaluation"],
    "description": "Evaluation processes leveraging community expertise",
    "nlu_profile": {
      "primary_concept": "Distributed safety testing where the public or domain experts evaluate the model's safety properties.",
      "semantic_anchors": [
        "crowdsourced safety testing",
        "community red teaming",
        "public safety evaluation",
        "external safety evaluation",
        "third-party safety evaluation",
        "domain expert safety evaluation",
        "bug bounty for AI safety"
      ],
      "entailment_hypothesis": "The model underwent community-based evaluation."
    }
  },
  {
    "id": "tech-input-guardrail-systems",
    "name": "Input Guardrail Systems",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["harmful_content", "security_and_misuse"],
    "lifecycleStages": ["inference"],
    "description": "Infrastructure for input classification and filtering (e.g., LlamaGuard, NeMo)",
    "nlu_profile": {
      "primary_concept": "A dedicated, named guardrail system or classifier that inspects user prompts before they reach the main model.",
      "semantic_anchors": [
        "input filtering",
        "prompt classification",
        "Llama Guard",
        "NeMo Guardrails",
        "input moderation",
        "input classifier",
        "ShieldGemma",
        "Aegis Guard",
        "Qwen3Guard",
        "prompt filtering"
      ],
      "entailment_hypothesis": "A dedicated guardrail system or classifier filters user prompts before the main model processes them."
    }
  },
  {
    "id": "tech-system-prompts",
    "name": "System Prompts / Metaprompts",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": [],
    "lifecycleStages": ["inference"],
    "description": "Instructions provided at the system level to enforce behavior and safety",
    "nlu_profile": {
      "primary_concept": "Pre-pended instructional context or 'system message' that defines behavioral boundaries at runtime.",
      "semantic_anchors": [
        "system prompt",
        "metaprompt",
        "system message",
        "hidden context",
        "developer instructions",
        "system instruction",
        "pre-prompt"
      ],
      "entailment_hypothesis": "The system uses a hidden system prompt to enforce safety constraints."
    }
  },
  {
    "id": "tech-output-filtering-systems",
    "name": "Output Safety Systems",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["inference"],
    "description": "Infrastructure for output content moderation and safety filtering",
    "nlu_profile": {
      "primary_concept": "A dedicated post-generation filtering system or classifier that analyzes and blocks unsafe model outputs.",
      "semantic_anchors": [
        "output moderation",
        "response filtering",
        "toxicity detection",
        "completion blocking",
        "post-generation filter",
        "output classifier",
        "content moderation API",
        "completion filtering",
        "response moderation"
      ],
      "entailment_hypothesis": "A dedicated post-generation system filters or blocks model outputs that violate safety policies."
    }
  },
  {
    "id": "tech-hallucination-grounding",
    "name": "Hallucination Detection & Grounding",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["misinformation"],
    "lifecycleStages": ["inference"],
    "description": "Mechanisms to verify factuality and reduce fabrication",
    "nlu_profile": {
      "primary_concept": "Techniques to detect or prevent fabricated information in model outputs.",
      "semantic_anchors": [
        "factuality check",
        "hallucination mitigation",
        "citation enforcement",
        "grounding metric",
        "factual grounding",
        "source attribution"
      ],
      "entailment_hypothesis": "The model uses mechanisms to detect or reduce hallucinations."
    }
  },
  {
    "id": "tech-rag-guardrails",
    "name": "RAG Guardrails",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["misinformation"],
    "lifecycleStages": ["inference"],
    "description": "Checks for grounding and citation accuracy in RAG systems",
    "nlu_profile": {
      "primary_concept": "Verification mechanisms in retrieval systems ensuring outputs are supported by context.",
      "semantic_anchors": [
        "hallucination detection",
        "grounding check",
        "citation verification",
        "context adherence",
        "RAG verification"
      ],
      "entailment_hypothesis": "The system uses grounding checks to ensure RAG accuracy."
    }
  },
  {
    "id": "tech-realtime-fact-checking",
    "name": "Real-time Fact Checking",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["misinformation"],
    "lifecycleStages": ["inference"],
    "description": "Dynamic verification of information accuracy",
    "nlu_profile": {
      "primary_concept": "Dynamic verification of claims against external knowledge bases during generation.",
      "semantic_anchors": [
        "fact checking",
        "claim verification",
        "truthfulness check",
        "google grounding",
        "real-time verification",
        "search grounding"
      ],
      "entailment_hypothesis": "The system performs real-time fact checking or claim verification against external sources."
    }
  },
  {
    "id": "tech-watermarking",
    "name": "Provenance & Watermarking",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["transparency"],
    "lifecycleStages": ["inference"],
    "description": "Embedding detectable patterns in AI-generated content",
    "nlu_profile": {
      "primary_concept": "Techniques to embed imperceptible signals to prove AI origin.",
      "semantic_anchors": [
        "watermarking",
        "SynthID",
        "C2PA",
        "provenance",
        "digital signature",
        "AI-generated content detection",
        "synthetic content marking"
      ],
      "entailment_hypothesis": "The system applies watermarking or provenance metadata."
    }
  },
  {
    "id": "tech-multistage-pipeline",
    "name": "Multi-stage Safety Pipeline",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": [],
    "lifecycleStages": ["inference"],
    "description": "Multiple layers of safety checks in defense-in-depth architecture",
    "nlu_profile": {
      "primary_concept": "Defense-in-depth architecture using multiple cascaded safety checks.",
      "semantic_anchors": [
        "defense in depth",
        "cascaded checks",
        "layered safety",
        "pipeline architecture",
        "multi-layer safety"
      ],
      "entailment_hypothesis": "The system employs a multi-stage safety pipeline."
    }
  },
  {
    "id": "tech-configurable-policies",
    "name": "Configurable Safety Policies",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": [],
    "lifecycleStages": ["inference", "monitoring"],
    "description": "User or admin configurable safety thresholds",
    "nlu_profile": {
      "primary_concept": "Allowing users to adjust safety thresholds for their use case.",
      "semantic_anchors": [
        "custom safety settings",
        "adjustable filters",
        "configurable moderation",
        "safety mode",
        "strictness level",
        "harm category toggle"
      ],
      "entailment_hypothesis": "Users can configure or customize the safety policies."
    }
  },
  {
    "id": "tech-observability-monitoring",
    "name": "Observability & Audit Logging",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": [],
    "lifecycleStages": ["monitoring"],
    "description": "Real-time monitoring, logging, and drift detection",
    "nlu_profile": {
      "primary_concept": "Continuous runtime tracking of system inputs and outputs for monitoring.",
      "semantic_anchors": [
        "audit logging",
        "runtime monitoring",
        "drift detection",
        "violation logging",
        "observability"
      ],
      "entailment_hypothesis": "The deployment includes real-time monitoring or logging."
    }
  },
  {
    "id": "tech-circuit-breakers",
    "name": "Circuit Breakers / Kill Switches",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["autonomy_and_control"],
    "lifecycleStages": ["monitoring"],
    "status": "aspirational",
    "status_note": "No tracked provider documents production deployment. Gray Swan AI (untracked) has early-stage Cygnet. Anthropic RSP and OpenAI Preparedness Framework reference emergency protocols at policy level only.",
    "description": "Mechanisms to immediately disconnect system if dangerous actions occur",
    "nlu_profile": {
      "primary_concept": "A pre-built emergency mechanism that can instantly halt all model serving or shut down the system when dangerous autonomous behaviour is detected.",
      "semantic_anchors": [
        "emergency model shutdown mechanism",
        "kill switch for model serving",
        "circuit breaker halts model execution",
        "automatic emergency stop for AI system"
      ],
      "entailment_hypothesis": "The provider has deployed a dedicated emergency kill switch or circuit breaker mechanism that can instantly halt model serving when dangerous behaviour is detected.",
      "excluded_terms": ["rate limiting", "load balancing", "service degradation", "failover", "availability", "infrastructure monitoring"]
    }
  },
  {
    "id": "tech-prompt-injection-defense",
    "name": "Jailbreak & Injection Defense",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["security_and_misuse"],
    "lifecycleStages": ["inference"],
    "description": "Detection and prevention of prompt injection and jailbreak attempts",
    "nlu_profile": {
      "primary_concept": "Detection of adversarial attempts to hijack the model through prompt manipulation.",
      "semantic_anchors": [
        "prompt injection",
        "jailbreak defense",
        "indirect injection",
        "DAN mode",
        "jailbreak",
        "prompt attack",
        "adversarial prompt",
        "instruction injection"
      ],
      "entailment_hypothesis": "The system includes defenses against prompt injection."
    }
  },
  {
    "id": "tech-code-execution-sandboxing",
    "name": "Code Execution Sandboxing",
    "categoryId": "cat-runtime-safety",
    "riskAreaIds": ["security_and_misuse"],
    "lifecycleStages": ["inference"],
    "description": "Isolating code generated or executed by the LLM",
    "nlu_profile": {
      "primary_concept": "A dedicated sandboxed or containerized environment specifically for executing model-generated code in isolation from production systems.",
      "semantic_anchors": [
        "code execution sandbox",
        "sandboxed code interpreter",
        "containerized code execution environment",
        "isolated code execution for model output",
        "model-generated code sandbox"
      ],
      "entailment_hypothesis": "The provider runs model-generated code in a dedicated sandbox or isolated container to prevent it from affecting production systems.",
      "excluded_terms": ["sandbox evaluation", "sandboxed testing", "development sandbox", "sandbox environment for training"]
    }
  },
  {
    "id": "tech-violence-detection",
    "name": "Violence & Gore Detection",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["pre-training", "inference"],
    "description": "Detection of graphic violence, terrorism threats, and violent content",
    "nlu_profile": {
      "primary_concept": "Classification of violent content including graphic violence, terrorism, and threats.",
      "semantic_anchors": [
        "violence detection",
        "graphic violence",
        "gore detection",
        "terrorism",
        "violent threats",
        "violent content",
        "graphic content",
        "violence classification"
      ],
      "entailment_hypothesis": "The system detects and classifies violent content."
    }
  },
  {
    "id": "tech-hate-speech-detection",
    "name": "Hate Speech & Harassment Detection",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["harmful_content", "bias_and_fairness"],
    "lifecycleStages": ["pre-training", "inference"],
    "description": "Detection of hate speech, discrimination, harassment, and offensive language",
    "nlu_profile": {
      "primary_concept": "Classification of hate speech, discrimination, stereotyping, harassment, and offensive language.",
      "semantic_anchors": [
        "hate speech",
        "discrimination detection",
        "harassment detection",
        "offensive language",
        "stereotyping",
        "hate detection",
        "toxic language",
        "discriminatory content"
      ],
      "entailment_hypothesis": "The system detects hate speech, discrimination, or harassment."
    }
  },
  {
    "id": "tech-self-harm-prevention",
    "name": "Self-Harm & Suicide Prevention",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["inference"],
    "description": "Detection and intervention for suicide encouragement and self-harm instructions",
    "nlu_profile": {
      "primary_concept": "Detection and prevention of content encouraging suicide, self-harm, or other self-destructive behavior.",
      "semantic_anchors": [
        "suicide prevention",
        "self-harm detection",
        "suicide encouragement",
        "self-harm instructions",
        "psychological harm",
        "mental health crisis",
        "self-injury",
        "eating disorders",
        "self-destructive behavior"
      ],
      "entailment_hypothesis": "The system detects or prevents self-harm, suicide, or self-destructive content."
    }
  },
  {
    "id": "tech-sexual-content-moderation",
    "name": "Sexual Content Moderation",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["pre-training", "inference"],
    "description": "Detection and filtering of pornographic or sexually explicit content",
    "nlu_profile": {
      "primary_concept": "Classification of pornographic, sexually explicit, or inappropriate sexual content.",
      "semantic_anchors": [
        "sexual content",
        "pornography detection",
        "NSFW detection",
        "sexually explicit",
        "adult content",
        "sexual solicitation"
      ],
      "entailment_hypothesis": "The system detects and moderates sexual content."
    }
  },
  {
    "id": "tech-weapons-illegal-activity",
    "name": "Weapons & Illegal Activity Detection",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["harmful_content", "security_and_misuse"],
    "lifecycleStages": ["inference"],
    "description": "Detection of instructions for weapons creation, CBRNE threats, and other illegal activities",
    "nlu_profile": {
      "primary_concept": "Detection of instructions for weapons creation, CBRNE threats, drug production, fraud, and other illegal activities.",
      "semantic_anchors": [
        "illegal activity",
        "fraud detection",
        "hacking instructions",
        "drug production",
        "theft instructions",
        "money laundering",
        "criminal activity",
        "chemical weapons",
        "biological weapons",
        "explosives",
        "CBRN",
        "CBRNE",
        "radiological",
        "nuclear weapons",
        "weapons of mass destruction",
        "WMD",
        "bioweapons",
        "bio-risk",
        "weapons instructions",
        "dangerous content",
        "harmful requests",
        "content policy violations",
        "prohibited content categories",
        "unsafe content",
        "illicit activities"
      ],
      "entailment_hypothesis": "The system blocks or detects content related to weapons, CBRN threats, or illegal activities."
    }
  },
  {
    "id": "tech-csam-detection",
    "name": "CSAM Detection & Prevention",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["pre-training", "inference"],
    "description": "Detection and removal of child sexual abuse material",
    "nlu_profile": {
      "primary_concept": "Detection, prevention, and removal of child sexual abuse material and content involving minors.",
      "semantic_anchors": [
        "CSAM",
        "hash matching",
        "PhotoDNA",
        "NCMEC",
        "child safety",
        "child sexual abuse material",
        "minor safety",
        "child exploitation",
        "CSEA",
        "child sexual exploitation",
        "protecting minors",
        "content involving minors",
        "age-appropriate content",
        "underage content",
        "content depicting minors"
      ],
      "entailment_hypothesis": "The system detects or prevents child sexual abuse material or content that exploits minors."
    }
  },
  {
    "id": "tech-pii-detection",
    "name": "PII Detection & Redaction",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["privacy_and_pii"],
    "lifecycleStages": ["pre-training", "inference"],
    "description": "Detection and redaction of personally identifiable information",
    "nlu_profile": {
      "primary_concept": "Identification and obfuscation of Personally Identifiable Information.",
      "semantic_anchors": [
        "PII redaction",
        "anonymization",
        "data masking",
        "personally identifiable information",
        "privacy filtering",
        "de-identification"
      ],
      "entailment_hypothesis": "The system detects and redacts personally identifiable information."
    }
  },
  {
    "id": "tech-misinformation-detection",
    "name": "Misinformation & False Claims Detection",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["misinformation"],
    "lifecycleStages": ["inference"],
    "description": "Detection and flagging of verifiably false information",
    "nlu_profile": {
      "primary_concept": "Detection of verifiably false claims, disinformation campaigns, and misleading content.",
      "semantic_anchors": [
        "misinformation",
        "false claims",
        "fact checking",
        "fake news",
        "disinformation",
        "false information",
        "misleading content",
        "propaganda detection"
      ],
      "entailment_hypothesis": "The system detects misinformation or false claims."
    }
  },
  {
    "id": "tech-copyright-ip-violation",
    "name": "Copyright & IP Violation Detection",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["copyright_and_ip"],
    "lifecycleStages": ["pre-training", "inference"],
    "description": "Detection of copyrighted content, trademark violations, and intellectual property infringement",
    "nlu_profile": {
      "primary_concept": "Detection and prevention of copyrighted content reproduction, trademark violations, and IP infringement.",
      "semantic_anchors": [
        "copyright filtering",
        "licensed content removal",
        "IP protection",
        "copyrighted material",
        "trademark violation",
        "intellectual property",
        "verbatim reproduction",
        "content attribution",
        "copyright infringement",
        "memorization prevention"
      ],
      "entailment_hypothesis": "The system detects or prevents copyright and intellectual property violations."
    }
  },
  {
    "id": "tech-sycophancy-detection",
    "name": "Sycophancy Detection",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["harmful_content"],
    "lifecycleStages": ["evaluation", "inference"],
    "description": "Detection of excessive agreement, user flattery, and opinion mirroring behaviours",
    "nlu_profile": {
      "primary_concept": "A dedicated evaluation benchmark, classifier, or training intervention specifically targeting sycophantic model responses.",
      "semantic_anchors": [
        "sycophancy evaluation benchmark",
        "sycophancy mitigation training",
        "anti-sycophancy reward signal",
        "sycophancy classifier",
        "sycophantic behavior detection system",
        "sycophancy rate measurement"
      ],
      "entailment_hypothesis": "The provider implemented a dedicated benchmark, classifier, or training intervention to measure or reduce sycophantic responses in the model.",
      "excluded_terms": ["user satisfaction", "helpfulness evaluation", "preference optimization"]
    }
  },
  {
    "id": "tech-cybersecurity-threat",
    "name": "Cybersecurity Threat Detection",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["security_and_misuse"],
    "lifecycleStages": ["evaluation", "inference"],
    "description": "Detection of cybersecurity attack instructions, exploit generation, and offensive cyber capabilities",
    "nlu_profile": {
      "primary_concept": "A dedicated runtime classifier or output filter that specifically detects and blocks model-generated exploit code, malware instructions, or cyberattack content.",
      "semantic_anchors": [
        "cybersecurity content classifier",
        "exploit code generation filter",
        "malware instruction blocking",
        "cyberattack output filter",
        "block offensive cyber content"
      ],
      "entailment_hypothesis": "The provider deployed a dedicated runtime filter or classifier that specifically blocks the model from generating exploit code, malware instructions, or cyberattack content.",
      "excluded_terms": ["cyber capability evaluation", "offensive cyber benchmark", "cybersecurity assessment", "cyber risk category", "cybersecurity policy", "refuse harmful requests"]
    }
  },
  {
    "id": "tech-autonomous-behaviour",
    "name": "Autonomous Behaviour Classification",
    "categoryId": "cat-harm-classification",
    "riskAreaIds": ["autonomy_and_control", "dual_use"],
    "lifecycleStages": ["evaluation", "inference"],
    "description": "Classification of autonomous, power-seeking, scheming, and deceptively aligned behaviours",
    "nlu_profile": {
      "primary_concept": "A dedicated evaluation suite or runtime classifier that specifically tests whether the model exhibits scheming, power-seeking, alignment faking, or sandbagging behaviours.",
      "semantic_anchors": [
        "scheming evaluation suite",
        "power-seeking propensity test",
        "alignment faking evaluation",
        "sandbagging detection evaluation",
        "autonomous replication test",
        "instrumental convergence evaluation",
        "self-preservation behaviour test"
      ],
      "entailment_hypothesis": "The provider ran a dedicated evaluation or deployed a classifier specifically designed to test for scheming, power-seeking, alignment faking, or sandbagging in the model.",
      "excluded_terms": ["agentic capability", "tool use evaluation", "agent benchmark", "autonomous coding", "agentic task"]
    }
  },
  {
    "id": "tech-capability-monitoring",
    "name": "Capability Threshold Monitoring",
    "categoryId": "cat-governance",
    "riskAreaIds": ["dual_use", "autonomy_and_control"],
    "lifecycleStages": ["evaluation", "governance"],
    "description": "Monitoring dangerous capabilities against safety thresholds using framework-level assessments",
    "nlu_profile": {
      "primary_concept": "A formal framework for tracking whether model capabilities have crossed predefined danger thresholds requiring upgraded safeguards.",
      "semantic_anchors": [
        "dangerous capabilities",
        "capability overhang",
        "ASL-3",
        "ASL-4",
        "RSP",
        "Responsible Scaling Policy",
        "capability threshold",
        "capability evaluation framework",
        "Preparedness Framework",
        "responsible scaling",
        "safety level classification",
        "risk threshold",
        "capability elicitation"
      ],
      "entailment_hypothesis": "A formal framework monitors whether model capabilities exceed predefined danger thresholds."
    }
  },
  {
    "id": "tech-safety-advisory",
    "name": "Independent Safety Advisory",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["governance"],
    "description": "External advisory board for safety oversight",
    "nlu_profile": {
      "primary_concept": "External advisory board or committee providing independent safety oversight.",
      "semantic_anchors": [
        "safety board",
        "advisory committee",
        "ethics board",
        "independent oversight",
        "safety advisory",
        "external review board",
        "governance committee",
        "safety council",
        "external auditors"
      ],
      "entailment_hypothesis": "An independent safety advisory board or committee provides oversight."
    }
  },
  {
    "id": "tech-incident-reporting",
    "name": "Incident Reporting Systems",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["monitoring", "governance"],
    "description": "Systems for reporting safety incidents and vulnerabilities",
    "nlu_profile": {
      "primary_concept": "Processes for tracking and disclosing safety failures.",
      "semantic_anchors": [
        "incident reporting",
        "bug bounty",
        "vulnerability disclosure",
        "failure tracking",
        "safety incident",
        "abuse reporting"
      ],
      "entailment_hypothesis": "A mechanism exists for reporting and tracking safety incidents."
    }
  },
  {
    "id": "tech-responsible-release",
    "name": "Responsible Release Protocols",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["governance"],
    "description": "Structured processes for staged release",
    "nlu_profile": {
      "primary_concept": "Staged deployment strategies to monitor safety before full release.",
      "semantic_anchors": [
        "staged release",
        "phased deployment",
        "controlled rollout",
        "gradual release",
        "beta release",
        "early access"
      ],
      "entailment_hypothesis": "The model was released using a responsible release protocol."
    }
  },
  {
    "id": "tech-stakeholder-engagement",
    "name": "Stakeholder Engagement",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["governance"],
    "description": "Processes for collecting feedback from users and communities",
    "nlu_profile": {
      "primary_concept": "A formal, structured program for soliciting and incorporating safety-related feedback from external stakeholders such as civil society organizations, affected communities, or multi-stakeholder advisory bodies.",
      "semantic_anchors": [
        "external stakeholder safety consultation",
        "civil society engagement program",
        "multi-stakeholder advisory board",
        "public safety consultation process",
        "community safety feedback program",
        "NGO partnership for AI safety"
      ],
      "entailment_hypothesis": "The provider established a formal program or process to solicit safety-related feedback from external stakeholders, civil society, or community advisory bodies.",
      "excluded_terms": ["academic research collaboration", "open source community", "user feedback", "bug bounty", "red teaming", "crowdsourced evaluation"]
    }
  },
  {
    "id": "tech-regulatory-compliance",
    "name": "Regulatory Compliance",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["governance"],
    "description": "Compliance with government regulatory requirements",
    "nlu_profile": {
      "primary_concept": "Adherence to specific government regulations and compliance frameworks.",
      "semantic_anchors": [
        "regulatory compliance",
        "EU AI Act",
        "NIST AI RMF",
        "compliance audit",
        "GDPR",
        "Executive Order",
        "SOC 2",
        "ISO 27001",
        "FedRAMP"
      ],
      "entailment_hypothesis": "The system or organization complies with specific government regulations or compliance standards."
    }
  },
  {
    "id": "tech-access-control-documentation",
    "name": "Access Control Documentation",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["governance"],
    "description": "Documentation of access control and authentication",
    "nlu_profile": {
      "primary_concept": "Formal documentation of role-based access control policies, API key management, and authentication requirements for the AI system.",
      "semantic_anchors": [
        "access control policy documentation",
        "role-based access control for model access",
        "API key management policy",
        "authentication requirements for AI system",
        "model access permissions documentation"
      ],
      "entailment_hypothesis": "The organization publishes formal documentation defining role-based access controls, API authentication policies, or permission tiers for its AI system.",
      "excluded_terms": ["access to the model weights", "open access", "model availability", "API endpoint", "API usage"]
    }
  },
  {
    "id": "tech-enterprise-integration",
    "name": "Enterprise Integration Safety",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["governance"],
    "description": "Safety protocols for enterprise deployment",
    "nlu_profile": {
      "primary_concept": "Safety protocols for corporate workflows (SSO, VPC).",
      "semantic_anchors": [
        "enterprise safety",
        "corporate deployment",
        "VPC",
        "business compliance"
      ],
      "entailment_hypothesis": "The system includes safety features for enterprise integration."
    }
  },
  {
    "id": "tech-data-sovereignty",
    "name": "Data Sovereignty Controls",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["governance"],
    "description": "Controls ensuring data processing complies with regional requirements",
    "nlu_profile": {
      "primary_concept": "Mechanisms ensuring data processing occurs within specific jurisdictions.",
      "semantic_anchors": [
        "data residency",
        "sovereignty controls",
        "geo-fencing",
        "local processing"
      ],
      "entailment_hypothesis": "The system enforces data sovereignty or residency controls."
    }
  },
  {
    "id": "tech-data-retention-policies",
    "name": "Data Retention Policies",
    "categoryId": "cat-governance",
    "riskAreaIds": ["privacy_and_pii"],
    "lifecycleStages": ["governance"],
    "description": "Policies for data retention and lifecycle management",
    "nlu_profile": {
      "primary_concept": "A formal, documented policy specifying how long user data, conversation logs, or training data are retained before scheduled deletion.",
      "semantic_anchors": [
        "data retention policy",
        "conversation log retention period",
        "automatic data deletion schedule",
        "data lifecycle management policy",
        "user data retention limit"
      ],
      "entailment_hypothesis": "The organization has a formal policy specifying retention periods and scheduled deletion of user data or conversation logs.",
      "excluded_terms": ["data preprocessing", "data filtering", "training data curation", "data deduplication"]
    }
  },
  {
    "id": "tech-ethical-labour-sourcing",
    "name": "Ethical Human Labour Sourcing",
    "categoryId": "cat-governance",
    "riskAreaIds": [],
    "lifecycleStages": ["governance"],
    "description": "Ethical sourcing and welfare standards for human annotation and content review workers",
    "nlu_profile": {
      "primary_concept": "Policies and practices ensuring fair treatment, welfare, and ethical sourcing of human workers involved in annotation, content review, and model training.",
      "semantic_anchors": [
        "crowd worker welfare",
        "crowd worker wellness standards",
        "fair and ethical compensation to workers",
        "annotation workers",
        "contractor welfare",
        "content reviewer wellbeing",
        "ethical sourcing of human workers",
        "human labeller welfare",
        "annotator welfare",
        "data labelling ethics",
        "content moderator wellbeing",
        "worker conditions",
        "safe workplace practices"
      ],
      "entailment_hypothesis": "The organization addresses ethical treatment, fair compensation, or welfare of human annotation or crowd workers."
    }
  }
]
