[
  {
    "id": "commentary-001",
    "title": "Training language models to follow instructions with human feedback",
    "url": "https://arxiv.org/abs/2203.02155",
    "author": "Long Ouyang, Jeff Wu, Xu Jiang, et al.",
    "organization": "OpenAI",
    "date": "2022-03-04",
    "type": "academic_paper",
    "techniqueIds": ["tech-rlhf"],
    "summary": "Landmark InstructGPT paper demonstrating that fine-tuning GPT-3 with RLHF produces outputs preferred over those from a 100x larger model, establishing the foundational RLHF methodology for aligning LLMs.",
    "sentiment": "positive"
  },
  {
    "id": "commentary-002",
    "title": "Constitutional AI: Harmlessness from AI Feedback",
    "url": "https://arxiv.org/abs/2212.08073",
    "author": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, et al.",
    "organization": "Anthropic",
    "date": "2022-12-15",
    "type": "academic_paper",
    "techniqueIds": ["tech-constitutional-ai", "tech-rlhf"],
    "summary": "Introduces Constitutional AI, training a harmless AI assistant using AI-generated feedback guided by a set of principles, replacing human labels for harmlessness with RL from AI Feedback (RLAIF).",
    "sentiment": "positive"
  },
  {
    "id": "commentary-003",
    "title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model",
    "url": "https://arxiv.org/abs/2305.18290",
    "author": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn",
    "organization": "Stanford University",
    "date": "2023-05-29",
    "type": "academic_paper",
    "techniqueIds": ["tech-dpo", "tech-rlhf"],
    "summary": "Proposes DPO as a simpler alternative to RLHF that eliminates the separate reward model and RL training loop, using a closed-form classification loss to directly optimize language model policies from human preferences.",
    "sentiment": "positive"
  },
  {
    "id": "commentary-004",
    "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
    "url": "https://arxiv.org/abs/2307.15217",
    "author": "Stephen Casper, Xander Davies, Claudia Shi, et al.",
    "organization": "MIT, UC Berkeley, ETH Zurich",
    "date": "2023-07-27",
    "type": "academic_paper",
    "techniqueIds": ["tech-rlhf", "tech-dpo"],
    "summary": "Comprehensive survey of 250+ papers identifying open problems and fundamental limitations of RLHF, including reward hacking, distributional shift, and evaluation difficulties, while proposing auditing standards.",
    "sentiment": "mixed"
  },
  {
    "id": "commentary-005",
    "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
    "url": "https://arxiv.org/abs/2209.07858",
    "author": "Deep Ganguli, Liane Lovitt, Jackson Kernion, et al.",
    "organization": "Anthropic",
    "date": "2022-09-16",
    "type": "academic_paper",
    "techniqueIds": ["tech-red-teaming", "tech-rlhf"],
    "summary": "Investigates human red teaming across model sizes and types, releases a dataset of 38,961 red team attacks, and finds that RLHF-trained models become harder to red team as they scale.",
    "sentiment": "positive"
  },
  {
    "id": "commentary-006",
    "title": "Red Teaming Language Models with Language Models",
    "url": "https://arxiv.org/abs/2202.03286",
    "author": "Ethan Perez, Saffron Huang, Francis Song, et al.",
    "organization": "DeepMind",
    "date": "2022-02-07",
    "type": "academic_paper",
    "techniqueIds": ["tech-red-teaming", "tech-adversarial-training"],
    "summary": "Proposes automated red teaming using one LLM to generate adversarial test cases for another, discovering tens of thousands of offensive outputs from a 280B parameter chatbot.",
    "sentiment": "positive"
  },
  {
    "id": "commentary-007",
    "title": "HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal",
    "url": "https://arxiv.org/abs/2402.04249",
    "author": "Mantas Mazeika, Long Phan, Xuwang Yin, et al.",
    "organization": "Center for AI Safety, UIUC, CMU",
    "date": "2024-02-06",
    "type": "academic_paper",
    "techniqueIds": ["tech-red-teaming", "tech-adversarial-training", "tech-input-guardrail-systems", "tech-output-filtering-systems"],
    "summary": "Standardized benchmark comparing 18 red teaming attack methods against 33 target LLMs and defenses, proposing an efficient adversarial training method for robustness.",
    "sentiment": "neutral"
  },
  {
    "id": "commentary-008",
    "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
    "url": "https://arxiv.org/abs/2307.15043",
    "author": "Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson",
    "organization": "CMU, Google DeepMind, Bosch Center for AI",
    "date": "2023-07-27",
    "type": "academic_paper",
    "techniqueIds": ["tech-adversarial-training", "tech-rlhf", "tech-input-guardrail-systems"],
    "summary": "Demonstrates the GCG attack: adversarial suffixes optimized on open-source models transfer to break safety alignment on ChatGPT, Bard, and Claude, revealing fundamental vulnerabilities in current safety training.",
    "sentiment": "negative"
  },
  {
    "id": "commentary-009",
    "title": "Jailbroken: How Does LLM Safety Training Fail?",
    "url": "https://arxiv.org/abs/2307.02483",
    "author": "Alexander Wei, Nika Haghtalab, Jacob Steinhardt",
    "organization": "UC Berkeley",
    "date": "2023-07-05",
    "type": "academic_paper",
    "techniqueIds": ["tech-rlhf", "tech-constitutional-ai", "tech-input-guardrail-systems", "tech-output-filtering-systems"],
    "summary": "Identifies two failure modes of safety training — competing objectives and mismatched generalization — and demonstrates successful jailbreak attacks against GPT-4 and Claude v1.3.",
    "sentiment": "negative"
  },
  {
    "id": "commentary-010",
    "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
    "url": "https://arxiv.org/abs/2308.03825",
    "author": "Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang",
    "organization": "CISPA Helmholtz Center for Information Security",
    "date": "2023-08-07",
    "type": "academic_paper",
    "techniqueIds": ["tech-input-guardrail-systems", "tech-red-teaming", "tech-rlhf"],
    "summary": "Analyzes 1,405 real-world jailbreak prompts from online communities, finding five prompts with 0.95 attack success rates against GPT-3.5 and GPT-4, with some persisting for over 240 days.",
    "sentiment": "negative"
  },
  {
    "id": "commentary-011",
    "title": "Not What You've Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
    "url": "https://arxiv.org/abs/2302.12173",
    "author": "Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz",
    "organization": "CISPA Helmholtz Center, Saarland University",
    "date": "2023-02-23",
    "type": "academic_paper",
    "techniqueIds": ["tech-input-guardrail-systems", "tech-output-filtering-systems", "tech-prompt-injection-defense"],
    "summary": "Introduces indirect prompt injection as a new attack vector against LLM-integrated applications, demonstrating data theft and remote code execution against real-world systems including Bing Chat.",
    "sentiment": "negative"
  },
  {
    "id": "commentary-012",
    "title": "Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations",
    "url": "https://arxiv.org/abs/2312.06674",
    "author": "Hakan Inan, Kartikeya Upasani, Jianfeng Chi, et al.",
    "organization": "Meta",
    "date": "2023-12-07",
    "type": "academic_paper",
    "techniqueIds": ["tech-input-guardrail-systems", "tech-output-filtering-systems"],
    "summary": "Introduces Llama Guard, a fine-tuned LLM classifier for both input prompt and output response safety classification, establishing a taxonomy-based approach to content moderation.",
    "sentiment": "positive"
  },
  {
    "id": "commentary-013",
    "title": "A Watermark for Large Language Models",
    "url": "https://arxiv.org/abs/2301.10226",
    "author": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein",
    "organization": "University of Maryland",
    "date": "2023-01-24",
    "type": "academic_paper",
    "techniqueIds": ["tech-watermarking"],
    "summary": "Proposes a statistical watermarking framework for LLM-generated text using green-list token promotion, detectable from short text spans without model access, with formal guarantees and minimal quality impact.",
    "sentiment": "positive"
  },
  {
    "id": "commentary-014",
    "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
    "url": "https://transformer-circuits.pub/2024/scaling-monosemanticity/",
    "author": "Adly Templeton, Tom Conerly, Jonathan Marcus, et al.",
    "organization": "Anthropic",
    "date": "2024-05-21",
    "type": "academic_paper",
    "techniqueIds": ["tech-capability-monitoring"],
    "summary": "Demonstrates that sparse autoencoders can extract millions of interpretable features from a production-scale LLM (Claude 3 Sonnet), including safety-relevant features related to deception, bias, and dangerous content.",
    "sentiment": "positive"
  },
  {
    "id": "commentary-015",
    "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?",
    "url": "https://dl.acm.org/doi/10.1145/3442188.3445922",
    "author": "Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell",
    "organization": "University of Washington",
    "date": "2021-03-01",
    "type": "academic_paper",
    "techniqueIds": ["tech-bias-mitigation"],
    "summary": "Influential critique arguing that large language models encode and amplify societal biases from training data, raising concerns about environmental costs and harms of deploying systems that produce fluent text without understanding.",
    "sentiment": "negative"
  },
  {
    "id": "commentary-016",
    "title": "Bias and Fairness in Large Language Models: A Survey",
    "url": "https://arxiv.org/abs/2309.00770",
    "author": "Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, et al.",
    "organization": "Adobe Research, University of Minnesota",
    "date": "2023-09-01",
    "type": "academic_paper",
    "techniqueIds": ["tech-bias-mitigation"],
    "summary": "Comprehensive survey consolidating notions of social bias and fairness in NLP, proposing taxonomies for bias evaluation metrics, datasets, and mitigation techniques for LLMs.",
    "sentiment": "neutral"
  },
  {
    "id": "commentary-017",
    "title": "Adversarial Training for High-Stakes Reliability",
    "url": "https://arxiv.org/abs/2205.01663",
    "author": "Daniel M. Ziegler, Seraphina Nix, Lawrence Chan, et al.",
    "organization": "Anthropic, Redwood Research",
    "date": "2022-05-03",
    "type": "academic_paper",
    "techniqueIds": ["tech-adversarial-training", "tech-red-teaming"],
    "summary": "Demonstrates that adversarial training with human red teamers doubles the time required to find adversarial examples (13 to 26 minutes) while preserving in-distribution performance.",
    "sentiment": "positive"
  },
  {
    "id": "commentary-018",
    "title": "NIST AI 600-1: Artificial Intelligence Risk Management Framework - Generative AI Profile",
    "url": "https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf",
    "author": "National Institute of Standards and Technology",
    "organization": "NIST, U.S. Department of Commerce",
    "date": "2024-07-26",
    "type": "report",
    "techniqueIds": ["tech-red-teaming", "tech-watermarking", "tech-input-guardrail-systems", "tech-output-filtering-systems", "tech-bias-mitigation"],
    "summary": "Official U.S. government risk management profile for generative AI identifying 12 risk categories and providing suggested actions for governance, content provenance, pre-deployment testing, and incident disclosure.",
    "sentiment": "neutral"
  }
]
